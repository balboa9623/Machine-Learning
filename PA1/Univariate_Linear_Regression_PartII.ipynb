{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Univariate Linear Regression-PartII-Incomplete.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFVfylAmmMt-",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Packages ##\n",
        "\n",
        "First, you need to import all the packages that you will need during this assignment. \n",
        "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
        "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
        "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2f2iYwbmMuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8xJSB0nmMuN",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Problem Statement ##\n",
        "\n",
        "You are given a dataset containing:\n",
        "    - a training set for a linear function\n",
        "    - a test set for testing the learned hypothesis function\n",
        "    \n",
        "You will build a simple linear regression algorithm that can correctly identify the parameters of w0 and w1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zminkiZemMuO",
        "colab_type": "text"
      },
      "source": [
        "## 3 - Forward and Backward propagation ##\n",
        "\n",
        "Forward Propagation:\n",
        "- You get X\n",
        "- You compute $h(x) = w_{1} * x + w_{0}$\n",
        "- You calculate the loss function:  $$L(W) = \\frac{1}{2m} \\sum_{i=1}^{n} \\left(h_{W}(x^{(i)})  - y^{(i)}\\right)^2$$. \n",
        "\n",
        "Here are the two formulas you will be using: \n",
        "\n",
        "$$ \\frac{\\partial L}{\\partial w_{1}} = \\frac{1}{m} \\sum_{i=1}^m (( w_{0} + w_{1} * x^{(i)} -y^{(i)}) * x^{(i)})\\tag{1}$$\n",
        "$$ \\frac{\\partial L}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^m (( w_{0} + w_{1} * x^{(i)} -y^{(i)}))\\tag{2}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP3hEbEfmMuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimplifiedNetwork:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        #the weight associated with the single feature, a scalar\n",
        "        self.w_1 = 0 \n",
        "        #bias, a scalar\n",
        "        self.w_0 = 0\n",
        "        #learning rate\n",
        "        self.alpha = learning_rate\n",
        "        \n",
        "    def set_learning_rate(self, alpha):\n",
        "        \"\"\"\n",
        "        This function accepts the learning rate\n",
        "        Arguments:\n",
        "        alpha -- the learning rate\n",
        "        \"\"\"\n",
        "        self.alpha = alpha\n",
        "        \n",
        "    def forward_back_propagation(self, X, Y):\n",
        "        \"\"\"\n",
        "        This function forward and backward propagation\n",
        "        Arguments:\n",
        "        X -- data of the series of single feature\n",
        "        Y -- true \"label\" vector\n",
        "\n",
        "        Return:\n",
        "        loss -- outcome of the loss function\n",
        "        gradient -- dictionary containing dw_1 and dw_0\n",
        "\n",
        "        \"\"\"\n",
        "        #number of training examples\n",
        "        m = X.shape[1]\n",
        "\n",
        "\n",
        "        loss = 0\n",
        "        dw_1 = 0 #gredient of w_1\n",
        "        dw_0 = 0 #gredient of w_0\n",
        "\n",
        "        weight = self.get_weights()\n",
        "        w0 = weight['w_0']\n",
        "        w1 = weight['w_1']\n",
        "\n",
        "        #iterate through all the training examples to\n",
        "        #    1. Calculate the loss\n",
        "        #    2. calcuate the accumulated gradient dw_1 and dw_0\n",
        "        for i in range(m):\n",
        "            #Your code starts from here\n",
        "            Y_hat = w0 + (w1 * X[0][i])\n",
        "            loss += (Y_hat - Y[0][i]) ** 2\n",
        "            dw_1 += (Y_hat - Y[0][i]) * X[0][i]\n",
        "            dw_0 += (Y_hat - Y[0][i])\n",
        "            #Your code ends here\n",
        "\n",
        "        #Use the accumulated loss and gredients to calculate the averaged counterparts\n",
        "        loss = loss / (2 * m)\n",
        "        dw_1 = dw_1 / m\n",
        "        dw_0 = dw_0 /m\n",
        "\n",
        "\n",
        "        gradients = {\n",
        "            \"dw_1\": dw_1,\n",
        "            \"dw_0\": dw_0\n",
        "        }\n",
        "\n",
        "        return gradients, loss\n",
        "    \n",
        "    \n",
        "    #Function predict: \n",
        "    #                Predict the value using learned linear regression parameters (w, b)\n",
        "    def predict(self, x):\n",
        "        '''\n",
        "        Predict the value using learned linear regression parameters (w_0, w_1)\n",
        "\n",
        "        Arguments:\n",
        "        X -- data set of single feature\n",
        "\n",
        "        Returns:\n",
        "        Y_prediction -- predictions for all items in X\n",
        "        '''\n",
        "        ## Your code starts here ##\n",
        "        # Hint: You can use matrix/array operation. \n",
        "        # For example, if B is a matrix, 2 * B ends up with every item in matrix B being multiplied by 2\n",
        "        A = self.w_1 * x + self.w_0\n",
        "        ## Your code ends here ##\n",
        "        return A\n",
        "\n",
        "    \n",
        "    def get_weights(self):\n",
        "        weights = {\n",
        "            'w_1': self.w_1,\n",
        "            'w_0': self.w_0\n",
        "        }\n",
        "        return weights\n",
        "\n",
        "\n",
        "    def fit(self, X, Y, epochs=1, print_loss = True):\n",
        "        \"\"\"\n",
        "        This function optimizes w_0 and w_1 by running a gradient descent algorithm\n",
        "\n",
        "        Arguments:\n",
        "        X -- data of the single feature\n",
        "        Y -- true \"label\" vector \n",
        "        num_iterations -- number of iterations of the optimization loop\n",
        "        learning_rate -- learning rate of the gradient descent update rule\n",
        "        print_loss -- True to print the loss every 100 steps\n",
        "\n",
        "        Returns:\n",
        "        params -- dictionary containing the weights w_1 and bias w_0\n",
        "        grads -- dictionary containing the gradients of the weights and bias with respect to the loss function\n",
        "        losses -- list of all the losses computed during the optimization, this will be used to plot the learning curve.\n",
        "\n",
        "        Tips:\n",
        "        You need to finish the following steps:\n",
        "            1) Calculate the loss and the gradient for the current parameters. Use propagate().\n",
        "            2) Update the parameters using gradient descent rule for w_0 and w_1.\n",
        "        \"\"\"\n",
        "        clr = ['b', 'g', 'c', 'm', 'y', 'k'] #color for labeling lines\n",
        "        losses = []\n",
        "        for i in range(epochs):\n",
        "            ##Your code starts from here##\n",
        "            gradients, loss = self.forward_back_propagation(X, Y)\n",
        "\n",
        "            dw_1 = gradients['dw_1']\n",
        "            dw_0 = gradients['dw_0']\n",
        "\n",
        "            self.w_1 = self.w_1 - (learning_rate * dw_1)\n",
        "            self.w_0 = self.w_0 - (learning_rate * dw_0)\n",
        "            ##Your code ends here##\n",
        "\n",
        "            # Print the loss every 200 training examples\n",
        "            if print_loss and i % 200 == 0:\n",
        "                losses.append(loss)\n",
        "                print (\"At Epoch %i, the loss = %f; w_0 = %f; w_1 = %f\" %(i, loss, self.w_0, self.w_1))\n",
        "                plt.plot(X, self.predict(X), marker = 'o', color=clr[int((i/200) % len(clr))], label='Epoch' + str(i))\n",
        "            \n",
        "\n",
        "        \n",
        "        params = {\n",
        "            \"w_1\": self.w_1,\n",
        "            \"w_0\": self.w_0\n",
        "        }\n",
        "\n",
        "        gradients = {\n",
        "            \"dw_1\": dw_1,\n",
        "            \"dw_0\": dw_0\n",
        "        }\n",
        "\n",
        "        return params, gradients, losses"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfE8EqFvmMuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 2000, learning_rate = 0.5, print_loss = False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array\n",
        "    Y_train -- training labels represented by a numpy array (vector)\n",
        "    X_test -- test set represented by a numpy array\n",
        "    Y_test -- test labels represented by a numpy array (vector)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_loss -- Set to true to print the loss every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    model = SimplifiedNetwork()\n",
        "    model.set_learning_rate(learning_rate)\n",
        "    parameters, grads, losses = model.fit(X_train, Y_train, epochs, print_loss)\n",
        "    \n",
        "    Y_prediction_test = model.predict(X_test)\n",
        "    Y_prediction_train = model.predict(X_train)\n",
        "    \n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)/Y_train) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)/Y_test) * 100))\n",
        "\n",
        "    d = {\"losses\": losses,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w_1\" : model.get_weights()['w_1'], \n",
        "         \"w_0\" : model.get_weights()['w_0'],\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"epochs\": epochs}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KNgPPKCmMuZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "4b3fa4d0-7fe1-4784-8ca8-6878334b6ffa"
      },
      "source": [
        "df = pd.read_csv('train.csv', header=None)\n",
        "\n",
        "X_train = df[0].values.reshape(-1, 1).T\n",
        "Y_train = df[1].values.reshape(-1, 1).T\n",
        "\n",
        "df_test = pd.read_csv('test.csv', header=None)\n",
        "X_test = df_test[0].values.reshape(-1, 1).T\n",
        "Y_test = df_test[1].values.reshape(-1, 1).T\n",
        "\n",
        "\n",
        "plt.scatter(X_train, Y_train, color='r')\n",
        "\n",
        "##Your code starts from here##\n",
        "epochs = 1000\n",
        "learning_rate = 0.05\n",
        "##Your code ends here##\n",
        "\n",
        "d = Run_Experiment(X_train, Y_train, X_test, Y_test, epochs, learning_rate, print_loss = True)\n",
        "print(\"w_1 is \" + str(d['w_1']) + \" and w_0 is \" + str(d['w_0']))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "At Epoch 0, the loss = 175.580806; w_0 = 0.889418; w_1 = 5.406170\n",
            "At Epoch 200, the loss = 0.089383; w_0 = 6.983569; w_1 = 2.088133\n",
            "At Epoch 400, the loss = 0.043272; w_0 = 7.534471; w_1 = 2.006578\n",
            "At Epoch 600, the loss = 0.042935; w_0 = 7.581567; w_1 = 1.999606\n",
            "At Epoch 800, the loss = 0.042933; w_0 = 7.585593; w_1 = 1.999010\n",
            "train accuracy: 98.36486617914184 %\n",
            "test accuracy: 98.1737334474057 %\n",
            "w_1 is 1.9989594708735177 and w_0 is 7.585937155896865\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtjZERoBmMud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYcOtbIdmMug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    }
  ]
}
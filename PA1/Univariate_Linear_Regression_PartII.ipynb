{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Univariate Linear Regression-PartII.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nb6--HF5IvQ",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Packages ##\n",
        "\n",
        "First, you need to import all the packages that you will need during this assignment. \n",
        "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
        "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
        "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILDJ2DyG5IvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pw_liCc5IvV",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Problem Statement ##\n",
        "\n",
        "You are given a dataset containing:\n",
        "    - a training set for a linear function\n",
        "    - a test set for testing the learned hypothesis function\n",
        "    \n",
        "You will build a simple linear regression algorithm that can correctly identify the parameters of w0 and w1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzNF2Dzs5IvV",
        "colab_type": "text"
      },
      "source": [
        "## 3 - Forward and Backward propagation ##\n",
        "\n",
        "Forward Propagation:\n",
        "- You get X\n",
        "- You compute $h(x) = w_{1} * x + w_{0}$\n",
        "- You calculate the loss function:  $$L(W) = \\frac{1}{2m} \\sum_{i=1}^{n} \\left(h_{W}(x^{(i)})  - y^{(i)}\\right)^2$$. \n",
        "\n",
        "Here are the two formulas you will be using: \n",
        "\n",
        "$$ \\frac{\\partial L}{\\partial w_{1}} = \\frac{1}{m} \\sum_{i=1}^m (( w_{0} + w_{1} * x^{(i)} -y^{(i)}) * x^{(i)})\\tag{1}$$\n",
        "$$ \\frac{\\partial L}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^m (( w_{0} + w_{1} * x^{(i)} -y^{(i)}))\\tag{2}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWHzNybO5IvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimplifiedNetwork:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        #the weight associated with the single feature, a scalar\n",
        "        self.w_1 = 0 \n",
        "        #bias, a scalar\n",
        "        self.w_0 = 0\n",
        "        #learning rate\n",
        "        self.alpha = learning_rate\n",
        "        \n",
        "    def set_learning_rate(self, alpha):\n",
        "        \"\"\"\n",
        "        This function accepts the learning rate\n",
        "        Arguments:\n",
        "        alpha -- the learning rate\n",
        "        \"\"\"\n",
        "        self.alpha = alpha\n",
        "        \n",
        "    def forward_back_propagation(self, X, Y):\n",
        "        \"\"\"\n",
        "        This function forward and backward propagation\n",
        "        Arguments:\n",
        "        X -- data of the series of single feature\n",
        "        Y -- true \"label\" vector\n",
        "\n",
        "        Return:\n",
        "        loss -- outcome of the loss function\n",
        "        gradient -- dictionary containing dw_1 and dw_0\n",
        "\n",
        "        \"\"\"\n",
        "        #number of training examples\n",
        "        m = X.shape[1]\n",
        "\n",
        "\n",
        "        loss = 0\n",
        "        dw_1 = 0 #gredient of w_1\n",
        "        dw_0 = 0 #gredient of w_0\n",
        "\n",
        "        #iterate through all the training examples to\n",
        "        #    1. Calculate the loss\n",
        "        #    2. calcuate the accumulated gradient dw_1 and dw_0\n",
        "        for i in range(m):\n",
        "            #Your code starts from here\n",
        "            Y_hat = self.w_0 + (self.w_1 * X[0][i])\n",
        "            loss += (Y_hat - Y[0][i]) ** 2\n",
        "            dw_1 += (Y_hat - Y[0][i]) * X[0][i]\n",
        "            dw_0 += (Y_hat - Y[0][i]) * X[0][i]\n",
        "            #Your code ends here\n",
        "\n",
        "        #Use the accumulated loss and gredients to calculate the averaged counterparts\n",
        "        loss = loss / (2 * m)\n",
        "        dw_1 = dw_1 / m\n",
        "        dw_0 = dw_0 /m\n",
        "\n",
        "\n",
        "        gradients = {\n",
        "            \"dw_1\": dw_1,\n",
        "            \"dw_0\": dw_0\n",
        "        }\n",
        "\n",
        "        return gradients, loss\n",
        "    \n",
        "    \n",
        "    #Function predict: \n",
        "    #                Predict the value using learned linear regression parameters (w, b)\n",
        "    def predict(self, x):\n",
        "        '''\n",
        "        Predict the value using learned linear regression parameters (w_0, w_1)\n",
        "\n",
        "        Arguments:\n",
        "        X -- data set of single feature\n",
        "\n",
        "        Returns:\n",
        "        Y_prediction -- predictions for all items in X\n",
        "        '''\n",
        "        ## Your code starts here ##\n",
        "        # Hint: You can use matrix/array operation. \n",
        "        # For example, if B is a matrix, 2 * B ends up with every item in matrix B being multiplied by 2\n",
        "        A = self.w_1 * x + self.w_0\n",
        "        ## Your code ends here ##\n",
        "        return A\n",
        "\n",
        "    \n",
        "    def get_weights(self):\n",
        "        weights = {\n",
        "            'w_1': self.w_1,\n",
        "            'w_0': self.w_0\n",
        "        }\n",
        "        return weights\n",
        "\n",
        "\n",
        "    def fit(self, X, Y, epochs=1, print_loss = True):\n",
        "        \"\"\"\n",
        "        This function optimizes w_0 and w_1 by running a gradient descent algorithm\n",
        "\n",
        "        Arguments:\n",
        "        X -- data of the single feature\n",
        "        Y -- true \"label\" vector \n",
        "        num_iterations -- number of iterations of the optimization loop\n",
        "        learning_rate -- learning rate of the gradient descent update rule\n",
        "        print_loss -- True to print the loss every 100 steps\n",
        "\n",
        "        Returns:\n",
        "        params -- dictionary containing the weights w_1 and bias w_0\n",
        "        grads -- dictionary containing the gradients of the weights and bias with respect to the loss function\n",
        "        losses -- list of all the losses computed during the optimization, this will be used to plot the learning curve.\n",
        "\n",
        "        Tips:\n",
        "        You need to finish the following steps:\n",
        "            1) Calculate the loss and the gradient for the current parameters. Use propagate().\n",
        "            2) Update the parameters using gradient descent rule for w_0 and w_1.\n",
        "        \"\"\"\n",
        "        clr = ['b', 'g', 'c', 'm', 'y', 'k'] #color for labeling lines\n",
        "        losses = []\n",
        "        for i in range(epochs):\n",
        "            ##Your code starts from here##\n",
        "            gradients, loss = self.forward_back_propagation(X, Y)\n",
        "\n",
        "            dw_1 = gradients['dw_1']\n",
        "            dw_0 = gradients['dw_0']\n",
        "\n",
        "            self.w_1 = self.w_1 - (learning_rate * dw_1)\n",
        "            self.w_0 = self.w_0 - (learning_rate * dw_0)\n",
        "            ##Your code ends here##\n",
        "\n",
        "            # Print the loss every 200 training examples\n",
        "            if print_loss and i % 200 == 0:\n",
        "                losses.append(loss)\n",
        "                print (\"At Epoch %i, the loss = %f; w_0 = %f; w_1 = %f\" %(i, loss, self.w_0, self.w_1))\n",
        "                plt.plot(X, self.predict(X), marker = 'o', color=clr[int((i/200) % len(clr))], label='Epoch' + str(i))\n",
        "            \n",
        "\n",
        "        \n",
        "        params = {\n",
        "            \"w_1\": self.w_1,\n",
        "            \"w_0\": self.w_0\n",
        "        }\n",
        "\n",
        "        gradients = {\n",
        "            \"dw_1\": dw_1,\n",
        "            \"dw_0\": dw_0\n",
        "        }\n",
        "\n",
        "        return params, gradients, losses"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlkm-lmy5IvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 2000, learning_rate = 0.5, print_loss = False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array\n",
        "    Y_train -- training labels represented by a numpy array (vector)\n",
        "    X_test -- test set represented by a numpy array\n",
        "    Y_test -- test labels represented by a numpy array (vector)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_loss -- Set to true to print the loss every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    model = SimplifiedNetwork()\n",
        "    model.set_learning_rate(learning_rate)\n",
        "    parameters, grads, losses = model.fit(X_train, Y_train, epochs, print_loss)\n",
        "    \n",
        "    Y_prediction_test = model.predict(X_test)\n",
        "    Y_prediction_train = model.predict(X_train)\n",
        "    \n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)/Y_train) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)/Y_test) * 100))\n",
        "\n",
        "    d = {\"losses\": losses,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w_1\" : model.get_weights()['w_1'], \n",
        "         \"w_0\" : model.get_weights()['w_0'],\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"epochs\": epochs}\n",
        "    plt.title(\"Learning rate = \"+ str(d[\"learning_rate\"]))\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geBYkK795Ivc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "b9f4b459-d5bc-4589-ac81-4a777e03e54d"
      },
      "source": [
        "df = pd.read_csv('train.csv', header=None)\n",
        "\n",
        "X_train = df[0].values.reshape(-1, 1).T\n",
        "Y_train = df[1].values.reshape(-1, 1).T\n",
        "\n",
        "df_test = pd.read_csv('test.csv', header=None)\n",
        "X_test = df_test[0].values.reshape(-1, 1).T\n",
        "Y_test = df_test[1].values.reshape(-1, 1).T\n",
        "\n",
        "\n",
        "plt.scatter(X_train, Y_train, color='r')\n",
        "\n",
        "##Your code starts from here##\n",
        "epochs = 1000\n",
        "learning_rate =  0.05\n",
        "##Your code ends here##\n",
        "\n",
        "\n",
        "d = Run_Experiment(X_train, Y_train, X_test, Y_test, epochs, learning_rate, print_loss = True)\n",
        "print(\"w_1 is \" + str(d['w_1']) + \" and w_0 is \" + str(d['w_0']))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "At Epoch 0, the loss = 175.580806; w_0 = 5.406170; w_1 = 5.406170\n",
            "At Epoch 200, the loss = 8.591582; w_0 = 3.178295; w_1 = 3.178295\n",
            "At Epoch 400, the loss = 3.252393; w_0 = 2.794740; w_1 = 2.794740\n",
            "At Epoch 600, the loss = 3.027052; w_0 = 2.728707; w_1 = 2.728707\n",
            "At Epoch 800, the loss = 3.008823; w_0 = 2.717338; w_1 = 2.717338\n",
            "train accuracy: 84.01925797044522 %\n",
            "test accuracy: 83.1022946580751 %\n",
            "w_1 is 2.7145634411881585 and w_0 is 2.7145634411881585\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0LyCIQe5Ivf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXTCZZF75Ivi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
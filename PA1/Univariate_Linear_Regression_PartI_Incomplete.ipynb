{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Univariate Linear Regression-PartI-Incomplete.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FrXb0UJK2Ad",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Packages ##\n",
        "\n",
        "First, you need to import all the packages that you will need during this assignment. \n",
        "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
        "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
        "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFF1DAy1K2Ae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMxFXPsGK2Ai",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Problem Statement ##\n",
        "\n",
        "You are given a dataset containing:\n",
        "    - a training set for a linear function\n",
        "    - a test set for testing the learned hypothesis function\n",
        "    \n",
        "You will build a simple linear regression algorithm that can correctly identify the parameters of w0 and w1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6Ye-REoK2Aj",
        "colab_type": "text"
      },
      "source": [
        "## 3 - Forward and Backward propagation ##\n",
        "\n",
        "Forward Propagation:\n",
        "- You get X\n",
        "- You compute $h(x) = w_{1} * x + w_{0}$\n",
        "- You calculate the loss function:  $$L(W) = \\frac{1}{2m} \\sum_{i=1}^{n} \\left(h_{W}(x^{(i)})  - y^{(i)}\\right)^2$$. \n",
        "\n",
        "Here are the two formulas you will be using: \n",
        "\n",
        "$$ dw_1 = \\frac{\\partial L}{\\partial w_{1}} = \\frac{1}{m} \\sum_{i=1}^m (( w_{0} + w_{1} * x^{(i)} -y^{(i)}) * x^{(i)})\\tag{1}$$\n",
        "$$ dw_0 = \\frac{\\partial L}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^m (( w_{0} + w_{1} * x^{(i)} -y^{(i)}))\\tag{2}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aEC9RgEK2Aj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here we define a simplified neural network\n",
        "class SimplifiedNetwork:\n",
        "    def __init__(self):\n",
        "        #the weight associated with the single feature, a scalar\n",
        "        self.w_1 = 0 \n",
        "        #bias, a scalar\n",
        "        self.w_0 = 0\n",
        "        \n",
        "    def forward_back_propagation(self, X, Y):\n",
        "        \"\"\"\n",
        "        This function forward and backward propagation\n",
        "        Arguments:\n",
        "        X -- data of the series of single feature\n",
        "        Y -- true \"label\" vector\n",
        "\n",
        "        Return:\n",
        "        loss -- outcome of the loss function\n",
        "        gradient -- dictionary containing dw_1 and dw_0\n",
        "\n",
        "        \"\"\"\n",
        "        #number of training examples\n",
        "        m = X.shape[1]\n",
        "\n",
        "\n",
        "        loss = 0 #loss\n",
        "        dw_1 = 0 #gredient of w_1\n",
        "        dw_0 = 0 #gredient of w_0\n",
        "\n",
        "        #iterate through all the training examples to\n",
        "        #    1. Calculate the loss\n",
        "        #    2. calcuate the accumulated gradient dw_1 and dw_0\n",
        "        for i in range(m):\n",
        "            #Your code starts from here\n",
        "            #Y_hat is output of the hypothesis function\n",
        "            Y_hat = \n",
        "            loss += \n",
        "            dw_1 += \n",
        "            dw_0 += \n",
        "            #Your code ends here\n",
        "\n",
        "        #Use the accumulated loss and gredients to calculate the averaged counterparts\n",
        "        loss = loss / (2 * m)\n",
        "        dw_1 = dw_1 / m\n",
        "        dw_0 = dw_0 /m\n",
        "\n",
        "\n",
        "        gradients = {\n",
        "            \"dw_1\": dw_1,\n",
        "            \"dw_0\": dw_0\n",
        "        }\n",
        "\n",
        "        return gradients, loss\n",
        "    \n",
        "    \n",
        "    #Function predict: \n",
        "    #   Predict the value using learned linear regression parameters (w_0, w_1)\n",
        "    def predict(self, x):\n",
        "        '''\n",
        "        Predict the value using learned linear regression parameters (w_0, w_1)\n",
        "\n",
        "        Arguments:\n",
        "        X -- data set of single feature\n",
        "\n",
        "        Returns:\n",
        "        Y_prediction -- predictions for all items in X\n",
        "        '''\n",
        "        ## Your code starts here ##\n",
        "        # Hint: You can use matrix/array operation. \n",
        "        # For example, if B is a matrix, 2 * B ends up with every item in matrix B being multiplied by 2\n",
        "        A = \n",
        "        ## Your code ends here ##\n",
        "        return A\n",
        "\n",
        "    \n",
        "    def get_weights(self):\n",
        "        weights = {\n",
        "            'w_1': self.w_1,\n",
        "            'w_0': self.w_0\n",
        "        }\n",
        "        return weights\n",
        "\n",
        "\n",
        "    def fit(self, X, Y, epochs=1, learning_rate = 0.01, print_loss = True):\n",
        "        \"\"\"\n",
        "        This function optimizes w_1 and w_0 by running a gradient descent algorithm\n",
        "\n",
        "        Arguments:\n",
        "        X -- data of the single feature\n",
        "        Y -- true \"label\" vector \n",
        "        num_iterations -- number of iterations of the optimization loop\n",
        "        learning_rate -- learning rate of the gradient descent update rule\n",
        "        print_loss -- True to print the loss every 100 steps\n",
        "\n",
        "        Returns:\n",
        "        params -- dictionary containing the weights w_1 and bias w_0\n",
        "        grads -- dictionary containing the gradients of the weights and bias with respect to the loss function\n",
        "        losss -- list of all the losss computed during the optimization, this will be used to plot the learning curve.\n",
        "\n",
        "        Tips:\n",
        "        You need to finish the following steps:\n",
        "            1) Calculate the loss and gradients for the current parameters. Use forward_back_propagation().\n",
        "            2) Update the parameters using gradient descent rule for w_0 and w_1.\n",
        "        \"\"\"\n",
        "        losss = []\n",
        "        for i in range(epochs):\n",
        "            ##Your code starts from here##\n",
        "            gradients, loss = \n",
        "\n",
        "            dw_1 = gradients['dw_1']\n",
        "            dw_0 = gradients['dw_0']\n",
        "\n",
        "            self.w_1 = \n",
        "            self.w_0 = \n",
        "            ##Your code ends here##\n",
        "\n",
        "\n",
        "            # Print the loss every 100 training examples\n",
        "            if print_loss and i % 100 == 0:\n",
        "                losss.append(loss)\n",
        "                print (\"loss after iteration %i: %f\" %(i, loss))\n",
        "\n",
        "            \n",
        "\n",
        "        params = {\n",
        "            \"w_1\": self.w_1,\n",
        "            \"w_0\": self.w_0\n",
        "        }\n",
        "\n",
        "        gradients = {\n",
        "            \"dw_1\": dw_1,\n",
        "            \"dw_0\": dw_0\n",
        "        }\n",
        "\n",
        "        return params, gradients, losss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW2-ru9xK2Am",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 2000, learning_rate = 0.5, print_loss = False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array\n",
        "    Y_train -- training labels represented by a numpy array (vector)\n",
        "    X_test -- test set represented by a numpy array\n",
        "    Y_test -- test labels represented by a numpy array (vector)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_loss -- Set to true to print the loss every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    model = SimplifiedNetwork()\n",
        "    parameters, grads, losses = model.fit(X_train, Y_train, epochs, learning_rate, print_loss)\n",
        "    \n",
        "    Y_prediction_test = model.predict(X_test)\n",
        "    Y_prediction_train = model.predict(X_train)\n",
        "    \n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    d = {\"losses\": losses,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w_1\" : model.get_weights()['w_1'], \n",
        "         \"w_0\" : model.get_weights()['w_0'],\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"epochs\": epochs}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H-grOqfK2Ao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('train.csv', header=None)\n",
        "\n",
        "X_train = df[0].values.reshape(-1, 1).T\n",
        "Y_train = df[1].values.reshape(-1, 1).T\n",
        "\n",
        "df_test = pd.read_csv('test.csv', header=None)\n",
        "X_test = df_test[0].values.reshape(-1, 1).T\n",
        "Y_test = df_test[1].values.reshape(-1, 1).T\n",
        "\n",
        "\n",
        "plt.scatter(X_train, Y_train)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7XsbJAPK2Ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 1000, learning_rate = 0.01, print_loss = True)\n",
        "print(\"w_1 is \" + str(d['w_1']) + \" and w_0 is \" + str(d['w_0']))\n",
        "\n",
        "# Plot learning curve (with losses)\n",
        "losses = np.squeeze(d['losses'])\n",
        "plt.plot(losses)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87APmdteK2At",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
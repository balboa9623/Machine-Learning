{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FrXb0UJK2Ad"
   },
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First, you need to import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFF1DAy1K2Ae"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-fa18575525b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yMxFXPsGK2Ai"
   },
   "source": [
    "## 2 - Problem Statement ##\n",
    "\n",
    "You are given a dataset containing:\n",
    "    - a training set for a linear function\n",
    "    - a test set for testing the learned hypothesis function\n",
    "    \n",
    "You will build a simple linear regression algorithm that can correctly identify the parameters of w0 and w1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6Ye-REoK2Aj"
   },
   "source": [
    "## 3 - Forward and Backward propagation ##\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $h(x) = w_{1} * x + w_{0}$\n",
    "- You calculate the loss function:  $$L(W) = \\frac{1}{2m} \\sum_{i=1}^{n} \\left(h_{W}(x^{(i)})  - y^{(i)}\\right)^2$$. \n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ dw_1 = \\frac{\\partial L}{\\partial w_{1}} = \\frac{1}{m} \\sum_{i=1}^m (( w_{0} + w_{1} * x^{(i)} -y^{(i)}) * x^{(i)})\\tag{1}$$\n",
    "$$ dw_0 = \\frac{\\partial L}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^m (( w_{0} + w_{1} * x^{(i)} -y^{(i)}))\\tag{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7aEC9RgEK2Aj"
   },
   "outputs": [],
   "source": [
    "#Here we define a simplified neural network\n",
    "class SimplifiedNetwork:\n",
    "    def __init__(self):\n",
    "        #the weight associated with the single feature, a scalar\n",
    "        self.w_1 = 0 \n",
    "        #bias, a scalar\n",
    "        self.w_0 = 0\n",
    "        \n",
    "    def forward_back_propagation(self, X, Y):\n",
    "        \"\"\"\n",
    "        This function forward and backward propagation\n",
    "        Arguments:\n",
    "        X -- data of the series of single feature\n",
    "        Y -- true \"label\" vector\n",
    "\n",
    "        Return:\n",
    "        loss -- outcome of the loss function\n",
    "        gradient -- dictionary containing dw_1 and dw_0\n",
    "\n",
    "        \"\"\"\n",
    "        #number of training examples\n",
    "        m = X.shape[1]\n",
    "\n",
    "        loss = 0 #loss\n",
    "        dw_1 = 0 #gredient of w_1\n",
    "        dw_0 = 0 #gredient of w_0\n",
    "\n",
    "        weight = self.get_weights()\n",
    "        w0 = weight['w_0']\n",
    "        w1 = weight['w_1']\n",
    "\n",
    "        #iterate through all the training examples to\n",
    "        #    1. Calculate the loss\n",
    "        #    2. calcuate the accumulated gradient dw_1 and dw_0\n",
    "        for i in range(m):\n",
    "            #Your code starts from here\n",
    "            #Y_hat is output of the hypothesis function\n",
    "            Y_hat = w0 + (w1 * X[0][i])\n",
    "            loss += (Y_hat - Y[0][i]) ** 2\n",
    "            dw_1 += (Y_hat - Y[0][i]) * X[0][i]\n",
    "            dw_0 += Y_hat - Y[0][i]\n",
    "            #Your code ends here\n",
    "\n",
    "        #Use the accumulated loss and gredients to calculate the averaged counterparts\n",
    "        loss = loss / (2 * m)\n",
    "        dw_1 = dw_1 / m\n",
    "        dw_0 = dw_0 /m\n",
    "\n",
    "        # print(\"w_0 = \",w0, \" w_1 = \", w1)\n",
    "\n",
    "        gradients = {\n",
    "            \"dw_1\": dw_1,\n",
    "            \"dw_0\": dw_0\n",
    "        }\n",
    "\n",
    "        return gradients, loss\n",
    "    \n",
    "    \n",
    "    #Function predict: \n",
    "    #   Predict the value using learned linear regression parameters (w_0, w_1)\n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        Predict the value using learned linear regression parameters (w_0, w_1)\n",
    "\n",
    "        Arguments:\n",
    "        X -- data set of single feature\n",
    "\n",
    "        Returns:\n",
    "        Y_prediction -- predictions for all items in X\n",
    "        '''\n",
    "        ## Your code starts here ##\n",
    "        # Hint: You can use matrix/array operation. \n",
    "        # For example, if B is a matrix, 2 * B ends up with every item in matrix B being multiplied by 2\n",
    "\n",
    "        A = np.dot(self.w_1, x) # DON'T KNOW WHAT GOES HERE ???\n",
    "        \n",
    "        ## Your code ends here ##\n",
    "        return A\n",
    "\n",
    "    \n",
    "    def get_weights(self):\n",
    "        weights = {\n",
    "            'w_1': self.w_1,\n",
    "            'w_0': self.w_0\n",
    "        }\n",
    "        return weights\n",
    "\n",
    "\n",
    "    def fit(self, X, Y, epochs=1, learning_rate = 0.01, print_loss = True):\n",
    "        \"\"\"\n",
    "        This function optimizes w_1 and w_0 by running a gradient descent algorithm\n",
    "\n",
    "        Arguments:\n",
    "        X -- data of the single feature\n",
    "        Y -- true \"label\" vector\n",
    "        num_iterations -- number of iterations of the optimization loop\n",
    "        learning_rate -- learning rate of the gradient descent update rule\n",
    "        print_loss -- True to print the loss every 100 steps\n",
    "\n",
    "        Returns:\n",
    "        params -- dictionary containing the weights w_1 and bias w_0\n",
    "        grads -- dictionary containing the gradients of the weights and bias with respect to the loss function\n",
    "        losss -- list of all the losss computed during the optimization, this will be used to plot the learning curve.\n",
    "\n",
    "        Tips:\n",
    "        You need to finish the following steps:\n",
    "            1) Calculate the loss and gradients for the current parameters. Use forward_back_propagation().\n",
    "            2) Update the parameters using gradient descent rule for w_0 and w_1.\n",
    "        \"\"\"\n",
    "        losss = []\n",
    "        for i in range(epochs):\n",
    "            ##Your code starts from here##\n",
    "            gradients, loss = self.forward_back_propagation(X, Y)\n",
    "\n",
    "            dw_1 = gradients['dw_1']\n",
    "            dw_0 = gradients['dw_0']\n",
    "\n",
    "            self.w_1 = self.w_1 - (learning_rate * dw_1)\n",
    "            self.w_0 = self.w_0 - (learning_rate * dw_0)\n",
    "            ##Your code ends here##\n",
    "\n",
    "\n",
    "            # Print the loss every 100 training examples\n",
    "            if print_loss and i % 100 == 0:\n",
    "                losss.append(loss)\n",
    "                print (\"loss after iteration %i: %f\" %(i, loss))\n",
    "\n",
    "            \n",
    "\n",
    "        params = {\n",
    "            \"w_1\": self.w_1,\n",
    "            \"w_0\": self.w_0\n",
    "        }\n",
    "\n",
    "        gradients = {\n",
    "            \"dw_1\": dw_1,\n",
    "            \"dw_0\": dw_0\n",
    "        }\n",
    "\n",
    "        return params, gradients, losss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FW2-ru9xK2Am"
   },
   "outputs": [],
   "source": [
    "def Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 2000, learning_rate = 0.5, print_loss = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array\n",
    "    Y_train -- training labels represented by a numpy array (vector)\n",
    "    X_test -- test set represented by a numpy array\n",
    "    Y_test -- test labels represented by a numpy array (vector)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_loss -- Set to true to print the loss every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = SimplifiedNetwork()\n",
    "    parameters, grads, losses = model.fit(X_train, Y_train, epochs, learning_rate, print_loss)\n",
    "    \n",
    "    Y_prediction_test = model.predict(X_test)\n",
    "    Y_prediction_train = model.predict(X_train)\n",
    "    \n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    d = {\"losses\": losses,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w_1\" : model.get_weights()['w_1'], \n",
    "         \"w_0\" : model.get_weights()['w_0'],\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"epochs\": epochs}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "6H-grOqfK2Ao",
    "outputId": "8a1360ee-a09f-4901-dced-483c52ed6f50"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e5d9211c2e72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mY_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv', header=None)\n",
    "\n",
    "X_train = df[0].values.reshape(-1, 1).T\n",
    "Y_train = df[1].values.reshape(-1, 1).T\n",
    "\n",
    "df_test = pd.read_csv('test.csv', header=None)\n",
    "X_test = df_test[0].values.reshape(-1, 1).T\n",
    "Y_test = df_test[1].values.reshape(-1, 1).T\n",
    "\n",
    "\n",
    "plt.scatter(X_train, Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "colab_type": "code",
    "id": "s7XsbJAPK2Ar",
    "outputId": "00c41621-d90f-4db5-9187-595486475baa"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ef6ba59f2c70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRun_Experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"w_1 is \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'w_1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" and w_0 is \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'w_0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Plot learning curve (with losses)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'losses'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "d = Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 500, learning_rate = 0.01, print_loss = True)\n",
    "print(\"w_1 is \" + str(d['w_1']) + \" and w_0 is \" + str(d['w_0']))\n",
    "\n",
    "# Plot learning curve (with losses)\n",
    "losses = np.squeeze(d['losses'])\n",
    "plt.plot(losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Univariate_Linear_Regression_PartI_Incomplete.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

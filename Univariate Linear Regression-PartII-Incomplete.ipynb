{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First, you need to import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Problem Statement ##\n",
    "\n",
    "You are given a dataset containing:\n",
    "    - a training set for a linear function\n",
    "    - a test set for testing the learned hypothesis function\n",
    "    \n",
    "You will build a simple linear regression algorithm that can correctly identify the parameters of w0 and w1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Forward and Backward propagation ##\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $h(x) = w_{1} * x + w_{0}$\n",
    "- You calculate the loss function:  $$L(W) = \\frac{1}{2m} \\sum_{i=1}^{n} \\left(h_{W}(x^{(i)})  - y^{(i)}\\right)^2$$. \n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w_{1}} = \\frac{1}{m} \\sum_{i=1}^m (( w_{0} + w_{1} * x^{(i)} -y^{(i)}) * x^{(i)})\\tag{1}$$\n",
    "$$ \\frac{\\partial L}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^m (( w_{0} + w_{1} * x^{(i)} -y^{(i)}))\\tag{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedNetwork:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        #the weight associated with the single feature, a scalar\n",
    "        self.w_1 = 0 \n",
    "        #bias, a scalar\n",
    "        self.w_0 = 0\n",
    "        #learning rate\n",
    "        self.alpha = learning_rate\n",
    "        \n",
    "    def set_learning_rate(self, alpha):\n",
    "        \"\"\"\n",
    "        This function accepts the learning rate\n",
    "        Arguments:\n",
    "        alpha -- the learning rate\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward_back_propagation(self, X, Y):\n",
    "        \"\"\"\n",
    "        This function forward and backward propagation\n",
    "        Arguments:\n",
    "        X -- data of the series of single feature\n",
    "        Y -- true \"label\" vector\n",
    "\n",
    "        Return:\n",
    "        loss -- outcome of the loss function\n",
    "        gradient -- dictionary containing dw_1 and dw_0\n",
    "\n",
    "        \"\"\"\n",
    "        #number of training examples\n",
    "        m = X.shape[1]\n",
    "\n",
    "\n",
    "        loss = 0\n",
    "        dw_1 = 0 #gredient of w_1\n",
    "        dw_0 = 0 #gredient of w_0\n",
    "\n",
    "        #iterate through all the training examples to\n",
    "        #    1. Calculate the loss\n",
    "        #    2. calcuate the accumulated gradient dw_1 and dw_0\n",
    "        for i in range(m):\n",
    "            #Your code starts from here\n",
    "            Y_hat = \n",
    "            loss += \n",
    "            dw_1 += \n",
    "            dw_0 += \n",
    "            #Your code ends here\n",
    "\n",
    "        #Use the accumulated loss and gredients to calculate the averaged counterparts\n",
    "        loss = loss / (2 * m)\n",
    "        dw_1 = dw_1 / m\n",
    "        dw_0 = dw_0 /m\n",
    "\n",
    "\n",
    "        gradients = {\n",
    "            \"dw_1\": dw_1,\n",
    "            \"dw_0\": dw_0\n",
    "        }\n",
    "\n",
    "        return gradients, loss\n",
    "    \n",
    "    \n",
    "    #Function predict: \n",
    "    #                Predict the value using learned linear regression parameters (w, b)\n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        Predict the value using learned linear regression parameters (w_0, w_1)\n",
    "\n",
    "        Arguments:\n",
    "        X -- data set of single feature\n",
    "\n",
    "        Returns:\n",
    "        Y_prediction -- predictions for all items in X\n",
    "        '''\n",
    "        ## Your code starts here ##\n",
    "        # Hint: You can use matrix/array operation. \n",
    "        # For example, if B is a matrix, 2 * B ends up with every item in matrix B being multiplied by 2\n",
    "        A = \n",
    "        ## Your code ends here ##\n",
    "        return A\n",
    "\n",
    "    \n",
    "    def get_weights(self):\n",
    "        weights = {\n",
    "            'w_1': self.w_1,\n",
    "            'w_0': self.w_0\n",
    "        }\n",
    "        return weights\n",
    "\n",
    "\n",
    "    def fit(self, X, Y, epochs=1, print_loss = True):\n",
    "        \"\"\"\n",
    "        This function optimizes w_0 and w_1 by running a gradient descent algorithm\n",
    "\n",
    "        Arguments:\n",
    "        X -- data of the single feature\n",
    "        Y -- true \"label\" vector \n",
    "        num_iterations -- number of iterations of the optimization loop\n",
    "        learning_rate -- learning rate of the gradient descent update rule\n",
    "        print_loss -- True to print the loss every 100 steps\n",
    "\n",
    "        Returns:\n",
    "        params -- dictionary containing the weights w_1 and bias w_0\n",
    "        grads -- dictionary containing the gradients of the weights and bias with respect to the loss function\n",
    "        losses -- list of all the losses computed during the optimization, this will be used to plot the learning curve.\n",
    "\n",
    "        Tips:\n",
    "        You need to finish the following steps:\n",
    "            1) Calculate the loss and the gradient for the current parameters. Use propagate().\n",
    "            2) Update the parameters using gradient descent rule for w_0 and w_1.\n",
    "        \"\"\"\n",
    "        clr = ['b', 'g', 'c', 'm', 'y', 'k'] #color for labeling lines\n",
    "        losses = []\n",
    "        for i in range(epochs):\n",
    "            ##Your code starts from here##\n",
    "            gradients, loss = \n",
    "\n",
    "            dw_1 = gradients['dw_1']\n",
    "            dw_0 = gradients['dw_0']\n",
    "\n",
    "            self.w_1 = \n",
    "            self.w_0 = \n",
    "            ##Your code ends here##\n",
    "\n",
    "            # Print the loss every 200 training examples\n",
    "            if print_loss and i % 200 == 0:\n",
    "                losses.append(loss)\n",
    "                print (\"At Epoch %i, the loss = %f; w_0 = %f; w_1 = %f\" %(i, loss, self.w_0, self.w_1))\n",
    "                plt.plot(X, self.predict(X), marker = 'o', color=clr[int((i/200) % len(clr))], label='Epoch' + str(i))\n",
    "            \n",
    "\n",
    "        \n",
    "        params = {\n",
    "            \"w_1\": self.w_1,\n",
    "            \"w_0\": self.w_0\n",
    "        }\n",
    "\n",
    "        gradients = {\n",
    "            \"dw_1\": dw_1,\n",
    "            \"dw_0\": dw_0\n",
    "        }\n",
    "\n",
    "        return params, gradients, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 2000, learning_rate = 0.5, print_loss = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array\n",
    "    Y_train -- training labels represented by a numpy array (vector)\n",
    "    X_test -- test set represented by a numpy array\n",
    "    Y_test -- test labels represented by a numpy array (vector)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_loss -- Set to true to print the loss every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = SimplifiedNetwork()\n",
    "    model.set_learning_rate(learning_rate)\n",
    "    parameters, grads, losses = model.fit(X_train, Y_train, epochs, print_loss)\n",
    "    \n",
    "    Y_prediction_test = model.predict(X_test)\n",
    "    Y_prediction_train = model.predict(X_train)\n",
    "    \n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    d = {\"losses\": losses,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w_1\" : model.get_weights()['w_1'], \n",
    "         \"w_0\" : model.get_weights()['w_0'],\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"epochs\": epochs}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', header=None)\n",
    "\n",
    "X_train = df[0].values.reshape(-1, 1).T\n",
    "Y_train = df[1].values.reshape(-1, 1).T\n",
    "\n",
    "df_test = pd.read_csv('test.csv', header=None)\n",
    "X_test = df_test[0].values.reshape(-1, 1).T\n",
    "Y_test = df_test[1].values.reshape(-1, 1).T\n",
    "\n",
    "\n",
    "plt.scatter(X_train, Y_train, color='r')\n",
    "\n",
    "##Your code starts from here##\n",
    "epochs = \n",
    "learning_rate =  \n",
    "##Your code ends here##\n",
    "\n",
    "d = Run_Experiment(X_train, Y_train, X_test, Y_test, epochs, learning_rate, print_loss = True)\n",
    "print(\"w_1 is \" + str(d['w_1']) + \" and w_0 is \" + str(d['w_0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Univariate_Linear_Regression_PartI_Incomplete.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FrXb0UJK2Ad",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Packages ##\n",
        "\n",
        "First, you need to import all the packages that you will need during this assignment. \n",
        "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
        "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
        "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFF1DAy1K2Ae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMxFXPsGK2Ai",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Problem Statement ##\n",
        "\n",
        "You are given a dataset containing:\n",
        "    - a training set for a linear function\n",
        "    - a test set for testing the learned hypothesis function\n",
        "    \n",
        "You will build a simple linear regression algorithm that can correctly identify the parameters of w0 and w1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6Ye-REoK2Aj",
        "colab_type": "text"
      },
      "source": [
        "## 3 - Forward and Backward propagation ##\n",
        "\n",
        "Forward Propagation:\n",
        "- You get X\n",
        "- You compute $h(x) = w_{1} * x + w_{0}$\n",
        "- You calculate the loss function:  $$L(W) = \\frac{1}{2m} \\sum_{i=1}^{n} \\left(h_{W}(x^{(i)})  - y^{(i)}\\right)^2$$. \n",
        "\n",
        "Here are the two formulas you will be using: \n",
        "\n",
        "$$ dw_1 = \\frac{\\partial L}{\\partial w_{1}} = \\frac{1}{m} \\sum_{i=1}^m (( w_{0} + w_{1} * x^{(i)} -y^{(i)}) * x^{(i)})\\tag{1}$$\n",
        "$$ dw_0 = \\frac{\\partial L}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^m (( w_{0} + w_{1} * x^{(i)} -y^{(i)}))\\tag{2}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aEC9RgEK2Aj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here we define a simplified neural network\n",
        "class SimplifiedNetwork:\n",
        "    def __init__(self):\n",
        "        #the weight associated with the single feature, a scalar\n",
        "        self.w_1 = 0 \n",
        "        #bias, a scalar\n",
        "        self.w_0 = 0\n",
        "        \n",
        "    def forward_back_propagation(self, X, Y):\n",
        "        \"\"\"\n",
        "        This function forward and backward propagation\n",
        "        Arguments:\n",
        "        X -- data of the series of single feature\n",
        "        Y -- true \"label\" vector\n",
        "\n",
        "        Return:\n",
        "        loss -- outcome of the loss function\n",
        "        gradient -- dictionary containing dw_1 and dw_0\n",
        "\n",
        "        \"\"\"\n",
        "        #number of training examples\n",
        "        m = X.shape[1]\n",
        "        mean = np.mean(X)\n",
        "        print(mean)\n",
        "\n",
        "        loss = 0 #loss\n",
        "        dw_1 = 0 #gredient of w_1\n",
        "        dw_0 = 0 #gredient of w_0\n",
        "\n",
        "        #iterate through all the training examples to\n",
        "        #    1. Calculate the loss\n",
        "        #    2. calcuate the accumulated gradient dw_1 and dw_0\n",
        "        for i in range(m):\n",
        "            #Your code starts from here\n",
        "            #Y_hat is output of the hypothesis function\n",
        "            # Y_hat = self.w_0 + np.dot(self.w_1, X[i])\n",
        "            Y_hat = self.w_0 + (self.w_1 * X[i])\n",
        "            loss += (Y_hat - Y[i]) ** 2\n",
        "            dw_1 += loss\n",
        "            dw_0 += (loss * X[i])\n",
        "            #Your code ends here\n",
        "\n",
        "        #Use the accumulated loss and gredients to calculate the averaged counterparts\n",
        "        loss = loss / (2 * m)\n",
        "        dw_1 = dw_1 / m\n",
        "        dw_0 = dw_0 /m\n",
        "\n",
        "\n",
        "        gradients = {\n",
        "            \"dw_1\": dw_1,\n",
        "            \"dw_0\": dw_0\n",
        "        }\n",
        "\n",
        "        return gradients, loss\n",
        "    \n",
        "    \n",
        "    #Function predict: \n",
        "    #   Predict the value using learned linear regression parameters (w_0, w_1)\n",
        "    def predict(self, x):\n",
        "        '''\n",
        "        Predict the value using learned linear regression parameters (w_0, w_1)\n",
        "\n",
        "        Arguments:\n",
        "        X -- data set of single feature\n",
        "\n",
        "        Returns:\n",
        "        Y_prediction -- predictions for all items in X\n",
        "        '''\n",
        "        ## Your code starts here ##\n",
        "        # Hint: You can use matrix/array operation. \n",
        "        # For example, if B is a matrix, 2 * B ends up with every item in matrix B being multiplied by 2\n",
        "\n",
        "        A = np.dot(2, x) # NOT KNOW WHAT GOES HERE ???\n",
        "        \n",
        "        ## Your code ends here ##\n",
        "        return A\n",
        "\n",
        "    \n",
        "    def get_weights(self):\n",
        "        weights = {\n",
        "            'w_1': self.w_1,\n",
        "            'w_0': self.w_0\n",
        "        }\n",
        "        return weights\n",
        "\n",
        "\n",
        "    def fit(self, X, Y, epochs=1, learning_rate = 0.01, print_loss = True):\n",
        "        \"\"\"\n",
        "        This function optimizes w_1 and w_0 by running a gradient descent algorithm\n",
        "\n",
        "        Arguments:\n",
        "        X -- data of the single feature\n",
        "        Y -- true \"label\" vector (Targeted output I think)\n",
        "        num_iterations -- number of iterations of the optimization loop\n",
        "        learning_rate -- learning rate of the gradient descent update rule\n",
        "        print_loss -- True to print the loss every 100 steps\n",
        "\n",
        "        Returns:\n",
        "        params -- dictionary containing the weights w_1 and bias w_0\n",
        "        grads -- dictionary containing the gradients of the weights and bias with respect to the loss function\n",
        "        losss -- list of all the losss computed during the optimization, this will be used to plot the learning curve.\n",
        "\n",
        "        Tips:\n",
        "        You need to finish the following steps:\n",
        "            1) Calculate the loss and gradients for the current parameters. Use forward_back_propagation().\n",
        "            2) Update the parameters using gradient descent rule for w_0 and w_1.\n",
        "        \"\"\"\n",
        "        losss = []\n",
        "        for i in range(epochs):\n",
        "            ##Your code starts from here##\n",
        "            gradients, loss = self.forward_back_propagation(X, Y)\n",
        "\n",
        "            dw_1 = gradients['dw_1']\n",
        "            dw_0 = gradients['dw_0']\n",
        "\n",
        "            self.w_1 = self.w_1 - (learning_rate * dw_1)\n",
        "            self.w_0 = self.w_0 - (learning_rate * dw_0)\n",
        "            ##Your code ends here##\n",
        "\n",
        "\n",
        "            # Print the loss every 100 training examples\n",
        "            if print_loss and i % 100 == 0:\n",
        "                losss.append(loss)\n",
        "                print (\"loss after iteration %i: %f\" %(i, loss))\n",
        "\n",
        "            \n",
        "\n",
        "        params = {\n",
        "            \"w_1\": self.w_1,\n",
        "            \"w_0\": self.w_0\n",
        "        }\n",
        "\n",
        "        gradients = {\n",
        "            \"dw_1\": dw_1,\n",
        "            \"dw_0\": dw_0\n",
        "        }\n",
        "\n",
        "        return params, gradients, losss"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW2-ru9xK2Am",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 2000, learning_rate = 0.5, print_loss = False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array\n",
        "    Y_train -- training labels represented by a numpy array (vector)\n",
        "    X_test -- test set represented by a numpy array\n",
        "    Y_test -- test labels represented by a numpy array (vector)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_loss -- Set to true to print the loss every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    model = SimplifiedNetwork()\n",
        "    parameters, grads, losses = model.fit(X_train, Y_train, epochs, learning_rate, print_loss)\n",
        "    \n",
        "    Y_prediction_test = model.predict(X_test)\n",
        "    Y_prediction_train = model.predict(X_train)\n",
        "    \n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    d = {\"losses\": losses,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w_1\" : model.get_weights()['w_1'], \n",
        "         \"w_0\" : model.get_weights()['w_0'],\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"epochs\": epochs}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H-grOqfK2Ao",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "3e1ed134-c954-407b-a6e4-2e8f990d2bf8"
      },
      "source": [
        "df = pd.read_csv('train.csv', header=None)\n",
        "\n",
        "X_train = df[0].values.reshape(-1, 1).T\n",
        "Y_train = df[1].values.reshape(-1, 1).T\n",
        "\n",
        "df_test = pd.read_csv('test.csv', header=None)\n",
        "X_test = df_test[0].values.reshape(-1, 1).T\n",
        "Y_test = df_test[1].values.reshape(-1, 1).T\n",
        "\n",
        "\n",
        "plt.scatter(X_train, Y_train)\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7XsbJAPK2Ar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "346d2baa-3111-4b5a-c481-bb1a7ac108bd"
      },
      "source": [
        "d = Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 1000, learning_rate = 0.01, print_loss = True)\n",
        "print(\"w_1 is \" + str(d['w_1']) + \" and w_0 is \" + str(d['w_0']))\n",
        "\n",
        "# Plot learning curve (with losses)\n",
        "losses = np.squeeze(d['losses'])\n",
        "plt.plot(losses)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.1038628\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-24795940a688>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRun_Experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"w_1 is \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'w_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" and w_0 is \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'w_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Plot learning curve (with losses)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'losses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-949993269926>\u001b[0m in \u001b[0;36mRun_Experiment\u001b[0;34m(X_train, Y_train, X_test, Y_test, epochs, learning_rate, print_loss)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimplifiedNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mY_prediction_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-74d3bfebca14>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, epochs, learning_rate, print_loss)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m##Your code starts from here##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_back_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mdw_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dw_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-74d3bfebca14>\u001b[0m in \u001b[0;36mforward_back_propagation\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# Y_hat = self.w_0 + np.dot(self.w_1, X[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mY_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mdw_1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mdw_0\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87APmdteK2At",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
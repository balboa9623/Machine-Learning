{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "LR_Multivariate_PartIII_incomplete.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ijcv3K3ufXK",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Packages ##\n",
        "\n",
        "First, you need to import all the packages that you will need during this assignment. \n",
        "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
        "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
        "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
        "- [jdc](https://alexhagen.github.io/jdc/) : Jupyter magic that allows defining classes over multiple jupyter notebook cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9NvYXOEusGk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e6a767d0-cc71-4adb-b2f3-25e8fb62a175"
      },
      "source": [
        "!pip install jdc"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jdc in /usr/local/lib/python3.6/dist-packages (0.0.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGgx8jS5ufXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import jdc\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plzs9_wiufXO",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Problem Statement ##\n",
        "\n",
        "You will create a neural network class - MultivariateNetwork:\n",
        "    - initialize parameters, such as weights, learning rate, etc.\n",
        "    - implement the gredient descent algorithm\n",
        "    - implement the predict function to make predictions for new data sets\n",
        "    - implement the normalization function\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jji0CuieufXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultivariateNetwork():\n",
        "    def __init__(self, num_of_features=1, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        This function creates a vector of zeros of shape (num_of_features, 1) for W and initializes w_0 to 0.\n",
        "\n",
        "        Argument:\n",
        "        num_of_features -- size of the W vector, i.e., the number of features, excluding the bias\n",
        "        \n",
        "        \"\"\"\n",
        "        # n is the number of features\n",
        "        self.n = num_of_features\n",
        "        # alpha is the learning rate\n",
        "        self.alpha = learning_rate\n",
        "\n",
        "        ### START YOUR CODE HERE ### \n",
        "        #initialize self.W and self.w_0 to be 0's\n",
        "        self.W = np.zeros(shape=(self.n, 1))\n",
        "        self.w_0 = 0\n",
        "        ### YOUR CODE ENDS ###\n",
        "        assert(self.W.shape == (self.n, 1))\n",
        "        assert(isinstance(self.w_0, float) or isinstance(self.w_0, int))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4TmyvZZufXS",
        "colab_type": "text"
      },
      "source": [
        "## 3 - Gradient Descent ##\n",
        "\n",
        "Forward Propagation:\n",
        "- You get X\n",
        "- You compute $h_{W}(X) = W^T * X + w_{0}\\tag{1}$\n",
        "- You calculate the cost function:  $$L(W) = \\frac{1}{2m} \\sum_{i=1}^{n} \\left(h_{W}(x^{(i)})  - y^{(i)}\\right)^2\\tag{2}$$. \n",
        "\n",
        "Here are the two formulas you will be using: \n",
        "\n",
        "$$ dw_{j} =\\frac{\\partial L}{\\partial w_{j}} = \\frac{1}{m} \\sum_{i=1}^m (( h_{W}(x^{(i)}) -y^{(i)}) * x_{j}^{(i)})\\tag{3}$$\n",
        "$$ dw_{0} = \\frac{\\partial L}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^m (h_{W}(x^{(i)}) -y^{(i)})\\tag{4}$$\n",
        "\n",
        "The weights will be updated:\n",
        "$$ w_{j} = w_{j} - {\\alpha} * \\frac{\\partial L}{\\partial w_{j}}\\tag{5}$$\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3khVDmiufXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%add_to MultivariateNetwork\n",
        "def fit(self, X, Y, epochs=1000, print_loss=True):\n",
        "    \"\"\"\n",
        "    This function implements the Gradient Descent Algorithm\n",
        "    Arguments:\n",
        "    X -- training data matrix: each column is a training example. \n",
        "            The number of columns is equal to the number of training examples\n",
        "    Y -- true \"label\" vector: shape (1, m)\n",
        "    epochs --\n",
        "\n",
        "    Return:\n",
        "    params -- dictionary containing weights\n",
        "    losses -- loss values of every 100 epochs\n",
        "    grads -- dictionary containing dw and dw_0\n",
        "    \"\"\"\n",
        "    losses = []\n",
        "\n",
        "    # print(\"W.T.shape = \", self.W.T.shape)\n",
        "    # print(\"X.shape = \", X.shape)\n",
        "    # print(\"Y.shape = \", Y.shape)\n",
        "    # print(\"W.shape = \", self.W.shape)\n",
        "\n",
        "    # Get the number of training examples\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    for i in range(epochs):\n",
        "\n",
        "      ### START YOUR CODE HERE ### \n",
        "      # Calculate the hypothesis outputs Y_hat (≈ 1 line of code)\n",
        "      Y_hat = np.dot(self.W.T, X) + self.w_0 #correct \n",
        "      \n",
        "      # Calculate loss (≈ 1 line of code)\n",
        "      loss = (1/(2*m)) * np.dot((Y_hat - Y), (Y_hat - Y).T) #correct\n",
        "      \n",
        "      # Calculate the gredients for W and w_0 (≈ 2 lines of code)\n",
        "      dw = np.sum((Y_hat - Y) * X)\n",
        "      dw_0 = np.sum(Y_hat - Y)\n",
        "      \n",
        "      dw /= m\n",
        "      dw_0 /= m\n",
        "\n",
        "      # Weight updates (≈ 2 lines of code)\n",
        "      self.W -= self.alpha * dw\n",
        "      self.w_0 -= self.alpha * dw_0\n",
        "      ### YOUR CODE ENDS ###\n",
        "      \n",
        "      if((i % 100) == 0):\n",
        "        losses.append(loss)\n",
        "        # Print the cost every 100 training examples\n",
        "        if print_loss:\n",
        "          print (\"Cost after iteration %i: %f\" %(i, loss))\n",
        "\n",
        "    params = {\n",
        "        \"W\": self.W,\n",
        "        \"w_0\": self.w_0\n",
        "    }\n",
        "\n",
        "    grads = {\n",
        "        \"dw\": dw,\n",
        "        \"dw_0\": dw_0\n",
        "    }\n",
        "\n",
        "    return params, grads, losses"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHaJMIttufXV",
        "colab_type": "text"
      },
      "source": [
        "### Make Predictions ###\n",
        "The predicted output is calculated as $h_{W}(X) = W^T * X + b$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbowzB1PufXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%add_to MultivariateNetwork\n",
        "def predict(self, X):\n",
        "    '''\n",
        "    Predict the actual values using learned parameters (self.W, self.w_0)\n",
        "\n",
        "    Arguments:\n",
        "    X -- data of size (n x m)\n",
        "\n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions for the examples in X\n",
        "    '''\n",
        "    m = X.shape[1]\n",
        "    # print(\"W.T shape = \", self.W.T.shape)\n",
        "    # print(\"X shape = \", X.shape)\n",
        "    \n",
        "    Y_prediction = np.zeros((1,m))\n",
        "\n",
        "    # Compute the actual values (≈ 1 line of code)\n",
        "    ### START YOUR CODE HERE ### \n",
        "    Y_prediction = np.dot(self.W.T, X) + self.w_0 #see fit function\n",
        "    #Y_prediction = self.W.T * X + self.w_0\n",
        "    #Y_prediction = self.W * X + self.w_0\n",
        "\n",
        "    ### YOUR CODE ENDS ###\n",
        "\n",
        "    return Y_prediction"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9ooB8aDufXY",
        "colab_type": "text"
      },
      "source": [
        "### Feature Scaling ###\n",
        "Here you normalize features using:\n",
        "$ \\frac{x_{i} - mean}{\\sigma}$, where $\\sigma$ is the standard deviation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9l8H0sSufXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%add_to MultivariateNetwork\n",
        "def normalize(self, matrix):\n",
        "    '''\n",
        "    matrix: the matrix that needs to be normalized. Note that each column represents a training example. \n",
        "         The number of columns is the the number of training examples\n",
        "    '''\n",
        "    # Calculate mean for each feature\n",
        "    # Pay attention to the value of axis = ?\n",
        "    # set keepdims=True to avoid rank-1 array\n",
        "    \n",
        "\n",
        "    ### START YOUR CODE HERE ### \n",
        "    # calculate mean (1 line of code)\n",
        "    mean = np.mean(matrix) #fix along row \n",
        "    \n",
        "    # calculate standard deviation (1 line of code)\n",
        "    # axis = 0 => column\n",
        "    # axis = 1 => row\n",
        "    std = np.std(matrix, axis=1, keepdims=True) # axis = ! #fix axis\n",
        "    \n",
        "    # normalize the matrix based on mean and std\n",
        "    matrix = (matrix - mean) / std\n",
        "    ### YOUR CODE ENDS ###\n",
        "\n",
        "    return matrix"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK5fzG7sufXb",
        "colab_type": "text"
      },
      "source": [
        "### Run the Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJB1AT-XufXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 2000, learning_rate = 0.5, print_loss = False):\n",
        "    \"\"\"\n",
        "    Builds the multivariate linear regression model by calling the function you've implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array \n",
        "    Y_train -- training labels represented by a numpy array (vector) \n",
        "    X_test -- test set represented by a numpy array\n",
        "    Y_test -- test labels represented by a numpy array (vector)\n",
        "    epochs -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_loss -- Set to true to print the cost every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    num_of_features = X_train.shape[0]\n",
        "    model = MultivariateNetwork(num_of_features, learning_rate)\n",
        "\n",
        "    # normalize\n",
        "    X_train = model.normalize(X_train)\n",
        "    Y_train = model.normalize(Y_train)\n",
        "\n",
        "    X_test = model.normalize(X_test)\n",
        "    Y_test = model.normalize(Y_test)\n",
        "    \n",
        "    \n",
        "    ### START YOUR CODE HERE ###\n",
        "    # Obtain the parameters, gredients, and losses by calling a model's method (≈ 1 line of code)\n",
        "    parameters, grads, losses = model.fit(X_train, Y_train, epochs, print_loss)\n",
        "    ### YOUR CODE ENDS ###\n",
        "    \n",
        "    ### START YOUR CODE HERE ###\n",
        "    # Predict test/train set examples (≈ 2 lines of code)\n",
        "    print(\"Y_Test shape = \", Y_test.shape)\n",
        "    Y_prediction_test = model.predict(X_test)\n",
        "    Y_prediction_train = model.predict(X_train)\n",
        "    ### YOUR CODE ENDS ###\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)/Y_train) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)/Y_test) * 100))\n",
        "\n",
        "    W = parameters['W']\n",
        "    w_0 = parameters['w_0']\n",
        "    print(\"W is \" + str(W))\n",
        "    print(\"w_0 is \" + str(w_0))\n",
        "    \n",
        "    d = {\"losses\": losses,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"W\" : W, \n",
        "         \"w_0\" : w_0,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"epochs\": epochs}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1IWwMQLufXf",
        "colab_type": "text"
      },
      "source": [
        "### Load Data and Start the Learning Process ###\n",
        "You can change num_iterations and learning_rate to see the learning process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg5Xoq_8ufXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('prj2house.csv', header=None)\n",
        "X_train = df[[0, 1]].values.T\n",
        "Y_train = df[2].values.reshape(-1, 1).T\n",
        "\n",
        "\n",
        "df_test = pd.read_csv('prj2house_test.csv', header=None)\n",
        "X_test = df_test[[0, 1]].values.T\n",
        "Y_test = df_test[2].values.reshape(-1, 1).T #one row by n columns "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TolV0RoufXi",
        "colab_type": "text"
      },
      "source": [
        "### Plot the learning curve ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAU96EG-ufXi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "outputId": "5234e9fe-c4d1-4240-cbd2-8656998c6266"
      },
      "source": [
        "d = Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 1000, learning_rate = 0.01, print_loss = True)\n",
        "\n",
        "# Plot learning curve (with costs)\n",
        "losses = np.squeeze(d['losses'])\n",
        "plt.plot(losses)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epochs (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.500000\n",
            "Cost after iteration 100: nan\n",
            "Cost after iteration 200: nan\n",
            "Cost after iteration 300: nan\n",
            "Cost after iteration 400: nan\n",
            "Cost after iteration 500: nan\n",
            "Cost after iteration 600: nan\n",
            "Cost after iteration 700: nan\n",
            "Cost after iteration 800: nan\n",
            "Cost after iteration 900: nan\n",
            "Y_Test shape =  (1, 2)\n",
            "train accuracy: nan %\n",
            "test accuracy: nan %\n",
            "W is [[nan]\n",
            " [nan]]\n",
            "w_0 is nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: RuntimeWarning: overflow encountered in multiply\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:90: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCOn8LZxHn2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "LR_Multivariate_incomplete.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1LSf1fPgcwv",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Packages ##\n",
        "\n",
        "First, you need to import all the packages that you will need during this assignment. \n",
        "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
        "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
        "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
        "- [jdc](https://alexhagen.github.io/jdc/) : Jupyter magic that allows defining classes over multiple jupyter notebook cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eacRsNjAgv3h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c094c0f9-64cb-4f5b-91fd-8530ae34e830"
      },
      "source": [
        "!pip install jdc"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jdc in /usr/local/lib/python3.6/dist-packages (0.0.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwOxHKxVgcww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import jdc\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90fl1zvxgcw0",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Problem Statement ##\n",
        "\n",
        "You will create a neural network class - MultivariateNetwork:\n",
        "    - initialize parameters, such as weights, learning rate, etc.\n",
        "    - implement the gredient descent algorithm\n",
        "    - implement the predict function to make predictions for new data sets\n",
        "    - implement the normalization function\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxe0i-9pgcw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultivariateNetwork():\n",
        "    def __init__(self, num_of_features=1, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        This function creates a vector of zeros of shape (num_of_features, 1) for W and initializes w_0 to 0.\n",
        "\n",
        "        Argument:\n",
        "        num_of_features -- size of the W vector, i.e., the number of features, excluding the bias\n",
        "        \n",
        "        \"\"\"\n",
        "        # n is the number of features\n",
        "        self.n = num_of_features\n",
        "        # alpha is the learning rate\n",
        "        self.alpha = learning_rate\n",
        "        \n",
        "        ### START YOUR CODE HERE ### \n",
        "        #initialize self.W and self.w_0 to be 0's\n",
        "        # self.W = 0\n",
        "        self.W = np.zeros(shape=(self.n, 1))\n",
        "        self.w_0 = 0 \n",
        "\n",
        "        ### YOUR CODE ENDS ###\n",
        "        assert(self.W.shape == (self.n, 1))\n",
        "        assert(isinstance(self.w_0, float) or isinstance(self.w_0, int))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URnNXoRRgcw3",
        "colab_type": "text"
      },
      "source": [
        "## 3 - Gradient Descent ##\n",
        "\n",
        "Forward Propagation:\n",
        "- You get X\n",
        "- You compute $h_{W}(X) = W^T * X + w_{0}\\tag{1}$\n",
        "- You calculate the cost function:  $$L(W) = \\frac{1}{2m} \\sum_{i=1}^{n} \\left(h_{W}(x^{(i)})  - y^{(i)}\\right)^2\\tag{2}$$. \n",
        "\n",
        "Here are the two formulas you will be using: \n",
        "\n",
        "$$ dw_{j} =\\frac{\\partial L}{\\partial w_{j}} = \\frac{1}{m} \\sum_{i=1}^m (( h_{W}(x^{(i)}) -y^{(i)}) * x_{j}^{(i)})\\tag{3}$$\n",
        "$$ dw_{0} = \\frac{\\partial L}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^m (h_{W}(x^{(i)}) -y^{(i)})\\tag{4}$$\n",
        "\n",
        "The weights will be updated:\n",
        "$$ w_{j} = w_{j} - {\\alpha} * \\frac{\\partial L}{\\partial w_{j}}\\tag{5}$$\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayTsJbIigcw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%add_to MultivariateNetwork\n",
        "def fit(self, X, Y, epochs=1000, print_loss=True):\n",
        "    \"\"\"\n",
        "    This function implements the Gradient Descent Algorithm\n",
        "    Arguments:\n",
        "    X -- training data matrix: each column is a training example. \n",
        "            The number of columns is equal to the number of training examples\n",
        "    Y -- true \"label\" vector: shape (1, m)\n",
        "    epochs --\n",
        "\n",
        "    Return:\n",
        "    params -- dictionary containing weights\n",
        "    losses -- loss values of every 100 epochs\n",
        "    grads -- dictionary containing dw and dw_0\n",
        "    \"\"\"\n",
        "    losses = []\n",
        "    print(\"X = \", X, \"\\n\\n\")\n",
        "    print(\"Y = \", Y)\n",
        "    \n",
        "    W_T = np.transpose(self.W)\n",
        "    for i in range(epochs):\n",
        "        # Get the number of training examples\n",
        "        m = X.shape[1]\n",
        "\n",
        "        ### START YOUR CODE HERE ### \n",
        "        # Calculate the hypothesis outputs Y_hat (≈ 1 line of code)\n",
        "        Y_hat = self.w_0 + (W_T * X[i])\n",
        "        \n",
        "        # Calculate loss (≈ 1 line of code)\n",
        "        # loss = np.dot((Y_hat - Y[i]), np.transpose((Y_hat - Y[i])))\n",
        "        loss = np.dot((Y_hat - Y[i]), (Y_hat - Y[i]).T)\n",
        "        \n",
        "        # Calculate the gredients for W and w_0 (≈ 2 lines of code)\n",
        "        dw = np.sum((Y_hat - Y[i])* X[i]) # don't know about X (Xj(i))\n",
        "        dw_0 = np.sum(Y_hat - Y[i])\n",
        "\n",
        "        # Weight updates (≈ 2 lines of code)\n",
        "        self.W = self.w_0 - (self.alpha * dw_0) # double check these two\n",
        "        self.w_0 = self.w_0 - (self.alpha * dw_0)\n",
        "        ### YOUR CODE ENDS ###\n",
        "\n",
        "        if((i % 100) == 0):\n",
        "            losses.append(loss)\n",
        "             # Print the cost every 100 training examples\n",
        "            if print_loss:\n",
        "                print (\"Cost after iteration %i: %f\" %(i, loss))\n",
        "\n",
        "\n",
        "    params = {\n",
        "        \"W\": self.W,\n",
        "        \"w_0\": self.w_0\n",
        "    }\n",
        "\n",
        "    grads = {\n",
        "        \"dw\": dw,\n",
        "        \"dw_0\": dw_0\n",
        "    }\n",
        "\n",
        "    return params, grads, losses"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jknpOj98gcw7",
        "colab_type": "text"
      },
      "source": [
        "### Make Predictions ###\n",
        "The predicted output is calculated as $h_{W}(X) = W^T * X + b$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr7D1C9agcw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%add_to MultivariateNetwork\n",
        "def predict(self, X):\n",
        "    '''\n",
        "    Predict the actual values using learned parameters (self.W, self.w_0)\n",
        "\n",
        "    Arguments:\n",
        "    X -- data of size (n x m)\n",
        "\n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions for the examples in X\n",
        "    '''\n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "\n",
        "    # Compute the actual values (≈ 1 line of code)\n",
        "    ### START YOUR CODE HERE ### \n",
        "    # Y_prediction = np.dot(np.transpose(self.W), X) + self.w_0\n",
        "    Y_prediction = np.dot(self.W.T, X) + self.w_0\n",
        "    ### YOUR CODE ENDS ###\n",
        "\n",
        "    return Y_prediction"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95tu-hndgcw-",
        "colab_type": "text"
      },
      "source": [
        "### Feature Scaling ###\n",
        "Here you normalize features using:\n",
        "$ \\frac{x_{i} - mean}{\\sigma}$, where $\\sigma$ is the standard deviation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKoSqTBngcw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%add_to MultivariateNetwork\n",
        "def normalize(self, matrix):\n",
        "    '''\n",
        "    matrix: the matrix that needs to be normalized. Note that each column represents a training example. \n",
        "         The number of columns is the the number of training examples\n",
        "    '''\n",
        "    # Calculate mean for each feature\n",
        "    # Pay attention to the value of axis = ?\n",
        "    # set keepdims=True to avoid rank-1 array\n",
        "    \n",
        "    ### START YOUR CODE HERE ### \n",
        "    # calculate mean (1 line of code)\n",
        "    print(\"matrix = \", matrix)\n",
        "    mean = np.mean(matrix)\n",
        "    print(\"mean = \", mean)\n",
        "\n",
        "    # calculate standard deviation (1 line of code)\n",
        "    std = np.std(matrix, axis=1, keepdims=True) # axis = ?\n",
        "    print(std)\n",
        "\n",
        "    # normalize the matrix based on mean and std\n",
        "    matrix = (X - mean) / std\n",
        "    print(\"matrix after normalization = \", matrix)\n",
        "    ### YOUR CODE ENDS ###\n",
        "\n",
        "    return matrix"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB31vTHCgcxB",
        "colab_type": "text"
      },
      "source": [
        "### Run the Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH3_CClHgcxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 2000, learning_rate = 0.5, print_loss = False):\n",
        "    \"\"\"\n",
        "    Builds the multivariate linear regression model by calling the function you've implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array \n",
        "    Y_train -- training labels represented by a numpy array (vector) \n",
        "    X_test -- test set represented by a numpy array\n",
        "    Y_test -- test labels represented by a numpy array (vector)\n",
        "    epochs -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_loss -- Set to true to print the cost every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    num_of_features = X_train.shape[0]\n",
        "    model = MultivariateNetwork(num_of_features, learning_rate)\n",
        "    \n",
        "    \n",
        "    ### START YOUR CODE HERE ###\n",
        "    # Obtain the parameters, gredients, and losses by calling a model's method (≈ 1 line of code)\n",
        "    parameters, grads, losses = fit(X_train, Y_train, epochs, print_loss)\n",
        "    ### YOUR CODE ENDS ###\n",
        "    \n",
        "    ### START YOUR CODE HERE ###\n",
        "    # Predict test/train set examples (≈ 2 lines of code)\n",
        "    Y_prediction_test = self.predict(Y_test)\n",
        "    Y_prediction_train = self.predict(Y_train)\n",
        "    ### YOUR CODE ENDS ###\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)/Y_train) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)/Y_test) * 100))\n",
        "\n",
        "    W = parameters['W']\n",
        "    w_0 = parameters['w_0']\n",
        "    print(\"W is \" + str(W))\n",
        "    print(\"w_0 is \" + str(w_0))\n",
        "    \n",
        "    d = {\"losses\": losses,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"W\" : W, \n",
        "         \"w_0\" : w_0,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"epochs\": epochs}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH4noPaygcxE",
        "colab_type": "text"
      },
      "source": [
        "### Load Data and Start the Learning Process ###\n",
        "You can change num_iterations and learning_rate to see the learning process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Onfwvq7ogcxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('prj2data1.csv', header=None)\n",
        "X_train = df[[0, 1]].values.T\n",
        "Y_train = df[2].values.reshape(-1, 1).T\n",
        "\n",
        "\n",
        "df_test = pd.read_csv('prj2data1_test.csv', header=None)\n",
        "X_test = df_test[[0, 1]].values.T\n",
        "Y_test = df_test[2].values.reshape(-1, 1).T"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS5ViCNGgcxH",
        "colab_type": "text"
      },
      "source": [
        "### Plot the learning curve ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45jZ4jw9gcxH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "d24d4f1f-4f4f-43b8-8abe-83e5df338445"
      },
      "source": [
        "d = Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 1000, learning_rate = 0.01, print_loss = True)\n",
        "\n",
        "# Plot learning curve (with costs)\n",
        "losses = np.squeeze(d['losses'])\n",
        "plt.plot(losses)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epochs (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-35df2743305e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRun_Experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Plot learning curve (with costs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'losses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-1a4de287ac2a>\u001b[0m in \u001b[0;36mRun_Experiment\u001b[0;34m(X_train, Y_train, X_test, Y_test, epochs, learning_rate, print_loss)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m### START YOUR CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Obtain the parameters, gredients, and losses by calling a model's method (≈ 1 line of code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;31m### YOUR CODE ENDS ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'MultivariateNetwork' object has no attribute 'fit'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdtcB7bIgcxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
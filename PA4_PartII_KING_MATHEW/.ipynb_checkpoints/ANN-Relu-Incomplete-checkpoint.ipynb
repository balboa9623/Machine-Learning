{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First, you need to import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- [jdc](https://alexhagen.github.io/jdc/) : Jupyter magic that allows defining classes over multiple jupyter notebook cells.\n",
    "- plot: plot_decision_boundary is a utility function for plotting the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jdc\n",
    "from plot import plot_decision_boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Help Functions ##\n",
    "\n",
    "Implement the helper function of sigmoid: $sigmoid(Z) = \\frac{1}{1 + e^{-Z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or a matrix.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    ## YOUR CODE STARTS HERE ### \n",
    "    s = \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 The Neural Network Class ##\n",
    "\n",
    "You will implement a two-layer nerual network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_First_Awesome_NN():\n",
    "    def __init__(self, n_x, n_h, n_y = 1, learning_rate = 0.1, eta = 0):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "        n_x -- size of the input layer\n",
    "        n_h -- size of the hidden layer\n",
    "        n_y -- size of the output layer, i.e., 1 in this assignment\n",
    "        learning_rate -- the learning rate alpha\n",
    "        eta -- used in leaky relu\n",
    "\n",
    "        Define:\n",
    "                self.W1 -- weight matrix of shape (n_h, n_x)\n",
    "                self.b1 -- bias vector of shape (n_h, 1)\n",
    "                self.W2 -- weight matrix of shape (n_y, n_h)\n",
    "                self.b2 -- bias vector of shape (n_y, 1)\n",
    "        \"\"\"\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        self.alpha = learning_rate # learning rate\n",
    "        self.eta = eta\n",
    "        \n",
    "        np.random.seed(7) # The seed will make sure your output matches mine although the initialization is random.\n",
    "        \n",
    "        ### YOUR CODE STARTS HERE ### (≈ 4 lines of code)\n",
    "        # hint (1): you can use np.random.randn to initialize self.W1 and self.W2. \n",
    "        # hint (2): In order to keep the weights small, you'd better multiply the random weights by a small number ,e.g., 0.5 \n",
    "        # hint (3): biases can be initialized to be zeros\n",
    "        self.W1 = \n",
    "        self.b1 = \n",
    "        self.W2 = \n",
    "        self.b2 = \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert (self.W1.shape == (n_h, n_x)), \"Matrix shape incorrect: self.W1\"\n",
    "        assert (self.b1.shape == (n_h, 1)), \"Vector shape incorrect: self.b1\"\n",
    "        assert (self.W2.shape == (n_y, n_h)), \"Matrix shape incorrect: self.W2\"\n",
    "        assert (self.b2.shape == (n_y, 1)), \"Vector shape incorrect: self.b2\"        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Forward Propagation ###\n",
    "\n",
    "Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$\n",
    "$$Z^{[1]} =  W^{[1]} \\cdot X + b^{[1]}\\tag{1}$$ \n",
    "$$A^{[1]} =  leakyrelu(Z^{[1]})\\tag{2}$$ \n",
    "$$Z^{[2]} =  W^{[2]} \\cdot A^{[1]} + b^{[2]}\\tag{3}$$\n",
    "$$A^{[2]} =  sigmoid(Z^{[2]})\\tag{4}$$ \n",
    "\n",
    "- We will use the *** leaky relu activation function *** in the hidden layer:\n",
    "g(z) = a =  $\\begin{cases}\n",
    "      z & \\text{if } z > 0 \\\\\n",
    "      \\eta * z & \\text{otherwise}\n",
    "    \\end{cases}$  \n",
    "- if $\\eta = 0$, it becomes regular relu function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to My_First_Awesome_NN\n",
    "\n",
    "def forward_propagation(self, X):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "   \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Implement Forward Propagation to calculate A2\n",
    "    ### YOUR CODE STARTS HERE ### (≈ 4 lines of code)\n",
    "    Z1 = \n",
    "    #Remember to use leaky relu as the actiation function\n",
    "    A1 = \n",
    "    # Start to handle the output layer\n",
    "    Z2 = \n",
    "    #Remember to use sigmoid function in the output layer\n",
    "    A2 = \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Back Propagation ###\n",
    "\n",
    "Based on the cache obtained during forward propagation, please implement the backward propagation.\n",
    "<img src=\"images/gredient-descent.png\" style=\"width:400px;height:200px;\">\n",
    "\n",
    "- Hint:\n",
    "    - To compute $dZ^{[1]}$, you will need to compute $g^{'}(Z^{[1]})$. Since the activation function is leaky relu, \n",
    "    $g^{'}(z) =  \\begin{cases}\n",
    "      1 & \\text{if } z > 0 \\\\\n",
    "      \\eta  & \\text{otherwise}\n",
    "    \\end{cases}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to My_First_Awesome_NN\n",
    "def backward_propagation(self, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation\n",
    "    \n",
    "    Arguments:\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (n, m)\n",
    "    Y -- \"true\" labels vector of shape (1, m)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- a python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    ### YOUR CODE STARTS HERE ### (≈ 2 lines of code)\n",
    "    A1 = \n",
    "    Z1 = \n",
    "    A2 = \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    ### YOUR CODE STARTS HERE ### (≈ 6 lines of code, corresponding to 6 equations above)\n",
    "    dZ2 = \n",
    "    dW2 = \n",
    "    db2 = \n",
    "    # Calculate the derivative for leaky relu based on Z values. See the hint above\n",
    "    dg_Z1 = \n",
    "    # Calculate dZ1 based on the formula given above\n",
    "    dZ1 = \n",
    "    \n",
    "    dW1 = \n",
    "    db1 = \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert dZ2.shape == (self.n_y, m), \"Matrix shape incorrect: dZ2\"\n",
    "    assert dW2.shape == (self.n_y, self.n_h), \"Matrix shape incorrect: dW2\"\n",
    "    assert db2.shape == (self.n_y, 1), \"Vector shape incorrect: db2\"\n",
    "    assert dZ1.shape == (self.n_h, m), \"Matrix shape incorrect: dZ1\"\n",
    "    assert dW1.shape == (self.n_h, self.n_x), \"Matrix shape incorrect: dW1\"\n",
    "    assert db1.shape == (self.n_h, 1), \"Vector shape incorrect: db1\"\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Compute Loss ###\n",
    "The loss function:  $$L(W) = -\\frac{1}{m} (Y * \\log(A^{[2]}) + (1-Y) * \\log(1-A^{[2]}))\\tag{1}$$ \n",
    "- Hint: Here, * in the above equation is numpy.multiply(), i.e., element-wise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to My_First_Awesome_NN\n",
    "def compute_loss(self, A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    Returns:\n",
    "    loss -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[0] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    ### YOUR CODE STARTS HERE ### (≈ 1 to 2 lines of code)\n",
    "    # You can do this in two steps or one step according to your preference\n",
    "    logprobs = \n",
    "    loss = \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    loss = np.squeeze(loss)     # makes sure cost is the dimension we expect. \n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(loss, float))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Update Weights ###\n",
    "Apply the gredient descent weight update rule here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to My_First_Awesome_NN\n",
    "\n",
    "def update_parameters(self, grads):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    grads -- a python dictionary containing gradients \n",
    "    \n",
    "    \"\"\"\n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    ### YOUR CODE STARTS HERE ### (≈ 4 lines of code)\n",
    "    dW1 = \n",
    "    db1 = \n",
    "    dW2 = \n",
    "    db2 = \n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    ### YOUR CODE STARTS HERE ### (≈ 4 lines of code)\n",
    "    self.W1 = \n",
    "    self.b1 = \n",
    "    self.W2 = \n",
    "    self.b2 = \n",
    "    ### END CODE HERE ###\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 - Training ###\n",
    "Implement the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to My_First_Awesome_NN\n",
    "\n",
    "def fit(self, X, Y, num_epochs = 10000, print_loss=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (n, m)\n",
    "    Y -- labels of shape (1, m)\n",
    "    num_epochs -- Number of iterations in gradient descent iterations\n",
    "    print_loss -- if True, print the loss every 1000 iterations\n",
    "    \n",
    "    \"\"\"\n",
    "    assert(self.n_x == X.shape[0])\n",
    "    assert(self.n_y == Y.shape[0])\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_epochs):\n",
    "         \n",
    "        ### YOUR CODE STARTS HERE ### (≈ 4 lines of code)\n",
    "        # Forward propagation. Inputs: \"X\". Outputs: \"A2, cache\".\n",
    "        A2, cache = \n",
    "        \n",
    "        # Loss function. Inputs: \"A2, Y, parameters\". Outputs: \"loss\".\n",
    "        loss = \n",
    " \n",
    "        # Backpropagation. Inputs: \"cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = \n",
    " \n",
    "        # Last step: Gradient descent parameter update. Inputs: \" grads\".\n",
    "        \n",
    "        \n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_loss and i % 1000 == 0:\n",
    "            losses.append(loss)\n",
    "            print (\"Loss after iteration %i: %f\" %(i, loss))\n",
    "    \n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 - predict ###\n",
    "**Hint**: prediction =  $\\begin{cases}\n",
    "      1 & \\text{if } \\hat y > 0.5 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to My_First_Awesome_NN\n",
    "\n",
    "def predict(self, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "   ### YOUR CODE STARTS HERE ### (≈ 2 lines of code)\n",
    "    A2, cache = \n",
    "    \n",
    "    # hint: with numpy, you can apply a threshold to a vector, e.g., M > 0.5, and it will return a vector of true/false\n",
    "    predictions = \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiments ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Load and display data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data.txt', delimiter=',')\n",
    "X = data[:, :-1].T\n",
    "Y = data[:, -1].T\n",
    "plt.scatter(X[0, :], X[1, :], c=Y,s=40, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Display the shape of X, Y, and # of training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.reshape(1, -1)\n",
    "### YOUR CODE STARTS HERE ### (≈ 3 lines of code)\n",
    "shape_X = \n",
    "shape_Y = \n",
    "m =      # training set size\n",
    "### END CODE HERE ###\n",
    "\n",
    "print ('The shape of X is: ' + str(shape_X))\n",
    "print ('The shape of Y is: ' + str(shape_Y))\n",
    "print ('There are m = %d training examples!' % (m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Define the model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an ANN model with a n_h-dimensional hidden layer\n",
    "### YOUR EXPERIMENT STARTS HERE ###\n",
    "num_hidden_units = 4\n",
    "alpha = 0.5\n",
    "eta = 0\n",
    "num_of_epochs = 10000\n",
    "### END CODE HERE ###\n",
    "\n",
    "model = My_First_Awesome_NN(n_x=X.shape[0], n_h = num_hidden_units, n_y = 1, learning_rate=alpha, eta = eta)\n",
    "losses = model.fit(X, Y, num_epochs = num_of_epochs, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "plot_decision_boundary(lambda x: model.predict(x.T), X, Y)\n",
    "plt.title(\"Decision Boundary for hidden layer size \" + str(num_hidden_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curve\n",
    "losses = np.squeeze(losses)\n",
    "plt.plot(losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy\n",
    "predictions = model.predict(X)\n",
    "print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

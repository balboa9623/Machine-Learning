{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "LR_Multivariate_PartII.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ijcv3K3ufXK",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Packages ##\n",
        "\n",
        "First, you need to import all the packages that you will need during this assignment. \n",
        "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
        "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
        "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
        "- [jdc](https://alexhagen.github.io/jdc/) : Jupyter magic that allows defining classes over multiple jupyter notebook cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9NvYXOEusGk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a5202318-0c29-4f2b-bbd4-ae84f15e77ec"
      },
      "source": [
        "!pip install jdc"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jdc\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/cb/9afea749985eef20f3160e8826a531c7502e40c35a038dfe49b67726e9a0/jdc-0.0.9-py2.py3-none-any.whl\n",
            "Installing collected packages: jdc\n",
            "Successfully installed jdc-0.0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGgx8jS5ufXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import jdc\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plzs9_wiufXO",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Problem Statement ##\n",
        "\n",
        "You will create a neural network class - MultivariateNetwork:\n",
        "    - initialize parameters, such as weights, learning rate, etc.\n",
        "    - implement the gredient descent algorithm\n",
        "    - implement the predict function to make predictions for new data sets\n",
        "    - implement the normalization function\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jji0CuieufXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultivariateNetwork():\n",
        "    def __init__(self, num_of_features=1, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        This function creates a vector of zeros of shape (num_of_features, 1) for W and initializes w_0 to 0.\n",
        "\n",
        "        Argument:\n",
        "        num_of_features -- size of the W vector, i.e., the number of features, excluding the bias\n",
        "        \n",
        "        \"\"\"\n",
        "        # n is the number of features\n",
        "        self.n = num_of_features\n",
        "        # alpha is the learning rate\n",
        "        self.alpha = learning_rate\n",
        "\n",
        "        ### START YOUR CODE HERE ### \n",
        "        #initialize self.W and self.w_0 to be 0's\n",
        "        self.W = np.zeros(shape=(self.n, 1))\n",
        "        self.w_0 = 0\n",
        "        ### YOUR CODE ENDS ###\n",
        "        assert(self.W.shape == (self.n, 1))\n",
        "        assert(isinstance(self.w_0, float) or isinstance(self.w_0, int))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4TmyvZZufXS",
        "colab_type": "text"
      },
      "source": [
        "## 3 - Gradient Descent ##\n",
        "\n",
        "Forward Propagation:\n",
        "- You get X\n",
        "- You compute $h_{W}(X) = W^T * X + w_{0}\\tag{1}$\n",
        "- You calculate the cost function:  $$L(W) = \\frac{1}{2m} \\sum_{i=1}^{n} \\left(h_{W}(x^{(i)})  - y^{(i)}\\right)^2\\tag{2}$$. \n",
        "\n",
        "Here are the two formulas you will be using: \n",
        "\n",
        "$$ dw_{j} =\\frac{\\partial L}{\\partial w_{j}} = \\frac{1}{m} \\sum_{i=1}^m (( h_{W}(x^{(i)}) -y^{(i)}) * x_{j}^{(i)})\\tag{3}$$\n",
        "$$ dw_{0} = \\frac{\\partial L}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^m (h_{W}(x^{(i)}) -y^{(i)})\\tag{4}$$\n",
        "\n",
        "The weights will be updated:\n",
        "$$ w_{j} = w_{j} - {\\alpha} * \\frac{\\partial L}{\\partial w_{j}}\\tag{5}$$\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3khVDmiufXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%add_to MultivariateNetwork\n",
        "def fit(self, X, Y, epochs=1000, print_loss=True):\n",
        "    \"\"\"\n",
        "    This function implements the Gradient Descent Algorithm\n",
        "    Arguments:\n",
        "    X -- training data matrix: each column is a training example. \n",
        "            The number of columns is equal to the number of training examples\n",
        "    Y -- true \"label\" vector: shape (1, m)\n",
        "    epochs --\n",
        "\n",
        "    Return:\n",
        "    params -- dictionary containing weights\n",
        "    losses -- loss values of every 100 epochs\n",
        "    grads -- dictionary containing dw and dw_0\n",
        "    \"\"\"\n",
        "    losses = []\n",
        "\n",
        "    # print(\"W.T.shape = \", self.W.T.shape)\n",
        "    # print(\"X.shape = \", X.shape)\n",
        "    # print(\"Y.shape = \", Y.shape)\n",
        "    # print(\"W.shape = \", self.W.shape)\n",
        "\n",
        "    # Get the number of training examples\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    for i in range(epochs):\n",
        "\n",
        "      ### START YOUR CODE HERE ### \n",
        "      # Calculate the hypothesis outputs Y_hat (≈ 1 line of code)\n",
        "      Y_hat = np.dot(self.W.T, X) + self.w_0 #correct \n",
        "      \n",
        "      # Calculate loss (≈ 1 line of code)\n",
        "      loss = (1/(2*m)) * np.dot((Y_hat - Y), (Y_hat - Y).T) #correct\n",
        "      \n",
        "      # Calculate the gredients for W and w_0 (≈ 2 lines of code)\n",
        "      dw = np.sum((Y_hat - Y) * X)\n",
        "      dw_0 = np.sum(Y_hat - Y)\n",
        "      \n",
        "      dw /= m\n",
        "      dw_0 /= m\n",
        "\n",
        "      # Weight updates (≈ 2 lines of code)\n",
        "      self.W -= self.alpha * dw\n",
        "      self.w_0 -= self.alpha * dw_0\n",
        "      ### YOUR CODE ENDS ###\n",
        "      \n",
        "      if((i % 100) == 0):\n",
        "        losses.append(loss)\n",
        "        # Print the cost every 100 training examples\n",
        "        if print_loss:\n",
        "          print (\"Cost after iteration %i: %f\" %(i, loss))\n",
        "\n",
        "    params = {\n",
        "        \"W\": self.W,\n",
        "        \"w_0\": self.w_0\n",
        "    }\n",
        "\n",
        "    grads = {\n",
        "        \"dw\": dw,\n",
        "        \"dw_0\": dw_0\n",
        "    }\n",
        "\n",
        "    return params, grads, losses"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHaJMIttufXV",
        "colab_type": "text"
      },
      "source": [
        "### Make Predictions ###\n",
        "The predicted output is calculated as $h_{W}(X) = W^T * X + b$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbowzB1PufXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%add_to MultivariateNetwork\n",
        "def predict(self, X):\n",
        "    '''\n",
        "    Predict the actual values using learned parameters (self.W, self.w_0)\n",
        "\n",
        "    Arguments:\n",
        "    X -- data of size (n x m)\n",
        "\n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions for the examples in X\n",
        "    '''\n",
        "    m = X.shape[1]\n",
        "    # print(\"W.T shape = \", self.W.T.shape)\n",
        "    # print(\"X shape = \", X.shape)\n",
        "    \n",
        "    Y_prediction = np.zeros((1,m))\n",
        "\n",
        "    # Compute the actual values (≈ 1 line of code)\n",
        "    ### START YOUR CODE HERE ### \n",
        "    Y_prediction = np.dot(self.W.T, X) + self.w_0 #see fit function\n",
        "    #Y_prediction = self.W.T * X + self.w_0\n",
        "    #Y_prediction = self.W * X + self.w_0\n",
        "\n",
        "    ### YOUR CODE ENDS ###\n",
        "\n",
        "    return Y_prediction"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9ooB8aDufXY",
        "colab_type": "text"
      },
      "source": [
        "### Feature Scaling ###\n",
        "Here you normalize features using:\n",
        "$ \\frac{x_{i} - mean}{\\sigma}$, where $\\sigma$ is the standard deviation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9l8H0sSufXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%add_to MultivariateNetwork\n",
        "def normalize(self, matrix):\n",
        "    '''\n",
        "    matrix: the matrix that needs to be normalized. Note that each column represents a training example. \n",
        "         The number of columns is the the number of training examples\n",
        "    '''\n",
        "    # Calculate mean for each feature\n",
        "    # Pay attention to the value of axis = ?\n",
        "    # set keepdims=True to avoid rank-1 array\n",
        "    \n",
        "    ### START YOUR CODE HERE ### \n",
        "    # calculate mean (1 line of code)\n",
        "    mean = np.mean(matrix, axis = 1, keepdims=True) #fix along row \n",
        "    \n",
        "    # calculate standard deviation (1 line of code)\n",
        "    std = np.std(matrix, axis=1, keepdims=True) # axis = ! #fix axis\n",
        "    \n",
        "    # normalize the matrix based on mean and std\n",
        "    matrix = (matrix - mean) / std\n",
        "    ### YOUR CODE ENDS ###\n",
        "\n",
        "    return matrix"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK5fzG7sufXb",
        "colab_type": "text"
      },
      "source": [
        "### Run the Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJB1AT-XufXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 2000, learning_rate = 0.5, print_loss = False):\n",
        "    \"\"\"\n",
        "    Builds the multivariate linear regression model by calling the function you've implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array \n",
        "    Y_train -- training labels represented by a numpy array (vector) \n",
        "    X_test -- test set represented by a numpy array\n",
        "    Y_test -- test labels represented by a numpy array (vector)\n",
        "    epochs -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_loss -- Set to true to print the cost every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    num_of_features = X_train.shape[0]\n",
        "    model = MultivariateNetwork(num_of_features, learning_rate)\n",
        "    \n",
        "    \n",
        "    ### START YOUR CODE HERE ###\n",
        "    # Obtain the parameters, gredients, and losses by calling a model's method (≈ 1 line of code)\n",
        "    parameters, grads, losses = model.fit(X_train, Y_train, epochs, print_loss)\n",
        "    ### YOUR CODE ENDS ###\n",
        "    \n",
        "    ### START YOUR CODE HERE ###\n",
        "    # Predict test/train set examples (≈ 2 lines of code)\n",
        "    print(\"Y_Test shape = \", Y_test.shape)\n",
        "    Y_prediction_test = model.predict(X_test)\n",
        "    Y_prediction_train = model.predict(X_train)\n",
        "    ### YOUR CODE ENDS ###\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)/Y_train) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)/Y_test) * 100))\n",
        "\n",
        "    W = parameters['W']\n",
        "    w_0 = parameters['w_0']\n",
        "    print(\"W is \" + str(W))\n",
        "    print(\"w_0 is \" + str(w_0))\n",
        "    \n",
        "    d = {\"losses\": losses,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"W\" : W, \n",
        "         \"w_0\" : w_0,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"epochs\": epochs}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1IWwMQLufXf",
        "colab_type": "text"
      },
      "source": [
        "### Load Data and Start the Learning Process ###\n",
        "You can change num_iterations and learning_rate to see the learning process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg5Xoq_8ufXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('prj2data1.csv', header=None)\n",
        "X_train = df[[0, 1]].values.T\n",
        "Y_train = df[2].values.reshape(-1, 1).T\n",
        "\n",
        "\n",
        "df_test = pd.read_csv('prj2data1_test.csv', header=None)\n",
        "X_test = df_test[[0, 1]].values.T\n",
        "Y_test = df_test[2].values.reshape(-1, 1).T #one row by n columns "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TolV0RoufXi",
        "colab_type": "text"
      },
      "source": [
        "### Plot the learning curve ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAU96EG-ufXi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "outputId": "c641fb6f-ac28-4d07-9ffc-a0fd98c41278"
      },
      "source": [
        "d = Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 1000, learning_rate = 0.01, print_loss = True)\n",
        "\n",
        "# Plot learning curve (with costs)\n",
        "losses = np.squeeze(d['losses'])\n",
        "plt.plot(losses)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epochs (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 370.221964\n",
            "Cost after iteration 100: 1.886501\n",
            "Cost after iteration 200: 1.621351\n",
            "Cost after iteration 300: 1.426048\n",
            "Cost after iteration 400: 1.282193\n",
            "Cost after iteration 500: 1.176233\n",
            "Cost after iteration 600: 1.098186\n",
            "Cost after iteration 700: 1.040698\n",
            "Cost after iteration 800: 0.998354\n",
            "Cost after iteration 900: 0.967165\n",
            "Y_Test shape =  (1, 7)\n",
            "train accuracy: 94.66597295963908 %\n",
            "test accuracy: 96.76735093911383 %\n",
            "W is [[2.91416503]\n",
            " [2.91416503]]\n",
            "w_0 is 3.6633405286198526\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCOn8LZxHn2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}
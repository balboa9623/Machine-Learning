{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ijcv3K3ufXK"
   },
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First, you need to import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- [jdc](https://alexhagen.github.io/jdc/) : Jupyter magic that allows defining classes over multiple jupyter notebook cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c9NvYXOEusGk",
    "outputId": "228ae121-b2ae-4cf9-8411-50c3ce65dd07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jdc in c:\\users\\richu\\anaconda\\lib\\site-packages (0.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install jdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGgx8jS5ufXL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jdc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "plzs9_wiufXO"
   },
   "source": [
    "## 2 - Problem Statement ##\n",
    "\n",
    "You will create a neural network class - MultivariateNetwork:\n",
    "    - initialize parameters, such as weights, learning rate, etc.\n",
    "    - implement the gredient descent algorithm\n",
    "    - implement the predict function to make predictions for new data sets\n",
    "    - implement the normalization function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jji0CuieufXP"
   },
   "outputs": [],
   "source": [
    "class MultivariateNetwork():\n",
    "    def __init__(self, num_of_features=1, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        This function creates a vector of zeros of shape (num_of_features, 1) for W and initializes w_0 to 0.\n",
    "\n",
    "        Argument:\n",
    "        num_of_features -- size of the W vector, i.e., the number of features, excluding the bias\n",
    "        \n",
    "        \"\"\"\n",
    "        # n is the number of features\n",
    "        self.n = num_of_features\n",
    "        # alpha is the learning rate\n",
    "        self.alpha = learning_rate\n",
    "\n",
    "        ### START YOUR CODE HERE ### \n",
    "        #initialize self.W and self.w_0 to be 0's\n",
    "        self.W = np.zeros(shape=(self.n, 1))\n",
    "        self.w_0 = 0\n",
    "        ### YOUR CODE ENDS ###\n",
    "        assert(self.W.shape == (self.n, 1))\n",
    "        assert(isinstance(self.w_0, float) or isinstance(self.w_0, int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4TmyvZZufXS"
   },
   "source": [
    "## 3 - Gradient Descent ##\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $h_{W}(X) = W^T * X + w_{0}\\tag{1}$\n",
    "- You calculate the cost function:  $$L(W) = \\frac{1}{2m} \\sum_{i=1}^{n} \\left(h_{W}(x^{(i)})  - y^{(i)}\\right)^2\\tag{2}$$. \n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ dw_{j} =\\frac{\\partial L}{\\partial w_{j}} = \\frac{1}{m} \\sum_{i=1}^m (( h_{W}(x^{(i)}) -y^{(i)}) * x_{j}^{(i)})\\tag{3}$$\n",
    "$$ dw_{0} = \\frac{\\partial L}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^m (h_{W}(x^{(i)}) -y^{(i)})\\tag{4}$$\n",
    "\n",
    "The weights will be updated:\n",
    "$$ w_{j} = w_{j} - {\\alpha} * \\frac{\\partial L}{\\partial w_{j}}\\tag{5}$$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b3khVDmiufXS"
   },
   "outputs": [],
   "source": [
    "%%add_to MultivariateNetwork\n",
    "def fit(self, X, Y, epochs=1000, print_loss=True):\n",
    "    \"\"\"\n",
    "    This function implements the Gradient Descent Algorithm\n",
    "    Arguments:\n",
    "    X -- training data matrix: each column is a training example. \n",
    "            The number of columns is equal to the number of training examples\n",
    "    Y -- true \"label\" vector: shape (1, m)\n",
    "    epochs --\n",
    "\n",
    "    Return:\n",
    "    params -- dictionary containing weights\n",
    "    losses -- loss values of every 100 epochs\n",
    "    grads -- dictionary containing dw and dw_0\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "\n",
    "    print(\"W.T.shape = \", self.W.T.shape)\n",
    "    print(\"X.shape = \", X.shape)\n",
    "    print(\"Y.shape = \", Y.shape)\n",
    "    print(\"W.shape = \", self.W.shape)\n",
    "\n",
    "    # Get the number of training examples\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "      ### START YOUR CODE HERE ### \n",
    "      # Calculate the hypothesis outputs Y_hat (≈ 1 line of code)\n",
    "      # Y_hat = np.dot(self.W.T, X[i]) + self.w_0\n",
    "      #Y_hat = (self.W.T * X) # + self.w_0\n",
    "      Y_hat = np.dot(self.W.T, X)\n",
    "      \n",
    "      # Calculate loss (≈ 1 line of code)\n",
    "      loss = (1/(2*m)) * np.dot((Y_hat - Y), (Y_hat - Y).T)\n",
    "      \n",
    "      # Calculate the gredients for W and w_0 (≈ 2 lines of code)\n",
    "      # dw = np.dot(np.sum(Y_hat - Y), X)\n",
    "      dw = np.sum((Y_hat - Y) * X)\n",
    "      dw_0 = np.sum(Y_hat - Y)\n",
    "      \n",
    "      dw /= m\n",
    "      dw_0 /= m\n",
    "\n",
    "      # Weight updates (≈ 2 lines of code)\n",
    "      self.W -= self.alpha * dw\n",
    "      self.w_0 -= self.alpha * dw_0\n",
    "      ### YOUR CODE ENDS ###\n",
    "      \n",
    "      if((i % 100) == 0):\n",
    "        losses.append(loss)\n",
    "        # Print the cost every 100 training examples\n",
    "        if print_loss:\n",
    "          print (\"Cost after iteration %i: %f\" %(i, loss))\n",
    "\n",
    "    params = {\n",
    "        \"W\": self.W,\n",
    "        \"w_0\": self.w_0\n",
    "    }\n",
    "\n",
    "    grads = {\n",
    "        \"dw\": dw,\n",
    "        \"dw_0\": dw_0\n",
    "    }\n",
    "\n",
    "    return params, grads, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aHaJMIttufXV"
   },
   "source": [
    "### Make Predictions ###\n",
    "The predicted output is calculated as $h_{W}(X) = W^T * X + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DbowzB1PufXW"
   },
   "outputs": [],
   "source": [
    "%%add_to MultivariateNetwork\n",
    "def predict(self, X):\n",
    "    '''\n",
    "    Predict the actual values using learned parameters (self.W, self.w_0)\n",
    "\n",
    "    Arguments:\n",
    "    X -- data of size (n x m)\n",
    "\n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "\n",
    "    # Compute the actual values (≈ 1 line of code)\n",
    "    ### START YOUR CODE HERE ### \n",
    "    Y_prediction = np.dot(X.T, self.W.T) + self.w_0\n",
    "    ### YOUR CODE ENDS ###\n",
    "\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z9ooB8aDufXY"
   },
   "source": [
    "### Feature Scaling ###\n",
    "Here you normalize features using:\n",
    "$ \\frac{x_{i} - mean}{\\sigma}$, where $\\sigma$ is the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U9l8H0sSufXZ"
   },
   "outputs": [],
   "source": [
    "%%add_to MultivariateNetwork\n",
    "def normalize(self, matrix):\n",
    "    '''\n",
    "    matrix: the matrix that needs to be normalized. Note that each column represents a training example. \n",
    "         The number of columns is the the number of training examples\n",
    "    '''\n",
    "    # Calculate mean for each feature\n",
    "    # Pay attention to the value of axis = ?\n",
    "    # set keepdims=True to avoid rank-1 array\n",
    "    \n",
    "    ### START YOUR CODE HERE ### \n",
    "    # calculate mean (1 line of code)\n",
    "    mean = np.mean(matrix)\n",
    "    \n",
    "    # calculate standard deviation (1 line of code)\n",
    "    std = np.std(matrix, axis=0, keepdims=True) # axis = !\n",
    "    \n",
    "    # normalize the matrix based on mean and std\n",
    "    matrix = (X - mean) / std\n",
    "    ### YOUR CODE ENDS ###\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cK5fzG7sufXb"
   },
   "source": [
    "### Run the Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJB1AT-XufXc"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 2000, learning_rate = 0.5, print_loss = False):\n",
    "    \"\"\"\n",
    "    Builds the multivariate linear regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array \n",
    "    Y_train -- training labels represented by a numpy array (vector) \n",
    "    X_test -- test set represented by a numpy array\n",
    "    Y_test -- test labels represented by a numpy array (vector)\n",
    "    epochs -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_loss -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    num_of_features = X_train.shape[0]\n",
    "    model = MultivariateNetwork(num_of_features, learning_rate)\n",
    "    \n",
    "    \n",
    "    ### START YOUR CODE HERE ###\n",
    "    # Obtain the parameters, gredients, and losses by calling a model's method (≈ 1 line of code)\n",
    "    parameters, grads, losses = model.fit(X_train, Y_train, epochs, print_loss)\n",
    "    ### YOUR CODE ENDS ###\n",
    "    \n",
    "    ### START YOUR CODE HERE ###\n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    print(\"Y_Test shape = \", Y_test.shape)\n",
    "    Y_prediction_test = model.predict(Y_test)\n",
    "    Y_prediction_train = model.predict(Y_train)\n",
    "    ### YOUR CODE ENDS ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"Y_prediction_train.T.shape = \", Y_prediction_train.T.shape)\n",
    "    print(\"Y_train.T.shape = \", Y_train.T.shape)\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)/Y_train) * 100))\n",
    "#     print(\"train accuracy: {} %\".format(100 - np.mean(Y_prediction_train - Y_train)**2)\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)/Y_test) * 100))\n",
    "\n",
    "    W = parameters['W']\n",
    "    w_0 = parameters['w_0']\n",
    "    print(\"W is \" + str(W))\n",
    "    print(\"w_0 is \" + str(w_0))\n",
    "    \n",
    "    d = {\"losses\": losses,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"W\" : W, \n",
    "         \"w_0\" : w_0,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"epochs\": epochs}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i1IWwMQLufXf"
   },
   "source": [
    "### Load Data and Start the Learning Process ###\n",
    "You can change num_iterations and learning_rate to see the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dg5Xoq_8ufXf"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('prj2data1.csv', header=None)\n",
    "X_train = df[[0, 1]].values.T\n",
    "Y_train = df[2].values.reshape(-1, 1).T\n",
    "\n",
    "\n",
    "df_test = pd.read_csv('prj2data1_test.csv', header=None)\n",
    "X_test = df_test[[0, 1]].values.T\n",
    "Y_test = df_test[2].values.reshape(-1, 1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TolV0RoufXi"
   },
   "source": [
    "### Plot the learning curve ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "rAU96EG-ufXi",
    "outputId": "61e2043d-f2fa-419d-aaf1-58dc267b28a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.T.shape =  (1, 2)\n",
      "X.shape =  (2, 1000)\n",
      "Y.shape =  (1, 1000)\n",
      "W.shape =  (2, 1)\n",
      "Cost after iteration 0: 370.221965\n",
      "Cost after iteration 100: 2.499101\n",
      "Cost after iteration 200: 2.498708\n",
      "Cost after iteration 300: 2.498708\n",
      "Cost after iteration 400: 2.498708\n",
      "Cost after iteration 500: 2.498708\n",
      "Cost after iteration 600: 2.498708\n",
      "Cost after iteration 700: 2.498708\n",
      "Cost after iteration 800: 2.498708\n",
      "Cost after iteration 900: 2.498708\n",
      "Y_Test shape =  (1, 7)\n",
      "Y_prediction_train.T.shape =  (2, 1000)\n",
      "Y_train.T.shape =  (1000, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1000,2) (1,1000) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-cfc422668c59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRun_Experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Plot learning curve (with costs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'losses'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-102-6e8b55a07537>\u001b[0m in \u001b[0;36mRun_Experiment\u001b[1;34m(X_train, Y_train, X_test, Y_test, epochs, learning_rate, print_loss)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Y_prediction_train.T.shape = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_prediction_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Y_train.T.shape = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train accuracy: {} %\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_prediction_train\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;31m#     print(\"train accuracy: {} %\".format(100 - np.mean(Y_prediction_train - Y_train)**2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test accuracy: {} %\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_prediction_test\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1000,2) (1,1000) "
     ]
    }
   ],
   "source": [
    "d = Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 1000, learning_rate = 0.01, print_loss = True)\n",
    "\n",
    "# Plot learning curve (with costs)\n",
    "losses = np.squeeze(d['losses'])\n",
    "plt.plot(losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LR_Multivariate_incomplete.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

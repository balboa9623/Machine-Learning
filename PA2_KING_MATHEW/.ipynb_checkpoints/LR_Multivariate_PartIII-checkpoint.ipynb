{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ijcv3K3ufXK"
   },
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First, you need to import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- [jdc](https://alexhagen.github.io/jdc/) : Jupyter magic that allows defining classes over multiple jupyter notebook cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c9NvYXOEusGk",
    "outputId": "30922414-9345-4094-f608-328b225af497"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jdc in /usr/local/lib/python3.6/dist-packages (0.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install jdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGgx8jS5ufXL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jdc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "plzs9_wiufXO"
   },
   "source": [
    "## 2 - Problem Statement ##\n",
    "\n",
    "You will create a neural network class - MultivariateNetwork:\n",
    "    - initialize parameters, such as weights, learning rate, etc.\n",
    "    - implement the gredient descent algorithm\n",
    "    - implement the predict function to make predictions for new data sets\n",
    "    - implement the normalization function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jji0CuieufXP"
   },
   "outputs": [],
   "source": [
    "class MultivariateNetwork():\n",
    "    def __init__(self, num_of_features=1, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        This function creates a vector of zeros of shape (num_of_features, 1) for W and initializes w_0 to 0.\n",
    "\n",
    "        Argument:\n",
    "        num_of_features -- size of the W vector, i.e., the number of features, excluding the bias\n",
    "        \n",
    "        \"\"\"\n",
    "        # n is the number of features\n",
    "        self.n = num_of_features\n",
    "        # alpha is the learning rate\n",
    "        self.alpha = learning_rate\n",
    "\n",
    "        ### START YOUR CODE HERE ### \n",
    "        #initialize self.W and self.w_0 to be 0's\n",
    "        self.W = np.zeros(shape=(self.n, 1))\n",
    "        self.w_0 = 0\n",
    "        ### YOUR CODE ENDS ###\n",
    "        assert(self.W.shape == (self.n, 1))\n",
    "        assert(isinstance(self.w_0, float) or isinstance(self.w_0, int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4TmyvZZufXS"
   },
   "source": [
    "## 3 - Gradient Descent ##\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $h_{W}(X) = W^T * X + w_{0}\\tag{1}$\n",
    "- You calculate the cost function:  $$L(W) = \\frac{1}{2m} \\sum_{i=1}^{n} \\left(h_{W}(x^{(i)})  - y^{(i)}\\right)^2\\tag{2}$$. \n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ dw_{j} =\\frac{\\partial L}{\\partial w_{j}} = \\frac{1}{m} \\sum_{i=1}^m (( h_{W}(x^{(i)}) -y^{(i)}) * x_{j}^{(i)})\\tag{3}$$\n",
    "$$ dw_{0} = \\frac{\\partial L}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^m (h_{W}(x^{(i)}) -y^{(i)})\\tag{4}$$\n",
    "\n",
    "The weights will be updated:\n",
    "$$ w_{j} = w_{j} - {\\alpha} * \\frac{\\partial L}{\\partial w_{j}}\\tag{5}$$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b3khVDmiufXS"
   },
   "outputs": [],
   "source": [
    "%%add_to MultivariateNetwork\n",
    "def fit(self, X, Y, epochs=1000, print_loss=True):\n",
    "    \"\"\"\n",
    "    This function implements the Gradient Descent Algorithm\n",
    "    Arguments:\n",
    "    X -- training data matrix: each column is a training example. \n",
    "            The number of columns is equal to the number of training examples\n",
    "    Y -- true \"label\" vector: shape (1, m)\n",
    "    epochs --\n",
    "\n",
    "    Return:\n",
    "    params -- dictionary containing weights\n",
    "    losses -- loss values of every 100 epochs\n",
    "    grads -- dictionary containing dw and dw_0\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "\n",
    "    # print(\"W.T.shape = \", self.W.T.shape)\n",
    "    # print(\"X.shape = \", X.shape)\n",
    "    # print(\"Y.shape = \", Y.shape)\n",
    "    # print(\"W.shape = \", self.W.shape)\n",
    "\n",
    "    # Get the number of training examples\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "      ### START YOUR CODE HERE ### \n",
    "      # Calculate the hypothesis outputs Y_hat (≈ 1 line of code)\n",
    "      Y_hat = np.dot(self.W.T, X) + self.w_0 #correct \n",
    "      \n",
    "      # Calculate loss (≈ 1 line of code)\n",
    "      loss = (1/(2*m)) * np.dot((Y_hat - Y), (Y_hat - Y).T) #correct\n",
    "      \n",
    "      # Calculate the gredients for W and w_0 (≈ 2 lines of code)\n",
    "      dw = np.sum((Y_hat - Y) * X)\n",
    "      dw_0 = np.sum(Y_hat - Y)\n",
    "      \n",
    "      dw /= m\n",
    "      dw_0 /= m\n",
    "\n",
    "      # Weight updates (≈ 2 lines of code)\n",
    "      self.W -= self.alpha * dw\n",
    "      self.w_0 -= self.alpha * dw_0\n",
    "      ### YOUR CODE ENDS ###\n",
    "      \n",
    "      if((i % 100) == 0):\n",
    "        losses.append(loss)\n",
    "        # Print the cost every 100 training examples\n",
    "        if print_loss:\n",
    "          print (\"Cost after iteration %i: %f\" %(i, loss))\n",
    "\n",
    "    params = {\n",
    "        \"W\": self.W,\n",
    "        \"w_0\": self.w_0\n",
    "    }\n",
    "\n",
    "    grads = {\n",
    "        \"dw\": dw,\n",
    "        \"dw_0\": dw_0\n",
    "    }\n",
    "\n",
    "    return params, grads, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aHaJMIttufXV"
   },
   "source": [
    "### Make Predictions ###\n",
    "The predicted output is calculated as $h_{W}(X) = W^T * X + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DbowzB1PufXW"
   },
   "outputs": [],
   "source": [
    "%%add_to MultivariateNetwork\n",
    "def predict(self, X):\n",
    "    '''\n",
    "    Predict the actual values using learned parameters (self.W, self.w_0)\n",
    "\n",
    "    Arguments:\n",
    "    X -- data of size (n x m)\n",
    "\n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    # print(\"W.T shape = \", self.W.T.shape)\n",
    "    # print(\"X shape = \", X.shape)\n",
    "    \n",
    "    Y_prediction = np.zeros((1,m))\n",
    "\n",
    "    # Compute the actual values (≈ 1 line of code)\n",
    "    ### START YOUR CODE HERE ### \n",
    "    Y_prediction = np.dot(self.W.T, X) + self.w_0 #see fit function\n",
    "    #Y_prediction = self.W.T * X + self.w_0\n",
    "    #Y_prediction = self.W * X + self.w_0\n",
    "\n",
    "    ### YOUR CODE ENDS ###\n",
    "\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z9ooB8aDufXY"
   },
   "source": [
    "### Feature Scaling ###\n",
    "Here you normalize features using:\n",
    "$ \\frac{x_{i} - mean}{\\sigma}$, where $\\sigma$ is the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U9l8H0sSufXZ"
   },
   "outputs": [],
   "source": [
    "%%add_to MultivariateNetwork\n",
    "def normalize(self, matrix):\n",
    "    '''\n",
    "    matrix: the matrix that needs to be normalized. Note that each column represents a training example. \n",
    "         The number of columns is the the number of training examples\n",
    "    '''\n",
    "    # Calculate mean for each feature\n",
    "    # Pay attention to the value of axis = ?\n",
    "    # set keepdims=True to avoid rank-1 array\n",
    "    \n",
    "\n",
    "    ### START YOUR CODE HERE ### \n",
    "    # calculate mean (1 line of code)\n",
    "    mean = np.mean(matrix, axis = 1, keepdims=True) \n",
    "    \n",
    "    # calculate standard deviation (1 line of code)\n",
    "    # axis = 0 => column\n",
    "    # axis = 1 => row\n",
    "    std = np.std(matrix, axis=1, keepdims=True) \n",
    "    \n",
    "    # normalize the matrix based on mean and std\n",
    "    matrix = (matrix - mean) / std\n",
    "    ### YOUR CODE ENDS ###\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cK5fzG7sufXb"
   },
   "source": [
    "### Run the Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJB1AT-XufXc"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 2000, learning_rate = 0.5, print_loss = False):\n",
    "    \"\"\"\n",
    "    Builds the multivariate linear regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array \n",
    "    Y_train -- training labels represented by a numpy array (vector) \n",
    "    X_test -- test set represented by a numpy array\n",
    "    Y_test -- test labels represented by a numpy array (vector)\n",
    "    epochs -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_loss -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    num_of_features = X_train.shape[0]\n",
    "    model = MultivariateNetwork(num_of_features, learning_rate)\n",
    "\n",
    "    # normalize\n",
    "    X_test = model.normalize(X_test)\n",
    "    #Y_test = model.normalize(Y_test)\n",
    "    \n",
    "    X_train = model.normalize(X_train)\n",
    "    #Y_train = model.normalize(Y_train)\n",
    "\n",
    "\n",
    "    ### START YOUR CODE HERE ###\n",
    "    # Obtain the parameters, gredients, and losses by calling a model's method (≈ 1 line of code)\n",
    "    parameters, grads, losses = model.fit(X_train, Y_train, epochs, print_loss)\n",
    "    ### YOUR CODE ENDS ###\n",
    "    \n",
    "    ### START YOUR CODE HERE ###\n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    print(\"Y_Test shape = \", Y_test.shape)\n",
    "    Y_prediction_test = model.predict(X_test)\n",
    "    Y_prediction_train = model.predict(X_train)\n",
    "    ### YOUR CODE ENDS ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)/Y_train) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)/Y_test) * 100))\n",
    "\n",
    "    W = parameters['W']\n",
    "    w_0 = parameters['w_0']\n",
    "    print(\"W is \" + str(W))\n",
    "    print(\"w_0 is \" + str(w_0))\n",
    "    \n",
    "    d = {\"losses\": losses,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"W\" : W, \n",
    "         \"w_0\" : w_0,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"epochs\": epochs}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i1IWwMQLufXf"
   },
   "source": [
    "### Load Data and Start the Learning Process ###\n",
    "You can change num_iterations and learning_rate to see the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dg5Xoq_8ufXf"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('prj2house.csv', header=None)\n",
    "X_train = df[[0, 1]].values.T\n",
    "Y_train = df[2].values.reshape(-1, 1).T\n",
    "\n",
    "\n",
    "df_test = pd.read_csv('prj2house_test.csv', header=None)\n",
    "X_test = df_test[[0, 1]].values.T\n",
    "Y_test = df_test[2].values.reshape(-1, 1).T #one row by n columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TolV0RoufXi"
   },
   "source": [
    "### Plot the learning curve ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "colab_type": "code",
    "id": "rAU96EG-ufXi",
    "outputId": "350efadb-790e-43a4-d797-bef8a8e137ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 65591548106.457443\n",
      "Cost after iteration 100: 11294338815.945570\n",
      "Cost after iteration 200: 4564292793.308242\n",
      "Cost after iteration 300: 3663565079.532423\n",
      "Cost after iteration 400: 3542887571.190079\n",
      "Cost after iteration 500: 3526719240.857438\n",
      "Cost after iteration 600: 3524553013.221766\n",
      "Cost after iteration 700: 3524262782.747480\n",
      "Cost after iteration 800: 3524223897.762901\n",
      "Cost after iteration 900: 3524218687.965311\n",
      "Y_Test shape =  (1, 2)\n",
      "train accuracy: 78.55834823017564 %\n",
      "test accuracy: 75.68588290052429 %\n",
      "W is [[51434.72080774]\n",
      " [51434.72080774]]\n",
      "w_0 is 340397.96353531966\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 1000, learning_rate = 0.01, print_loss = True)\n",
    "\n",
    "# Plot learning curve (with costs)\n",
    "losses = np.squeeze(d['losses'])\n",
    "plt.plot(losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yCOn8LZxHn2c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LR_Multivariate_PartIII_incomplete.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "DT_Part-II-Incomplete.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efhkat9Z6McV"
      },
      "source": [
        "## 1 - Packages ##\n",
        "\n",
        "First, you need to import all the packages that you will need during this assignment. \n",
        "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
        "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
        "- [jdc](https://alexhagen.github.io/jdc/) : Jupyter magic that allows defining classes over multiple jupyter notebook cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH5_7rlW6McW",
        "outputId": "da84b237-273c-4e67-e3d7-484b2a713e3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install jdc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import jdc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jdc\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/cb/9afea749985eef20f3160e8826a531c7502e40c35a038dfe49b67726e9a0/jdc-0.0.9-py2.py3-none-any.whl\n",
            "Installing collected packages: jdc\n",
            "Successfully installed jdc-0.0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsaxTjuy6Mca"
      },
      "source": [
        "## 2 - Required Methods ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO8tLBpF6Mcb"
      },
      "source": [
        "### 2.1 - Return the index of the maximum value in an array ###\n",
        "- [Numpy.argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RlUi9hQ6Mcb",
        "outputId": "a9468e75-4c9b-4025-996f-a8e3848c990b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Define a DataFrame object\n",
        "df_sample = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 0]]),\n",
        "                   columns=['a', 'b', 'c'])\n",
        "\n",
        "print(df_sample)\n",
        "# print items in column \"a\"\n",
        "print(df_sample['a'])\n",
        "\n",
        "# print the index of the item that has the maximum value\n",
        "print(np.argmax(df_sample['a']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   a  b  c\n",
            "0  1  2  3\n",
            "1  4  5  6\n",
            "2  7  8  0\n",
            "0    1\n",
            "1    4\n",
            "2    7\n",
            "Name: a, dtype: int64\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9nE8DUHG6Mce",
        "outputId": "81949568-b13b-4de0-9406-b578faa96553",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print the maximum value\n",
        "idx = np.argmax(df_sample['a'])\n",
        "\n",
        "print(df_sample['a'][idx])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iedzmsEF6Mch"
      },
      "source": [
        "#### Graded Excercise #### \n",
        "- Print out the index of the maximun item in column \"c\"\n",
        "- Use the index to print its value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4wOqEXR6Mch",
        "outputId": "09b19238-db32-4214-dd06-9d1e91d69b53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Will be graded\n",
        "\n",
        "# Print the column names of the training data\n",
        "### START CODE HERE ### (â‰ˆ 1 line of code)\n",
        "# get the index of the maximum value in column \"c\" in df_sample\n",
        "idx_c = np.argmax(df_sample['c'])\n",
        "\n",
        "# print the index\n",
        "print(f'The index for the maximum value in column c is {idx_c}')\n",
        "\n",
        "# get the maximum value based on idx_c for column \"c\" in df_sample\n",
        "max_val_c = df_sample['c'][idx_c]\n",
        "print(\"The maximun value in column c is\", max_val_c)\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The index for the maximum value in column c is 1\n",
            "The maximun value in column c is 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVQr4CLy6Mck"
      },
      "source": [
        "## 3 - Fundamentals in Decision Tree ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y50Vy2I86Mck"
      },
      "source": [
        "### 3.1 - Entropy ###\n",
        "- As usual, we will define a class named \"Decision_Tree\"\n",
        "- We will implement entropy function:\n",
        "$$ H = -\\sum_{i=1}^{n} P_{i} * \\log_{2}P_{i}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSuTLsRS6Mcl"
      },
      "source": [
        "class Decision_Tree():\n",
        "    def __init__(self):\n",
        "        # a dictionary of generated decision tree\n",
        "        self.tree = None\n",
        "        \n",
        "    def entropy(self, column):\n",
        "        \"\"\"\n",
        "        Calculate the entropy of a given data column.\n",
        "        column: the data column\n",
        "        \"\"\"\n",
        "        \n",
        "        # the list that will contain every -pi * log(pi)\n",
        "        ent_list = []\n",
        "        \n",
        "        ### START CODE HERE ###\n",
        "        # Determine the unique values in the column and their corresponding counts of each unique value\n",
        "        values, counts = np.unique(column, return_counts=True)\n",
        "        \n",
        "        # The number of disctint values\n",
        "        num_distinct_values = len(values)\n",
        "        # the total number of items in the column\n",
        "        total_items_in_column = len(column)\n",
        "        \n",
        "        for i in range(num_distinct_values):\n",
        "            # calculate the probability pi for the ith value \n",
        "            p_i = counts[i]/ np.sum(counts)\n",
        "            # calculate -pi * log(pi)\n",
        "            ent_i =  -p_i * np.log2(p_i)\n",
        "            #put the result into the list\n",
        "            ent_list.append(ent_i)\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        return np.sum(ent_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxymp9_d6Mco"
      },
      "source": [
        "### 3.2 - Information Gain ###\n",
        "$$ IG(S|a) = entropy(S) - \\sum_{v \\in values(a)} \\frac {|S_{v}|} {|S|} * entropy(S_{v})$$\n",
        "where\n",
        "- $IG(S|a)$ means the information gain if we split data S using attribute a\n",
        "- $|S_{v}|$ means the number of items with $a = v$\n",
        "- $|S|$ means the total number of items in S\n",
        "- $\\frac {|S_{v}|} {|S|}$ is the probability of $ a = v $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66KjTe-W6Mcp"
      },
      "source": [
        "%%add_to Decision_Tree\n",
        "def Infomation_Gain(self, S, entropy_before_splitting, a, class_name = \"class\"):\n",
        "    \"\"\"\n",
        "    Calculate the information gain of a dataset. This function takes four parameters:\n",
        "    1. S: the overall dataset (See the equation above)\n",
        "    2. entropy_before_splitting: the entropy before splitting\n",
        "    3. a: the attribute that we will use to split the data (See the equation above)\n",
        "    4. class_name = the class that the set of data is classified as\n",
        "    \"\"\"    \n",
        "    # the list that will contain the weighted entropy of each child, i.e., Pv * Hv, \n",
        "    #       where Pv is the probability of being in the child node v, and\n",
        "    #       Hv is the entropy of child node v: entropy(Sv).\n",
        "    H_list = []\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    #determine the unique values and their corresponding counts for the split attribute \n",
        "    unique_vals, counts= np.unique(S[a], return_counts=True)\n",
        "    \n",
        "    #calculate the total number of items in S\n",
        "    total_S = len(S)\n",
        "    #Calculate the total number of unnique values with regard to attribute a\n",
        "    total_Sv = len(unique_vals)\n",
        "    \n",
        "    for i in range(total_Sv):\n",
        "        # the probablity of being in the ith child node\n",
        "        P_i = counts[i]/np.sum(counts)\n",
        "        # the value v of the ith child\n",
        "        v = unique_vals[i]\n",
        "        # the subset Sv, where a = v\n",
        "        # hint: use DataFrame.where and only return the \"class\" column\n",
        "        S_v = S.where(S[a] == v).dropna()[class_name]\n",
        "        # the entropy of child node Sv\n",
        "        H_i = self.entropy(S_v)\n",
        "        # put P_i * H_i into the H_list \n",
        "        H_list.append(P_i * H_i)\n",
        "    \n",
        "    # calculate the conditional entropy based on H_list\n",
        "    conditional_entropy = np.sum(H_list)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    #Calculate the information gain\n",
        "    Information_Gain = entropy_before_splitting - conditional_entropy\n",
        "    return Information_Gain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A52H-5o36Mcr"
      },
      "source": [
        "### 3.3 - Determine the majority class for a given data vector ###\n",
        "- If a data vector is equally split among multiple classes, we return the global_majority_class as the the class for the data vector\n",
        "- Otherwise, return the majority class of the data vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKLiLQSw6Mcs"
      },
      "source": [
        "%%add_to Decision_Tree\n",
        "\n",
        "#This function determines the majority class for a data_vector\n",
        "def Get_Majority_Class(self, data_vector, global_majority_class=None):\n",
        "    '''\n",
        "    Parameters:\n",
        "    - data_vector: the class vector of a child node\n",
        "    - global_majority_class: the majority class in the original data. If \"data_vector\" is equally split among multiple classes, we return the global_majority_class as the label \n",
        "    '''\n",
        "    # get the unique values and their corresponding counts\n",
        "    values, counts = np.unique(data_vector, return_counts = True)\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    if(counts[0] == counts[1]): \n",
        "        # as there are only two classes, if this split yields equal subsets,\n",
        "        # return the majority class of the original data set \n",
        "        return global_majority_class\n",
        "    else: # otherwise, return the majority class\n",
        "        \n",
        "        # get the index of the majority class\n",
        "        # hint: use \"argmax\"\n",
        "        majority_index = np.argmax(data_vector[global_majority_class])\n",
        "        \n",
        "        # get the value of the majority class based on the majority_index\n",
        "        majority_class = data_vector[global_majority_class][majority_index]\n",
        "        \n",
        "        # return the majority class of \"data_vector\"\n",
        "        return majority_class\n",
        "    ### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d91EIAPA6Mcu"
      },
      "source": [
        "### 3.4 - ID3 Without Prunning Algorithm ###\n",
        "Please refer to our lecture in detail. Remember that this is a recursive function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC5oMIko6Mcv"
      },
      "source": [
        "%%add_to Decision_Tree\n",
        "\n",
        "# We will implement the ID3 Decision Tree algorithm\n",
        "def ID3_No_Prune(self, data, global_majority_class, features, target_attribute_name=\"class\"):\n",
        "    \"\"\"\n",
        "    Decision Tree algorithm (ID3) without prunning\n",
        "    Paramters:\n",
        "    - data: the data for which the decision tree algorithm will run --> In the first run, it is the original dataset\n",
        " \n",
        "    - global_majority_class: The majority class of the original dataset\n",
        "\n",
        "    - features: the feature space of the dataset . When a feature is used for splitting, this feature will be removed from this features set.\n",
        "\n",
        "    - target_attribute_name: the name of the target attribute\n",
        "\n",
        "    \"\"\"   \n",
        "    ### START CODE HERE ###\n",
        "    #Define the stopping rules for recursion --> If one of this is satisfied, we want to return a leaf node#\n",
        "    \n",
        "    # Rule1: If the dataset is empty, return the majority class in the original dataset\n",
        "    if len(data)==0:\n",
        "        return np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name], return_coounts=True)[1])]\n",
        "    \n",
        "    # Rule2: If all target_values belong to the same class, return this class\n",
        "    elif len(np.unique(data[target_attribute_name])) == 1:\n",
        "        return np.unique(data[target_attribute_name])[0]\n",
        "    \n",
        "    # Rule3: If the feature space is empty, return the majority class in data by calling the Get_Majority_Class function\n",
        "    elif len(features) ==0:\n",
        "        #return the majority class\n",
        "        return Get_Majority_Class(data, global_majority_class)\n",
        "    \n",
        "    #If none of the above holds true, grow the tree!\n",
        "    else:\n",
        "        \n",
        "        # Calculate the entropy for data, i.e., before splitting\n",
        "        total_entropy = self.entropy(data)\n",
        "        \n",
        "        # this is a list containing all the information gains for all the features\n",
        "        item_values = []\n",
        "        for feature in features:\n",
        "            # Calculate the information gain for each feature\n",
        "            info_gain_feature = self.Infomation_Gain(data, total_entropy, target_attribute_name) ### DOUBLE CHECK PARAMETERS\n",
        "            \n",
        "            # put the informatin gain of the feature into item_values list\n",
        "            item_values.append(info_gain_feature)\n",
        "        \n",
        "        # determine the index of the feature which best splits the dataset\n",
        "        # hint: use argmax\n",
        "        best_feature_index = np.argmax(item_values)\n",
        "        # # determine the feature which best splits the dataset\n",
        "        best_feature = features[best_feature_index]\n",
        "        \n",
        "        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information\n",
        "        #gain in the first run\n",
        "        # \"tree\" is a dictionary\n",
        "        tree = {best_feature:{}}\n",
        "        \n",
        "        #Remove the feature with the best inforamtion gain from the feature space\n",
        "        features = [i for i in features if i != best_feature]\n",
        "        \n",
        "        #Grow a branch under the root node for each possible value of the root node feature\n",
        "        for value in np.unique(data[best_feature]):\n",
        "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
        "            # hint: use dataframe.where\n",
        "            sub_data = data.where(data[best_feature] == value).dropna()\n",
        "            \n",
        "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
        "            subtree = ID3_No_Prune(sub_data, global_majority_class, features, target_attribute_name)\n",
        "            \n",
        "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
        "            tree[best_feature][value] = subtree\n",
        "        \n",
        "        self.tree = tree\n",
        "        ### END CODE HERE ###\n",
        "        return(tree)    "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwivnR146Mcx"
      },
      "source": [
        "### 3.5 Get the Tree generated by ID3 ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPlMQCS-6Mcx"
      },
      "source": [
        "%%add_to Decision_Tree\n",
        "# No change to this function is needed\n",
        "def get_tree(self):\n",
        "    ''' \n",
        "    - This is a getter method\n",
        "    - Return the tree dictionary generated by the ID3_No_Prune algorithm\n",
        "    '''\n",
        "    return self.tree"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I85Hplg-6Mcz"
      },
      "source": [
        "### 3.6 - Predict for a new query ###\n",
        "- query: is a dictionary generated from a test set (see 4.1 below)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdMMzX026Mc0"
      },
      "source": [
        "%%add_to Decision_Tree\n",
        "# No change to this function is needed\n",
        "def predict(self, query, tree, default = 1):\n",
        "    \"\"\"\n",
        "    Prediction of a new/unseen query instance. This takes three parameters:\n",
        "    - query: a dictionary of the shape {\"feature_name\":feature_value,...}\n",
        "    - tree: a dictionary containing the decision tree informaiton that is generated by the ID3 algorithm \n",
        "    - default: when exception happens, it returns the default class\n",
        "    \"\"\"\n",
        "    \n",
        "    for key in list(query.keys()):\n",
        "        if key in list(tree.keys()):\n",
        "            # if the key is in the tree, try to get its result\n",
        "            try:\n",
        "                result = tree[key][query[key]] \n",
        "            except:\n",
        "                return default\n",
        "  \n",
        "            # Get the value based on the key\n",
        "            result = tree[key][query[key]]\n",
        "            # the value could be a dictionary\n",
        "            if isinstance(result,dict):\n",
        "                # As it is still a dictionary, leaf has not reached yet\n",
        "                # Recursively call predict on dict\n",
        "                return self.predict(query,result)\n",
        "            else:\n",
        "                # Leaf has been reached, simply return the label\n",
        "                return result\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWRItPsp6Mc2"
      },
      "source": [
        "## 4 - Experiment ##\n",
        "### 4.1 - Evaluate the generated decision tree with a test set ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK90tgVB6Mc2"
      },
      "source": [
        "# No change is needed for this function\n",
        "def test_with_test_set(dtree_object, test_set):\n",
        "    '''\n",
        "    Evaluate the tree with a test set\n",
        "    Parameters:\n",
        "    - dtree_object: an instance of the Decision_Tree class\n",
        "    - test_set: a test_set\n",
        "    '''\n",
        "    # Create new query instances by simply removing the target feature column from the original dataset and \n",
        "    # convert it to a dictionary\n",
        "    queries = test_set.iloc[:,:-1].to_dict(orient = \"records\")\n",
        "    \n",
        "    #Create am empty DataFrame where the predictions of the tree are stored in this column\n",
        "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
        "    \n",
        "    #Calculate the prediction accuracy\n",
        "    for i in range(len(test_set)):\n",
        "        predicted.loc[i,\"predicted\"] = dtree_object.predict(queries[i], dtree_object.get_tree(), 1.0) \n",
        "    print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == test_set[\"class\"])/len(test_set))*100,'%')"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sacKanDL6Mc4"
      },
      "source": [
        "### 4.2 Experiment 1: Obtain a Decision Tree Object ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1vUO9cr6Mc5"
      },
      "source": [
        "def experiment_1_get_dtree():\n",
        "    '''\n",
        "    In this exmperiment, you will obtain:\n",
        "    1- the overall training set (i.e., dataframe), df_train\n",
        "    2- the tree object, dtree\n",
        "    '''\n",
        "    # Load the training data\n",
        "    df_train = pd.read_csv('train.csv')\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # 1. create a Decision_Tree object\n",
        "    dtree = Decision_Tree()\n",
        "    # 2. get the majority class of the original data. \n",
        "    # hint: use a function defined in Decision_Tree\n",
        "    global_majority_class = dtree.Get_Majority_Class(df_train, 'tea') ## DOUBLE CHECK\n",
        "    \n",
        "    #3. Build up the decision tree based on df_train\n",
        "    tree = dtree.ID3_No_Prune(df_train, global_majority_class, df_train, 'class')\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return df_train, dtree"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fwLq05nz6Mc7",
        "outputId": "84f12930-2451-4ef9-a200-024a4f227b33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        }
      },
      "source": [
        "# Call experiment_1_get_dtree to get the training set and the tree object\n",
        "df_train, dtree_object = experiment_1_get_dtree()\n",
        "print(dtree_object.get_tree())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-87496c168d84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Call experiment_1_get_dtree to get the training set and the tree object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtree_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment_1_get_dtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtree_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-082aca8acf53>\u001b[0m in \u001b[0;36mexperiment_1_get_dtree\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#3. Build up the decision tree based on df_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID3_No_Prune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_majority_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'class'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mID3_No_Prune\u001b[0;34m(self, data, global_majority_class, features, target_attribute_name)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isZmeDFY6Mc8"
      },
      "source": [
        "### 4.3 Experiment 2 ###\n",
        "\n",
        "- Obtain the test accuracy with the training set\n",
        "- Obtain the test accuracy with the test set\n",
        "- Obtain the test accuracies for the trees determined by using 100, 200, 300, 400, 500, 600, 700, and 800 training examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJvl9HlU6Mc9"
      },
      "source": [
        "def experiment_2_test(dtree_object, train_set):\n",
        "    ### START CODE HERE ###\n",
        "    # Test1: test with the training set\n",
        "    print(\"Test accuracy for the training set is\")\n",
        "    ???\n",
        "    \n",
        "    # Load the test data\n",
        "    df_test = \n",
        "    \n",
        "    # Test2: test with the test set\n",
        "    print(\"Test accuracy for the testing set is\")\n",
        "    ???\n",
        "    \n",
        "    # The training size\n",
        "    training_size = 100\n",
        "    \n",
        "    # Test 3: train the tree using training size of 100, 200, 300, 400, 500, 600, 700, 800\n",
        "    while(training_size <= 800):\n",
        "        df_partial = train_set.iloc[0:training_size, :]\n",
        "        \n",
        "        #Determine the majority class for df_partial\n",
        "        majority_class = \n",
        "        # Build up the decision tree based on df_partial\n",
        "        tree = \n",
        "        \n",
        "        # test with the testing set for the tree generated from df_partial\n",
        "        print(f\"Test accuracy for a training set of %d data points is\"%training_size)\n",
        "        ???\n",
        "        # increase the train set by 100\n",
        "        training_size += 100\n",
        "    ### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9lG1Y6r6Mc-"
      },
      "source": [
        "experiment_2_test(dtree_object, df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkRes4Ci6MdB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
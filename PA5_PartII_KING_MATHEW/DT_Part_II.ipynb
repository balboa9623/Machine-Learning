{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "DT_Part_II.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efhkat9Z6McV"
      },
      "source": [
        "## 1 - Packages ##\n",
        "\n",
        "First, you need to import all the packages that you will need during this assignment. \n",
        "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
        "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
        "- [jdc](https://alexhagen.github.io/jdc/) : Jupyter magic that allows defining classes over multiple jupyter notebook cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH5_7rlW6McW",
        "outputId": "6511b781-aebc-4209-88e8-9ec86fa78e24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install jdc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import jdc\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jdc in /usr/local/lib/python3.6/dist-packages (0.0.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsaxTjuy6Mca"
      },
      "source": [
        "## 2 - Required Methods ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO8tLBpF6Mcb"
      },
      "source": [
        "### 2.1 - Return the index of the maximum value in an array ###\n",
        "- [Numpy.argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RlUi9hQ6Mcb",
        "outputId": "1e27ac0b-31ab-408c-d2cf-ed01a31bbc59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Define a DataFrame object\n",
        "df_sample = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 0]]),\n",
        "                   columns=['a', 'b', 'c'])\n",
        "\n",
        "print(df_sample)\n",
        "# print items in column \"a\"\n",
        "print(df_sample['a'])\n",
        "\n",
        "# print the index of the item that has the maximum value\n",
        "print(np.argmax(df_sample['a']))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   a  b  c\n",
            "0  1  2  3\n",
            "1  4  5  6\n",
            "2  7  8  0\n",
            "0    1\n",
            "1    4\n",
            "2    7\n",
            "Name: a, dtype: int64\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9nE8DUHG6Mce",
        "outputId": "80677e8a-4d52-4056-c91a-312a4132890f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print the maximum value\n",
        "idx = np.argmax(df_sample['a'])\n",
        "\n",
        "print(df_sample['a'][idx])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iedzmsEF6Mch"
      },
      "source": [
        "#### Graded Excercise #### \n",
        "- Print out the index of the maximun item in column \"c\"\n",
        "- Use the index to print its value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4wOqEXR6Mch",
        "outputId": "093859da-d355-4063-94c6-76ff96142050",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Will be graded\n",
        "\n",
        "# Print the column names of the training data\n",
        "### START CODE HERE ### (â‰ˆ 1 line of code)\n",
        "# get the index of the maximum value in column \"c\" in df_sample\n",
        "idx_c = np.argmax(df_sample['c'])\n",
        "\n",
        "# print the index\n",
        "print(f'The index for the maximum value in column c is {idx_c}')\n",
        "\n",
        "# get the maximum value based on idx_c for column \"c\" in df_sample\n",
        "max_val_c = df_sample['c'][idx_c]\n",
        "print(\"The maximun value in column c is\", max_val_c)\n",
        "### END CODE HERE ###"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The index for the maximum value in column c is 1\n",
            "The maximun value in column c is 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVQr4CLy6Mck"
      },
      "source": [
        "## 3 - Fundamentals in Decision Tree ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y50Vy2I86Mck"
      },
      "source": [
        "### 3.1 - Entropy ###\n",
        "- As usual, we will define a class named \"Decision_Tree\"\n",
        "- We will implement entropy function:\n",
        "$$ H = -\\sum_{i=1}^{n} P_{i} * \\log_{2}P_{i}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSuTLsRS6Mcl"
      },
      "source": [
        "class Decision_Tree():\n",
        "    def __init__(self):\n",
        "        # a dictionary of generated decision tree\n",
        "        self.tree = None\n",
        "        \n",
        "    def entropy(self, column):\n",
        "        \"\"\"\n",
        "        Calculate the entropy of a given data column.\n",
        "        column: the data column\n",
        "        \"\"\"\n",
        "        \n",
        "        # the list that will contain every -pi * log(pi)\n",
        "        ent_list = []\n",
        "        \n",
        "        ### START CODE HERE ###\n",
        "        # Determine the unique values in the column and their corresponding counts of each unique value\n",
        "        values, counts = np.unique(column, return_counts=True)\n",
        "        \n",
        "        # The number of disctint values\n",
        "        num_distinct_values = len(values)\n",
        "        # the total number of items in the column\n",
        "        total_items_in_column = len(column)\n",
        "        \n",
        "        for i in range(num_distinct_values):\n",
        "            # calculate the probability pi for the ith value \n",
        "            p_i = counts[i]/ total_items_in_column\n",
        "            # calculate -pi * log(pi)\n",
        "            ent_i =  -p_i * np.log2(p_i)\n",
        "            #put the result into the list\n",
        "            ent_list.append(ent_i)\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        return np.sum(ent_list)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxymp9_d6Mco"
      },
      "source": [
        "### 3.2 - Information Gain ###\n",
        "$$ IG(S|a) = entropy(S) - \\sum_{v \\in values(a)} \\frac {|S_{v}|} {|S|} * entropy(S_{v})$$\n",
        "where\n",
        "- $IG(S|a)$ means the information gain if we split data S using attribute a\n",
        "- $|S_{v}|$ means the number of items with $a = v$\n",
        "- $|S|$ means the total number of items in S\n",
        "- $\\frac {|S_{v}|} {|S|}$ is the probability of $ a = v $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66KjTe-W6Mcp"
      },
      "source": [
        "%%add_to Decision_Tree\n",
        "def Infomation_Gain(self, S, entropy_before_splitting, a, class_name = \"class\"):\n",
        "    \"\"\"\n",
        "    Calculate the information gain of a dataset. This function takes four parameters:\n",
        "    1. S: the overall dataset (See the equation above)\n",
        "    2. entropy_before_splitting: the entropy before splitting\n",
        "    3. a: the attribute that we will use to split the data (See the equation above)\n",
        "    4. class_name = the class that the set of data is classified as\n",
        "    \"\"\"    \n",
        "    # the list that will contain the weighted entropy of each child, i.e., Pv * Hv, \n",
        "    #       where Pv is the probability of being in the child node v, and\n",
        "    #       Hv is the entropy of child node v: entropy(Sv).\n",
        "    H_list = []\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    #determine the unique values and their corresponding counts for the split attribute \n",
        "    unique_vals, counts= np.unique(S[a], return_counts=True)\n",
        "    \n",
        "    #calculate the total number of items in S\n",
        "    total_S = len(S)\n",
        "    #Calculate the total number of unnique values with regard to attribute a\n",
        "    total_Sv = len(unique_vals)\n",
        "    \n",
        "    for i in range(total_Sv):\n",
        "        # the probablity of being in the ith child node\n",
        "        P_i = counts[i]/total_S\n",
        "        # the value v of the ith child\n",
        "        v = unique_vals[i]\n",
        "        # the subset Sv, where a = v\n",
        "        # hint: use DataFrame.where and only return the \"class\" column\n",
        "        S_v = S.where(S[a] == v).dropna()[class_name]\n",
        "        # the entropy of child node Sv\n",
        "        H_i = self.entropy(S_v)\n",
        "        # put P_i * H_i into the H_list \n",
        "        H_list.append(P_i * H_i)\n",
        "    \n",
        "    # calculate the conditional entropy based on H_list\n",
        "    conditional_entropy = np.sum(H_list)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    #Calculate the information gain\n",
        "    Information_Gain = entropy_before_splitting - conditional_entropy\n",
        "    return Information_Gain"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A52H-5o36Mcr"
      },
      "source": [
        "### 3.3 - Determine the majority class for a given data vector ###\n",
        "- If a data vector is equally split among multiple classes, we return the global_majority_class as the the class for the data vector\n",
        "- Otherwise, return the majority class of the data vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKLiLQSw6Mcs"
      },
      "source": [
        "%%add_to Decision_Tree\n",
        "\n",
        "#This function determines the majority class for a data_vector\n",
        "def Get_Majority_Class(self, data_vector, global_majority_class=None):\n",
        "    '''\n",
        "    Parameters:\n",
        "    - data_vector: the class vector of a child node\n",
        "    - global_majority_class: the majority class in the original data. If \"data_vector\" is equally split among multiple classes, we return the global_majority_class as the label \n",
        "    '''\n",
        "    # get the unique values and their corresponding counts\n",
        "    values, counts = np.unique(data_vector, return_counts = True)\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    if(counts[0] == counts[1]): \n",
        "        # as there are only two classes, if this split yields equal subsets,\n",
        "        # return the majority class of the original data set \n",
        "        return global_majority_class\n",
        "    else: # otherwise, return the majority class\n",
        "        \n",
        "        # get the index of the majority class\n",
        "        # hint: use \"argmax\"\n",
        "        #majority_index = np.argmax(data_vector[global_majority_class])\n",
        "        majority_index = np.argmax(counts)\n",
        "        \n",
        "        # get the value of the majority class based on the majority_index\n",
        "        #majority_class = data_vector[global_majority_class][majority_index]\n",
        "        majority_class = values[majority_index]\n",
        "\n",
        "        \n",
        "        # return the majority class of \"data_vector\"\n",
        "        return majority_class\n",
        "    ### END CODE HERE ###"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d91EIAPA6Mcu"
      },
      "source": [
        "### 3.4 - ID3 Without Prunning Algorithm ###\n",
        "Please refer to our lecture in detail. Remember that this is a recursive function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC5oMIko6Mcv"
      },
      "source": [
        "%%add_to Decision_Tree\n",
        "\n",
        "# We will implement the ID3 Decision Tree algorithm\n",
        "def ID3_No_Prune(self, data, global_majority_class, features, target_attribute_name=\"class\"):\n",
        "    \"\"\"\n",
        "    Decision Tree algorithm (ID3) without prunning\n",
        "    Paramters:\n",
        "    - data: the data for which the decision tree algorithm will run --> In the first run, it is the original dataset\n",
        " \n",
        "    - global_majority_class: The majority class of the original dataset\n",
        "\n",
        "    - features: the feature space of the dataset . When a feature is used for splitting, this feature will be removed from this features set.\n",
        "\n",
        "    - target_attribute_name: the name of the target attribute\n",
        "\n",
        "    \"\"\"   \n",
        "    ### START CODE HERE ###\n",
        "    #Define the stopping rules for recursion --> If one of this is satisfied, we want to return a leaf node#\n",
        "    \n",
        "    # Rule1: If the dataset is empty, return the majority class in the original dataset\n",
        "    if len(data)==0:\n",
        "        return global_majority_class ##\n",
        "    \n",
        "    # Rule2: If all target_values belong to the same class, return this class\n",
        "    elif len(np.unique(data[target_attribute_name])) == 1:\n",
        "        return np.unique(data[target_attribute_name])[0]\n",
        "    \n",
        "    # Rule3: If the feature space is empty, return the majority class in data by calling the Get_Majority_Class function\n",
        "    elif len(features) ==0:\n",
        "        #return the majority class\n",
        "        return self.Get_Majority_Class(data[target_attribute_name], global_majority_class)\n",
        "    \n",
        "    #If none of the above holds true, grow the tree!\n",
        "    else:\n",
        "        \n",
        "        # Calculate the entropy for data, i.e., before splitting\n",
        "        total_entropy = self.entropy(data['class'])\n",
        "        \n",
        "        # this is a list containing all the information gains for all the features\n",
        "        item_values = []\n",
        "        for feature in features:\n",
        "            # Calculate the information gain for each feature\n",
        "            info_gain_feature = self.Infomation_Gain(data, total_entropy, feature, target_attribute_name) ### DOUBLE CHECK PARAMETERS\n",
        "            \n",
        "            # put the informatin gain of the feature into item_values list\n",
        "            item_values.append(info_gain_feature)\n",
        "        \n",
        "        # determine the index of the feature which best splits the dataset\n",
        "        # hint: use argmax\n",
        "        best_feature_index = np.argmax(item_values)\n",
        "        # # determine the feature which best splits the dataset\n",
        "        best_feature = features[best_feature_index]\n",
        "        \n",
        "        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information\n",
        "        #gain in the first run\n",
        "        # \"tree\" is a dictionary\n",
        "        tree = {best_feature:{}}\n",
        "        \n",
        "        #Remove the feature with the best inforamtion gain from the feature space\n",
        "        features = [i for i in features if i != best_feature]\n",
        "        \n",
        "        #Grow a branch under the root node for each possible value of the root node feature\n",
        "        for value in np.unique(data[best_feature]):\n",
        "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
        "            # hint: use dataframe.where\n",
        "            sub_data = data.where(data[best_feature] == value).dropna()\n",
        "            \n",
        "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
        "            subtree = self.ID3_No_Prune(sub_data, global_majority_class, features, target_attribute_name)\n",
        "            \n",
        "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
        "            tree[best_feature][value] = subtree\n",
        "        \n",
        "        self.tree = tree\n",
        "        ### END CODE HERE ###\n",
        "        return(tree)    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwivnR146Mcx"
      },
      "source": [
        "### 3.5 Get the Tree generated by ID3 ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPlMQCS-6Mcx"
      },
      "source": [
        "%%add_to Decision_Tree\n",
        "# No change to this function is needed\n",
        "def get_tree(self):\n",
        "    ''' \n",
        "    - This is a getter method\n",
        "    - Return the tree dictionary generated by the ID3_No_Prune algorithm\n",
        "    '''\n",
        "    return self.tree"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I85Hplg-6Mcz"
      },
      "source": [
        "### 3.6 - Predict for a new query ###\n",
        "- query: is a dictionary generated from a test set (see 4.1 below)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdMMzX026Mc0"
      },
      "source": [
        "%%add_to Decision_Tree\n",
        "# No change to this function is needed\n",
        "def predict(self, query, tree, default = 1):\n",
        "    \"\"\"\n",
        "    Prediction of a new/unseen query instance. This takes three parameters:\n",
        "    - query: a dictionary of the shape {\"feature_name\":feature_value,...}\n",
        "    - tree: a dictionary containing the decision tree informaiton that is generated by the ID3 algorithm \n",
        "    - default: when exception happens, it returns the default class\n",
        "    \"\"\"\n",
        "    \n",
        "    for key in list(query.keys()):\n",
        "        if key in list(tree.keys()):\n",
        "            # if the key is in the tree, try to get its result\n",
        "            try:\n",
        "                result = tree[key][query[key]] \n",
        "            except:\n",
        "                return default\n",
        "  \n",
        "            # Get the value based on the key\n",
        "            result = tree[key][query[key]]\n",
        "            # the value could be a dictionary\n",
        "            if isinstance(result,dict):\n",
        "                # As it is still a dictionary, leaf has not reached yet\n",
        "                # Recursively call predict on dict\n",
        "                return self.predict(query,result)\n",
        "            else:\n",
        "                # Leaf has been reached, simply return the label\n",
        "                return result\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWRItPsp6Mc2"
      },
      "source": [
        "## 4 - Experiment ##\n",
        "### 4.1 - Evaluate the generated decision tree with a test set ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK90tgVB6Mc2"
      },
      "source": [
        "# No change is needed for this function\n",
        "def test_with_test_set(dtree_object, test_set):\n",
        "    '''\n",
        "    Evaluate the tree with a test set\n",
        "    Parameters:\n",
        "    - dtree_object: an instance of the Decision_Tree class\n",
        "    - test_set: a test_set\n",
        "    '''\n",
        "    # Create new query instances by simply removing the target feature column from the original dataset and \n",
        "    # convert it to a dictionary\n",
        "    queries = test_set.iloc[:,:-1].to_dict(orient = \"records\")\n",
        "    \n",
        "    #Create am empty DataFrame where the predictions of the tree are stored in this column\n",
        "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
        "    \n",
        "    #Calculate the prediction accuracy\n",
        "    for i in range(len(test_set)):\n",
        "        predicted.loc[i,\"predicted\"] = dtree_object.predict(queries[i], dtree_object.get_tree(), 1.0) \n",
        "    print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == test_set[\"class\"])/len(test_set))*100,'%')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sacKanDL6Mc4"
      },
      "source": [
        "### 4.2 Experiment 1: Obtain a Decision Tree Object ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1vUO9cr6Mc5"
      },
      "source": [
        "def experiment_1_get_dtree():\n",
        "    '''\n",
        "    In this exmperiment, you will obtain:\n",
        "    1- the overall training set (i.e., dataframe), df_train\n",
        "    2- the tree object, dtree\n",
        "    '''\n",
        "    # Load the training data\n",
        "    df_train = pd.read_csv('train.csv')\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # 1. create a Decision_Tree object\n",
        "    dtree = Decision_Tree()\n",
        "    # 2. get the majority class of the original data. \n",
        "    # hint: use a function defined in Decision_Tree\n",
        "    global_majority_class = dtree.Get_Majority_Class(df_train['class']) ## DOUBLE CHECK\n",
        "    \n",
        "    #3. Build up the decision tree based on df_train\n",
        "    tree = dtree.ID3_No_Prune(df_train, global_majority_class, df_train.columns[:-1], target_attribute_name = \"class\")\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return df_train, dtree"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fwLq05nz6Mc7",
        "outputId": "ca776e6c-b5f3-461e-e4e7-a4d0a301b13f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Call experiment_1_get_dtree to get the training set and the tree object\n",
        "df_train, dtree_object = experiment_1_get_dtree()\n",
        "print(dtree_object.get_tree())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'tea': {0: {'barclay': {0.0: {'romulan': {0.0: {'wesley': {0.0: {'honor': {0.0: {'poetry': {0.0: 0.0, 1.0: 0.0}}, 1.0: {'poetry': {0.0: 0.0, 1.0: 0.0}}}}, 1.0: {'honor': {0.0: {'poetry': {0.0: 0.0, 1.0: 0.0}}, 1.0: 0.0}}}}, 1.0: {'honor': {0.0: {'poetry': {0.0: 0.0, 1.0: {'wesley': {0.0: 0.0, 1.0: 0.0}}}}, 1.0: {'wesley': {0.0: {'poetry': {0.0: 0.0, 1.0: 0.0}}, 1.0: {'poetry': {0.0: 0.0, 1.0: 0.0}}}}}}}}, 1.0: {'honor': {0.0: {'romulan': {0.0: {'poetry': {0.0: {'wesley': {0.0: 0.0, 1.0: 0.0}}, 1.0: {'wesley': {0.0: 0.0, 1.0: 0.0}}}}, 1.0: {'wesley': {0.0: {'poetry': {0.0: 0.0, 1.0: 0.0}}, 1.0: {'poetry': {0.0: 0.0, 1.0: 0.0}}}}}}, 1.0: {'romulan': {0.0: {'wesley': {0.0: {'poetry': {0.0: 0.0, 1.0: 0.0}}, 1.0: {'poetry': {0.0: 0.0, 1.0: 0.0}}}}, 1.0: {'wesley': {0.0: {'poetry': {0.0: 1.0, 1.0: 1.0}}, 1.0: {'poetry': {0.0: 0.0, 1.0: 0}}}}}}}}}}, 1: {'barclay': {0.0: {'poetry': {0.0: {'honor': {0.0: {'wesley': {0.0: {'romulan': {0.0: 0.0, 1.0: 0.0}}, 1.0: {'romulan': {0.0: 0.0, 1.0: 0.0}}}}, 1.0: {'wesley': {0.0: {'romulan': {0.0: 1.0, 1.0: 1.0}}, 1.0: {'romulan': {0.0: 0.0, 1.0: 0.0}}}}}}, 1.0: {'romulan': {0.0: {'honor': {0.0: {'wesley': {0.0: 1.0, 1.0: 1.0}}, 1.0: 1.0}}, 1.0: {'wesley': {0.0: {'honor': {0.0: 1.0, 1.0: 1.0}}, 1.0: {'honor': {0.0: 1.0, 1.0: 0.0}}}}}}}}, 1.0: {'honor': {0.0: {'romulan': {0.0: {'poetry': {0.0: {'wesley': {0.0: 0.0, 1.0: 0.0}}, 1.0: 0.0}}, 1.0: {'wesley': {0.0: {'poetry': {0.0: 0.0, 1.0: 0.0}}, 1.0: 0.0}}}}, 1.0: {'wesley': {0.0: {'romulan': {0.0: {'poetry': {0.0: 0.0, 1.0: 1.0}}, 1.0: {'poetry': {0.0: 1.0, 1.0: 0.0}}}}, 1.0: {'romulan': {0.0: {'poetry': {0.0: 0.0, 1.0: 0.0}}, 1.0: {'poetry': {0.0: 0.0, 1.0: 0.0}}}}}}}}}}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isZmeDFY6Mc8"
      },
      "source": [
        "### 4.3 Experiment 2 ###\n",
        "\n",
        "- Obtain the test accuracy with the training set\n",
        "- Obtain the test accuracy with the test set\n",
        "- Obtain the test accuracies for the trees determined by using 100, 200, 300, 400, 500, 600, 700, and 800 training examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJvl9HlU6Mc9"
      },
      "source": [
        "def experiment_2_test(dtree_object, train_set):\n",
        "    ### START CODE HERE ###\n",
        "    # Test1: test with the training set\n",
        "    print(\"Test accuracy for the training set is\")\n",
        "    test_with_test_set(dtree_object, train_set)\n",
        "    \n",
        "    # Load the test data\n",
        "    df_test = pd.read_csv('test.csv')\n",
        "\n",
        "    # Test2: test with the test set\n",
        "    print(\"Test accuracy for the testing set is\")\n",
        "    test_with_test_set(dtree_object, df_test)\n",
        "    \n",
        "    # The training size\n",
        "    training_size = 100\n",
        "    \n",
        "    # Test 3: train the tree using training size of 100, 200, 300, 400, 500, 600, 700, 800\n",
        "    while(training_size <= 800):\n",
        "        df_partial = train_set.iloc[0:training_size, :]\n",
        "        \n",
        "        #Determine the majority class for df_partial\n",
        "        majority_class = dtree_object.Get_Majority_Class(df_partial['class'])\n",
        "        # Build up the decision tree based on df_partial\n",
        "        tree = dtree_object.ID3_No_Prune(df_partial, majority_class, df_partial.columns[:-1], target_attribute_name=\"class\")\n",
        "        \n",
        "        # test with the testing set for the tree generated from df_partial\n",
        "        print(f\"Test accuracy for a training set of %d data points is\"%training_size)\n",
        "        test_with_test_set(dtree_object, df_test)\n",
        "        # increase the train set by 100\n",
        "        training_size += 100\n",
        "    ### END CODE HERE ###"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9lG1Y6r6Mc-",
        "outputId": "cce90c6d-3452-4ba5-9640-eae84ff417cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "experiment_2_test(dtree_object, df_train)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy for the training set is\n",
            "The prediction accuracy is:  85.375 %\n",
            "Test accuracy for the testing set is\n",
            "The prediction accuracy is:  81.2807881773399 %\n",
            "Test accuracy for a training set of 100 data points is\n",
            "The prediction accuracy is:  74.38423645320196 %\n",
            "Test accuracy for a training set of 200 data points is\n",
            "The prediction accuracy is:  81.2807881773399 %\n",
            "Test accuracy for a training set of 300 data points is\n",
            "The prediction accuracy is:  84.23645320197043 %\n",
            "Test accuracy for a training set of 400 data points is\n",
            "The prediction accuracy is:  87.192118226601 %\n",
            "Test accuracy for a training set of 500 data points is\n",
            "The prediction accuracy is:  87.192118226601 %\n",
            "Test accuracy for a training set of 600 data points is\n",
            "The prediction accuracy is:  87.192118226601 %\n",
            "Test accuracy for a training set of 700 data points is\n",
            "The prediction accuracy is:  87.192118226601 %\n",
            "Test accuracy for a training set of 800 data points is\n",
            "The prediction accuracy is:  87.192118226601 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkRes4Ci6MdB",
        "outputId": "955bce96-f909-4303-d307-5b9d42bbca1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "x_test = [100, 200, 300, 400, 500, 600, 700, 800]\n",
        "y_test = [74.38, 81.28, 84.23, 87.19, 87.19, 87.19, 87.19, 87.19]\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Training size')\n",
        "plt.title('Accuracy vs Training size')\n",
        "plt.plot(x_test, y_test)\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qi-HkholxVj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Univariate_Linear_Regression_PartII.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nb6--HF5IvQ",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Packages ##\n",
        "\n",
        "First, you need to import all the packages that you will need during this assignment. \n",
        "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
        "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
        "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILDJ2DyG5IvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pw_liCc5IvV",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Problem Statement ##\n",
        "\n",
        "You are given a dataset containing:\n",
        "    - a training set for a linear function\n",
        "    - a test set for testing the learned hypothesis function\n",
        "    \n",
        "You will build a simple linear regression algorithm that can correctly identify the parameters of w0 and w1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzNF2Dzs5IvV",
        "colab_type": "text"
      },
      "source": [
        "## 3 - Forward and Backward propagation ##\n",
        "\n",
        "Forward Propagation:\n",
        "- You get X\n",
        "- You compute $h(x) = w_{1} * x + w_{0}$\n",
        "- You calculate the loss function:  $$L(W) = \\frac{1}{2m} \\sum_{i=1}^{n} \\left(h_{W}(x^{(i)})  - y^{(i)}\\right)^2$$. \n",
        "\n",
        "Here are the two formulas you will be using: \n",
        "\n",
        "$$ \\frac{\\partial L}{\\partial w_{1}} = \\frac{1}{m} \\sum_{i=1}^m (( w_{0} + w_{1} * x^{(i)} -y^{(i)}) * x^{(i)})\\tag{1}$$\n",
        "$$ \\frac{\\partial L}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^m (( w_{0} + w_{1} * x^{(i)} -y^{(i)}))\\tag{2}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWHzNybO5IvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimplifiedNetwork:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        #the weight associated with the single feature, a scalar\n",
        "        self.w_1 = 0 \n",
        "        #bias, a scalar\n",
        "        self.w_0 = 0\n",
        "        #learning rate\n",
        "        self.alpha = learning_rate\n",
        "        \n",
        "    def set_learning_rate(self, alpha):\n",
        "        \"\"\"\n",
        "        This function accepts the learning rate\n",
        "        Arguments:\n",
        "        alpha -- the learning rate\n",
        "        \"\"\"\n",
        "        self.alpha = alpha\n",
        "        \n",
        "    def forward_back_propagation(self, X, Y):\n",
        "        \"\"\"\n",
        "        This function forward and backward propagation\n",
        "        Arguments:\n",
        "        X -- data of the series of single feature\n",
        "        Y -- true \"label\" vector\n",
        "\n",
        "        Return:\n",
        "        loss -- outcome of the loss function\n",
        "        gradient -- dictionary containing dw_1 and dw_0\n",
        "\n",
        "        \"\"\"\n",
        "        #number of training examples\n",
        "        m = X.shape[1]\n",
        "\n",
        "\n",
        "        loss = 0\n",
        "        dw_1 = 0 #gredient of w_1\n",
        "        dw_0 = 0 #gredient of w_0\n",
        "\n",
        "        weight = self.get_weights()\n",
        "        w0 = weight['w_0']\n",
        "        w1 = weight['w_1']\n",
        "\n",
        "        #iterate through all the training examples to\n",
        "        #    1. Calculate the loss\n",
        "        #    2. calcuate the accumulated gradient dw_1 and dw_0\n",
        "        for i in range(m):\n",
        "            #Your code starts from here\n",
        "            Y_hat = w0 + (w1 * X[0, i])\n",
        "            loss += (Y_hat - Y[0, i]) ** 2\n",
        "            dw_1 += (Y_hat - Y[0, i]) * X[0, i]\n",
        "            dw_0 += (Y_hat - Y[0, i])\n",
        "            #Your code ends here\n",
        "\n",
        "        #Use the accumulated loss and gredients to calculate the averaged counterparts\n",
        "        loss = loss / (2 * m)\n",
        "        dw_1 = dw_1 / m\n",
        "        dw_0 = dw_0 /m\n",
        "\n",
        "\n",
        "        gradients = {\n",
        "            \"dw_1\": dw_1,\n",
        "            \"dw_0\": dw_0\n",
        "        }\n",
        "\n",
        "        return gradients, loss\n",
        "    \n",
        "    \n",
        "    #Function predict: \n",
        "    #                Predict the value using learned linear regression parameters (w, b)\n",
        "    def predict(self, x):\n",
        "        '''\n",
        "        Predict the value using learned linear regression parameters (w_0, w_1)\n",
        "\n",
        "        Arguments:\n",
        "        X -- data set of single feature\n",
        "\n",
        "        Returns:\n",
        "        Y_prediction -- predictions for all items in X\n",
        "        '''\n",
        "        ## Your code starts here ##\n",
        "        # Hint: You can use matrix/array operation. \n",
        "        # For example, if B is a matrix, 2 * B ends up with every item in matrix B being multiplied by 2\n",
        "        weight = self.get_weights()\n",
        "        w0 = weight['w_0']\n",
        "        w1 = weight['w_1']\n",
        "\n",
        "        A = w1 * x + w0\n",
        "        ## Your code ends here ##\n",
        "        return A\n",
        "\n",
        "    \n",
        "    def get_weights(self):\n",
        "        weights = {\n",
        "            'w_1': self.w_1,\n",
        "            'w_0': self.w_0\n",
        "        }\n",
        "        return weights\n",
        "\n",
        "\n",
        "    def fit(self, X, Y, epochs=1, print_loss = True):\n",
        "        \"\"\"\n",
        "        This function optimizes w_0 and w_1 by running a gradient descent algorithm\n",
        "\n",
        "        Arguments:\n",
        "        X -- data of the single feature\n",
        "        Y -- true \"label\" vector \n",
        "        num_iterations -- number of iterations of the optimization loop\n",
        "        learning_rate -- learning rate of the gradient descent update rule\n",
        "        print_loss -- True to print the loss every 100 steps\n",
        "\n",
        "        Returns:\n",
        "        params -- dictionary containing the weights w_1 and bias w_0\n",
        "        grads -- dictionary containing the gradients of the weights and bias with respect to the loss function\n",
        "        losses -- list of all the losses computed during the optimization, this will be used to plot the learning curve.\n",
        "\n",
        "        Tips:\n",
        "        You need to finish the following steps:\n",
        "            1) Calculate the loss and the gradient for the current parameters. Use propagate().\n",
        "            2) Update the parameters using gradient descent rule for w_0 and w_1.\n",
        "        \"\"\"\n",
        "        clr = ['b', 'g', 'c', 'm', 'y', 'k'] #color for labeling lines\n",
        "        losses = []\n",
        "        for i in range(epochs):\n",
        "            ##Your code starts from here##\n",
        "            gradients, loss = self.forward_back_propagation(X, Y)\n",
        "\n",
        "            dw_1 = gradients['dw_1']\n",
        "            dw_0 = gradients['dw_0']\n",
        "\n",
        "            self.w_1 = self.w_1 - (learning_rate * dw_1)\n",
        "            self.w_0 = self.w_0 - (learning_rate * dw_0)\n",
        "            ##Your code ends here##\n",
        "\n",
        "            # Print the loss every 200 training examples\n",
        "            if print_loss and i % 200 == 0:\n",
        "                losses.append(loss)\n",
        "                print (\"At Epoch %i, the loss = %f; w_0 = %f; w_1 = %f\" %(i, loss, self.w_0, self.w_1))\n",
        "                plt.plot(X, self.predict(X), marker = 'o', color=clr[int((i/200) % len(clr))], label='Epoch' + str(i))\n",
        "            \n",
        "\n",
        "        \n",
        "        params = {\n",
        "            \"w_1\": self.w_1,\n",
        "            \"w_0\": self.w_0\n",
        "        }\n",
        "\n",
        "        gradients = {\n",
        "            \"dw_1\": dw_1,\n",
        "            \"dw_0\": dw_0\n",
        "        }\n",
        "\n",
        "        return params, gradients, losses"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlkm-lmy5IvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 2000, learning_rate = 0.5, print_loss = False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array\n",
        "    Y_train -- training labels represented by a numpy array (vector)\n",
        "    X_test -- test set represented by a numpy array\n",
        "    Y_test -- test labels represented by a numpy array (vector)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_loss -- Set to true to print the loss every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    model = SimplifiedNetwork()\n",
        "    model.set_learning_rate(learning_rate)\n",
        "    parameters, grads, losses = model.fit(X_train, Y_train, epochs, print_loss)\n",
        "    \n",
        "    Y_prediction_test = model.predict(X_test)\n",
        "    Y_prediction_train = model.predict(X_train)\n",
        "    \n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs((Y_prediction_train - Y_train)/Y_train) * 100)))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs((Y_prediction_test - Y_test)/Y_test) * 100)))\n",
        "\n",
        "    d = {\"losses\": losses,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w_1\" : model.get_weights()['w_1'], \n",
        "         \"w_0\" : model.get_weights()['w_0'],\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"epochs\": epochs}\n",
        "    plt.title(\"Learning rate = \"+ str(d[\"learning_rate\"]))\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geBYkK795Ivc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "51be2e78-936b-4b77-ae56-67022210e799"
      },
      "source": [
        "df = pd.read_csv('train.csv', header=None)\n",
        "\n",
        "X_train = df[0].values.reshape(-1, 1).T\n",
        "Y_train = df[1].values.reshape(-1, 1).T\n",
        "\n",
        "df_test = pd.read_csv('test.csv', header=None)\n",
        "X_test = df_test[0].values.reshape(-1, 1).T\n",
        "Y_test = df_test[1].values.reshape(-1, 1).T\n",
        "\n",
        "\n",
        "plt.scatter(X_train, Y_train, color='r')\n",
        "\n",
        "##Your code starts from here##\n",
        "epochs = 1000\n",
        "learning_rate =  0.01\n",
        "##Your code ends here##\n",
        "\n",
        "\n",
        "d = Run_Experiment(X_train, Y_train, X_test, Y_test, epochs, learning_rate, print_loss = True)\n",
        "print(\"w_1 is \" + str(d['w_1']) + \" and w_0 is \" + str(d['w_0']))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "At Epoch 0, the loss = 175.580806; w_0 = 0.177884; w_1 = 1.081234\n",
            "At Epoch 200, the loss = 2.430920; w_0 = 3.223999; w_1 = 2.644695\n",
            "At Epoch 400, the loss = 0.940139; w_0 = 4.912268; w_1 = 2.394766\n",
            "At Epoch 600, the loss = 0.380028; w_0 = 5.947105; w_1 = 2.241570\n",
            "At Epoch 800, the loss = 0.169585; w_0 = 6.581416; w_1 = 2.147668\n",
            "train accuracy: 97.57197826197955 %\n",
            "test accuracy: 97.36067110998381 %\n",
            "w_1 is 2.0903326295089353 and w_0 is 6.968712318098739\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5RU5Z3u8e+vm4tcVBCFJRhEJ+NEzCTE0ytmjicZpTrGjE4wziTLRBQvHJSOg44ab+QcY2ZQ5OD1xAsMXlBrZNQYYUSjdoNh4hlNWoeIijGGiBEMqAFvIJfu9/yxdzW7qqu6qvbeVbWr+vms1YvuXbuq3m7h8e3fezPnHCIiUn+aat0AEREJRwEuIlKnFOAiInVKAS4iUqcU4CIidUoBLiJSpxTgkkhm9mUz+02t2yGSZApw6cXM3jCz1lq2wTn3H865v6hlGzLM7Bgze6tG7/1dM1tvZh+b2SNmtl8f904ys+fNbJv/56TAY8ea2Uoze9/M3qhK46XiFOBSE2bWXOs2AJgnkf8OzOwIYAFwGjAG2AbcWuDeQcBS4D5gJLAYWOpfB/gYuBP4foWbLVWUyL+4kkxm1mRml5nZ78zsPTN7INgjNLMHzeyPfi9vlR9AmcfuNrPbzOwxM/sYONbv6V9sZi/6z/k3M9vLvz+r19vXvf7jl5jZ22a20cymm5kzs08X+D6eNrM5ZvYMXigeamZnmtlaM/vQzNaZ2Tn+vcOAx4GxZvaR/zG22M8iJqcC/+6cW+Wc+wj4X8DJZrZ3nnuPAQYANzrndjjnbgYMmAzgnPulc+5eYF3MbZQaUoBLOf4BOAn4a2AssAW4JfD448CfA6OBF4B0zvO/C8wB9gZ+4V/7NnA8cAjwOeCMPt4/771mdjxwIdAKfBovzIo5DZjht2U9sBk4EdgHOBO4wcyOdM59DHwd2OicG+5/bCzhZ9HDzMab2dY+Pr5boI1HAL/OfOGc+x2wEziswL0vuuy9MV70r0uDGlDrBkhdORc4zzn3FoCZ/RB408xOc87tds7dmbnRf2yLme3rnHvfv7zUOfeM//knZgZwsx+ImNm/Az112zwK3ftt4C7n3MuB9z61yPdyd+Z+3/LA5z83syeBL+P9jyifPn8WwRudc28CI4q0J5/hwPs5197H+59OlHulQagHLuU4GPhppucIrAW6gDFm1mxmc/2SwgfAG/5z9g88/w95XvOPgc+34QVRIYXuHZvz2vneJ1fWPWb2dTN71sz+5H9vf0N223MV/FmU8N6l+gjvN4KgfYAPI94rDUIBLuX4A/B159yIwMdezrkNeOWRKXhljH2BCf5zLPD8Sm19+TZwUODrT5XwnJ62mNlg4CfAfGCMc24E8Bh72p6v3X39LLL4JZSP+vgo9NvCy8DnA69zKDAYeK3AvZ8z/9ca3+f869KgFOBSyEAz2yvwMQC4HZhjZgcDmNkBZjbFv39vYAfwHjAUuLqKbX0AONPMDjezoXiDfeUYhBeM7wC7zezrwHGBxzcBo8xs38C1vn4WWZxzbwbq5/k+cscKMtLA35o3J34Y8CPgYedcvl7103i/Acwys8Fmdp5/fYXfviZ/0Heg96XtFZihInVKAS6FPAZsD3z8ELgJWAY8aWYfAs8CR/n334M3GLgBeMV/rCqcc48DNwMrgdcD772jxOd/CMzC+x/BFrzfJpYFHn8VuB9Y55dMxtL3zyIWfo3+XLwg34z3P8m2zONm9riZXeHfuxNvUPV0YCtwFnCSfx3gK3j/HR8DxvufPxlne6X6TAc6SKMxs8OBl4DBuQOKIo1EPXBpCGb2Tb90MBK4Fm/+tMJbGpoCXBrFOXhlht/h1YJn1rY5IpWnEoqISJ1SD1xEpE5VdSXm/vvv7yZMmFDNtxQRqXvPP//8u865A3KvVzXAJ0yYQGdnZzXfUkSk7pnZ+nzXVUIREalTCnARkTqlABcRqVNFA9zfM+GXZvZrM3vZzK7yrx9iZs+Z2ev+5vraV0FEpIpK6YHvACY75z6Pt//y8Wb2JbzVbjc45z6Nt3/E2ZVrpoiI5Coa4M7zkf/lQP/D4R3V9JB/fTHeRjoiIuJLr0kz+J8GY1dZz0frPfGdF15SDdzfrH813lLlp/CWK28N7DXxFjCuwHNnmFmnmXW+8847cbRZRCTR2pa3YVcZUx+eys7unVmPdfy+I7YQL2keuHOuC5hkZiOAnwKfKfUNnHMLgYUALS0tWrcvIg1t5NyRbN2xtc97On7fEct7lbWQxzm31cxWAn8FjDCzAX4v/CC8faBFRPqlI245glfefaWq71nKLJQD/J43ZjYE+Cre+X8rgb/3b5sGLK1UI0VEksyusqqHN5RWAz8QWGlmLwK/Ap5yzj0KXApcaGavA6OAOyrXTBGR5Gm9pxW7yorfmCN1SCqW9y9aQnHOvQh8Ic/1dcAXY2mFiEgdKaXOzZ/NgnFTyD7XGwZ+sIb2KbNiaUdVN7MSEalnrfe0Fh+A/Ow82K/F+9x698537fs5Wlevpn3SpMjtUYCLiJSg6CDln82Ccf5ymDzBHdSxtUjvvUQKcBGRPpQ0u+TLT4IN6BXc8y6Clheyb+08Ei65Lp62KcBFRPJoW97GbZ23Fb4ht8bth3eqHf7xOhj6iX8552ktL3jBzvPR26gAFxEJKBrcULTHbbkXAqzzSFpi6oIrwEVEfEVnlxy5CIYf6n0eCO/lX4Mh/op5K6ELbjF1wRXgItLvFZ1dkmeActaNMGXpnny24IW+xjCN3oXxkBTgItJvlVbn7j2z5MnJMMD5OZ1qh9lz/Hsq1tS8FOAi0i+Nu24cGz/amP/BAqWSRWfAof7xwhasb5cb3DEFvQJcRPqVPsslBYI7K6ujBDeAgxEjq7SUXkSkEfQZ3Aek4DNXeKFdKLhhz2hlyOAGGNEJk35/GNwa4jVyKMBFpKGl16SZ+vDU/A8ekIJPnwcD980anDwpsLeq9RqtLLMBfnAPWQdHTc+8xu1wa/QEV4CLSMMqOC3wgBR85hKwgVkLcC6f423RapCd5BF63E0fwFdyD5x08ZxtowAXkYZTsFxSYKOpR06EfT72czprpDLEm/vZPPYROOzmEM8vgwJcRBpGn/uWHL0cmof0BHfuIkmLEtyZDvUOOHw+jCl2Ytrw4WW+QX4KcBGpe30OUObM5U61w+XXQFO3v2py+iIYs8l/vMw3zgT3djjmhBKfM2AA3H57mW9U4KVieRURkRopWOfOCe6sknaqHS65Gga68HOyHQzcDEefUsZzDj4Y5syBU08N+abZFOAiUrfyHmf2xSWw12j/hpzgzhS7IVJw0wWHzy2hVJIxc2Yss05yKcBFpK6k16Q565Gz2Nm9M/uBnOCGwLTtmGaU4GDs0jIGJ2PucedSgItI3cg7pzunVJI1OLnk77HR7/mPhXjDfHO4ixk1Cm66qWKhHaQAF5HES69JM33pdD7p+iT7gcC+3FmbAcaw3B3KDO5hw2DBgqoEd4YCXEQSq+BugYH53LNusj3VkSV/DzH0uMsanKxBcGcowEUkkXrtFnhACg6dDoNHA96eJU9OhgGT27HHr4XBu737wgb3Ljh8XhkDk83NMGNGRQYnS6UAF5HEGfrPQ9netd374oAUHHZhzyKcVDtcnMnrn7Vig7qizSgpNbgrNJMkiqIBbmafAu4BxuB9uwudczeZ2Q+B/wm84996hXPusUo1VEQaW979uTOlEr/G/Y1l0ORiqnE7OPzqEoK7hiWSYkrpge8GLnLOvWBmewPPm9lT/mM3OOfmV655ItIf9JrPHdiXe97F5mV1qh3710UwelPxY8vyKXcqYHMzLF6cyODOKBrgzrm3gbf9zz80s7XAuEo3TEQaW94BysDgZFZwL78ehmxvyKmAUZRVAzezCcAXgOeAo4HzzOx0oBOvl74lz3NmADMAxo8fH7G5ItIIem06lbMIZ/nXYMg/X4TNjz4VsOQZJYMGwZ13Jj60g8yVuC+tmQ0Hfg7Mcc49bGZjgHfxfkz/BBzonDurr9doaWlxnZ2dEZssIvWqV687sAhn+fHGkMziyuVfwyKefJN3H+5CEt7jNrPnnXMtuddL6oGb2UDgJ0DaOfcwgHNuU+DxfwEejamtItJgegX3ASn4zOVgTWBGx7FgmdDOiOPkm3xmzoTHHoM334Tx4yu61L3SSpmFYsAdwFrn3PWB6wf69XGAbwIvVaaJIlLPsmaXBIJ70Znmbb8960ZYsdTL66hnTV7Sx30Jnk0SVik98KOB04A1Zrbav3YF8B0zm4T343sDOKciLRSRupTV6/6zWTBuCmAs+ZZ5iyUXnYHFcPJNSYOTCZzDHYdSZqH8gvw/Xs35FpFe0mvSnPHTM9jt/JWR/gBlqsOY/Ww7LJmDNfs3R5jDXdJUwKYmOOechgxv0EpMEYlRVrnE73XPu9ho+R83wZSlWIrKb+eaSkF7e4g3qT8KcBGJbNCPBrHL7fK+8Odyz7oRpiwzLLOla6XLJHU4DTAqBbiIhJa1P7dfKpl3MV6P+/yl2AX+jSFnlPQ5MDl4MOzcWfczSaJQgItI2bIGKP0e95Jvwehrz4T56yPPKCk6h7tBByXLpQAXkbIMvWoQ290u+EsvuOddDC1XfxUe7Kr8VMAKH1FWbxTgIlKStuVt3Par27KD+5SLYf4LkYO7zxq3QrsgBbiI9KnnOLPdn8BRS1hy2mhG3/ItmP9eZYMbVCopQgEuIr2l0zB7NkO/s57tn0rBF+9n3uX70rLlRnhwWbjgLjYV0D9JHucScdpNPVCAi4jHD23Wr+eIc+GVMyC142dcvGwVg8+dBvM/CB/cxeZvDxgAd9+tMkmZFOAi4oX3jBm0/fU2bjsDUh8u4f9eNRpbfjx2XLRdAfscmFRPOxIFuEh/lelxv/kmNDUx6NIudh05jyXntDD62jOxFSH3KSllH+6hQ2HhQvW4I1KAi/Q36TRMmwZdXQBeuaT1Gtp3X0vTfpfAg0SqceftcQ8YAPvuC3/6U79eeBM3BbhIf5JOw1Rv5WT6L2Hq92exfOsvGLKfl7gWolTinJf1BUslCT8soZ4pwEX6k+nTvR733y1n1tpnWHHg1XBQtOAeVKhUoimAFacAF2lEwfq2X7Joe/FqbrvuaJ46YDXNzSfAZ6MF90j1uGtOAS7SKALTALOsX88VL9/Lt1p/w7cGeAcJhw5uBxOvhjEdeW4YPhxuv13BXUUKcJFGkE7DWWd5u/MFLL3lc+xz+It8lSdChTbklEq+Y9DdHb29EgsFuEgjOP/8rPBedsvn2PvwF9mHFyMH99DgcveZ50ZuqsSnqdYNEJEQ0mmYMME7MmzCBHjvvZ6H2h8Yxd6He8EdtlTCTpg4B46ZHAjvVEqDkgmjHrhIPWlr8+rMzu25tn49r6UmsuHCdTDkE5p5r+zgzrycbWtm4g1d2TXupia45x7VthNIAS5SL9ra4Lbbsi49vWgs7tCNwCvhe9uAbR/EMSfsBLqyb+iHx5TVEwW4SL0IhPfqWWPZctJGYGP44Hawq/1LHHfNs8DO3jdpOmDiKcBFkibPHG6eeQaAp5cPxA3ZRaTg7oaP7/kaJ97zBPBs9g1mcO65qnXXCQW4SJL4uwKybZv39fr1MHUqz8w7iJ0rAHZFGJgcyMf/OtkP7if2PKiedt1SgIskyezZe8IbWLVoJF2HbgHeihTcv102mRm3BoJ74kR4+eU4Wiw1VHQaoZl9ysxWmtkrZvaymZ3vX9/PzJ4ys9/6f46sfHNFGlA6Dfvv75Uv1q9nUwqefryJlSug69AtZU8HdM7/2DaE3z70NY49fpcf3r5USuHdIErpge8GLnLOvWBmewPPm9lTwBlAh3NurpldBlwGXFq5poo0gOBy9+bmni1dAV6bBRungDMw6w53hoKDrndHMffZSbRf38GxPKESSQMrGuDOubeBt/3PPzSztcA4YApwjH/bYuBpFOAivRXao8QP7+zgDnn4jT+rZN2vT2Svux6lfU2HgrsfKKsGbmYTgC8AzwFj/HAH+CMwpsBzZgAzAMaPHx+2nSL1J532lrgHVkkGbUrB2isiBLfbc07w9pcmseVfVnP2mke9C/fdp+DuB0oOcDMbDvwEuMA594EFinLOOWdmLt/znHMLgYUALS0tee8RaTgFNpcCv8d9kv9FlOBedzCdt97BExNbab9vtfdYczMsXqzw7idKCnAzG4gX3mnn3MP+5U1mdqBz7m0zOxDYXKlGitSdnM2lYE9wO8LtUZIJbrd9EFf/Yjn/74C5bHuhle+/4D+ukkm/U8osFAPuANY6564PPLQMmOZ/Pg1YGn/zROpEH5tLPbMEnl7h97pDbDDVM6tk+yAeubGD1Btf5rBff5Vto88MPOjg3XcV3v1MKT3wo4HTgDVm5v+exhXAXOABMzsbWA98uzJNFEmwdBrOOQc+/njPNX+w8rlFsP1Q/1qIA4L31LeP5ITPzgfbzNhDU7g5Y2GDqpFS2iyUX1D4r18q3uaIJFhwift++8GHH/Yqk2xKwbrpsCMzpB82uB/5Bo+8cQE3X9ANI5dhKydz71PDOPW8++AG9bLFo5WYIqXIHZTMmVmyKQW/PQ9270vIeYD+H+sOpvPWu7jkik4YOZmmp+GcfVPc+iMHP4ryDUgjUoCLlOLcc/POKNmUgrWX4v1LihLcm0exbdqD3HBhNx1nz2Hi2rW8/EOVSaRvOpFHJJ/g8nYz+OijrIdfmwVPd8Da2cBAQpVKcF6P+5PjVjDn1w9w4vVz6BjUSuqDt3n5Bxti+kakkakHLpJRaMVkwKYUvHYhdA0hdI87M4d72/fu4oaLHB03zYF3Ohi2dRgL/u4+Tv1L1bilNApwEei9jWuOzHJ3jPKCO1AFcbsM5l3B5tUpTnkQWPQI/O5meAfuO1nBLeVTgIuk03D66dDd3euh3FWTZXHg5syGjlYA1h3smH4X8MlmWHUKABP3n8jL39POgBKOAlz6lzznSuYTusftcw66lk5h7lEpOmb73fCP1sEq74j3EYNHsOWyLeW/sEiAAlz6jyLhvXoebG0JXIhQ49756BSOnzIL1s6BVR1Zt8xsmcmtJ+jIMolOAS6Nq8hugBmZXQFD9baDNe5te8ENF9G5pZVL5nfD2qvhnT3hrXKJxE0BLo0pnYYzz4RduwreErVMgvPmb3PKQzhg6RS4ebYDtwvWXou9s4J7NTgpFaQAl8Y0e3bB8M4qlYQJbvDC+4NhbP7eQ5yyEq/o3b0D1s7v6XUrvKXSFODSWPoom0QO7syqSb+7vW7pBUx/0MHO9+H1H/cE9wAbwN3fvFvhLRWnAJf6lE57y9uDKyQHDSrpAIWyZVZNLp2Cu/kCr1RyvoMp2XXuZppZfPJiBbdUjQJc6ktbG9x+u98NzpET3rH0uB24q2fjOlq94F4RmBL4wnT/pY1zW87VzBKpOgW4JF9bGyxYkHehTT6xlUo2j8Kd8hBXz2bPXO4N/upJn6YESi0pwCXZSlx4AxGnA4IX3N3grvFWT24eBaes8LvhG5ZmBbdh3HvyvSqXSE0pwCWZSpzDDTH2uNcdjJt+t1cqycws+VMnvHRJ1u3qdUtSKMAlWfIdUZbHphS8eim4zN/gsAtw/MFJbr7A63H3TAnsglfnZi3ESR2Sov309jLfSKRyFOBSW8FjyoYN67Xvdq5NKVh7MTCYSL1t/Pq2/2kguLvh1WuygnvYwGEs+NsFKpdI4ijApXZyjykrEt49pZIIM0rwpwICdB4Jl1yHf6o7WTNLMrTNqySZAlyqq4zadkakGnemxz3Hmwq4y2BeZofXTHB/shl+eUrW08YOH8uGi3QqjiSbAlyqp4wZJVn7lED44PZLJT17lVzAnuDOM0AJ6nVL/VCAS3WUGN7PLIFdo/0vIuwMiD+jpBu4JtjjzjM4maHdAqXeKMCl8koI71imAm4fBCc80VPuvjq3VFKgxz2keQjbfpD/KDWRJCt6Kr2Z3Wlmm83spcC1H5rZBjNb7X/8TWWbKYmXe4r7/vt710o4ROHpFYHByZCnu9N5JM4P784jIbUyEN67PoBVk/OG98yWmQpvqVul9MDvBn4M3JNz/Qbn3PzYWyT1I9+GUhnvvQdTp+Z9WtZUQIg2HdAvlQA8klvjhry97mZrZvE3temU1L+iAe6cW2VmEyrfFKkbIWaSZKx6BLr3IdI+3MHFN5AnuAuUSkCrKKWxRKmBn2dmpwOdwEXOubwntJrZDGAGwPjx4yO8nSRCCSfd5NqUgle/D26QfyFKj7vzSNwl17HuYJi+Mvi4Xyr5z5PyPVszS6Qhmcu3LWfuTV4P/FHn3Gf9r8cA7+L9s/on4EDn3FnFXqelpcV1dnZGaa/UUmsrdPSevVFIHEeW9fCDGwI9btizrWzOLoEZOv1dGoGZPe+ca8m9HqoH7pzbFHjhfwEejdA2SbK2Nli4ELq6Sn5KbMEdOG8SCgR3ntWTGep1S6MLFeBmdqBz7m3/y28CL/V1v9SpMnvcm1Kw9jKgmUibS2VW3ATGKfHHKf3g7r29a5A2nZL+omiAm9n9wDHA/mb2FnAlcIyZTcL75/YGcE4F2yi1kE6XHN7PLYLth/pfhA3uno1JsrYt6d3j7mOAUuUS6W9KmYXynTyX76hAWyQp0mk47bSit8Vy1uRug+NW9OT4jgEw/1J/DjeUVCoBzS6R/kkrMfur3KmATU0lHVm2KQVrL2fPErCY5nAHOuD+Pf5NeTaaCtLyd+nPFOD9TaHFN0XCOyu4o4T2+/vAj/8BOlqDHfCce5163CIlUID3BxEW3rw2CzZ+g/DBDV54B0YiCw9OAt074Dfz8242BdrmVSRIAd7ocg9NKFHk+jb4Ne4muPZynF/UDnTA/XuKD05mqNctkk0B3mgyR5StX+9tKlXCQq2g2I4s6wLmzu4plWT1tnvudeB2w38c1+dLqs4tkp8CvJGk0zBjBmzzd9crI7wjzeGGPcEdKGoHOuB7etzBdhWpc6tcItI3BXijSKdh2rSyVkxCTPtwQ880kjwd8MC9xRfhgBbiiJRKAd4IyjiqLKNnV0CINbgLzipxu+HVawsOTgIMahrEnSfdqeXvIiVSgNebvvbgLkEsi29ypgJCnnnc4Ad34SPMgtTrFimfAryepNNwxhmwe3dZT8sKbQgf3IFlknmnAvbc2/cOgUFNNNF1ZXllHxHxKMCTrK0NFiwoaYVkPptS8Oo/ghtKrItveu1TkvWcvvflDtLsEpFoFOBJVeZOgLkinXyTp3tdtFSSuavIACVoPrdIXBTgSRJi7+2gTSl47ULoGuJfCHtAcKB7XVJwl9jjVnCLxEsBngTpNJxzDnz8cainR9rOFQr2uLsM5l6RMxWw5zkqlYjUmgK81iKUSmIJ7pwJ2w7oboJrchff9DzHwa734fUfF51ZYhj3nnyvpgWKVIgCvBaCy91DiHvxTYFLOc9x4HbBq/OKBjdoFaVINSjAqy3EopuMZ5bArtH+FzGcNRm89MEwOCnfyaYlrp7MesqV5e2/IiLhKMCrIUKPe1MKfnse7N7XvxDzrJJH+poOWOIinAwdIixSXQrwSoiw/3ZQT407ynau0JPSwX5xnz1u1w2vXlNycGuQUqQ2FOBxi1AigZz6NoQ/JDjncMmCp9/0PK/0fbkzdIiwSG9Dh8L27YUfT6WgPaZdI5qK3yIlaW319t+OEN5PP+WHtwU+yuHwRiEnr4SvP5W1cnLdwQXC27k9UwJXTS4pvEcNGcV9J9+n8BbB67OZ7fnoK7zBm3TWmm+GVwjqgYeVqWu/+SYMGgQ7doR6mdh63HkW4BRd8g4l7VeS0Uwzi09erDq39FsRF0j3iOM1QAEeTu7BCSHCO7apgDmjkEWnA0LJJ+EEaRWl9EcRK6IVpwAvVcS52xmxBXfOKGSmx3117iEKWc8tv86t4Jb+5Igj4JVXat2K0inA+xLTbJJYpwIG5nAHLxecDgihyiV7Ne/FoimLVC6RhhVxB4tIUql4XqdogJvZncCJwGbn3Gf9a/sB/wZMAN4Avu2ca6wRrXQazjwTdu2K9DI9e3FHCW3oVRPJPDSnlB53GcEN6nVL4yo2Q6Qa4pyFUkoP/G7gx8A9gWuXAR3Oublmdpn/9aXxNKnGklQqyTPvr0BHPOe55S/CAc3nlsYzbhxs3Fi79585E26tYF+oaIA751aZ2YScy1OAY/zPFwNP0wgB3tYGt99e1mnuQVknu0NFBieLBjeU3ePWcWbSCGod1hBv77oUYWvgY5xzb/uf/xEYU+hGM5sBzAAYP358yLergnQ6Ung/vRwYQmwn3wQvQwmzSgA+2Qy/PKWst1a5ROpVXFP6who1Cm66CU6t4TBR5EFM55wzs4Kp55xbCCwEaGlpScYuR3Evdc8IO4c7T7e66MpJCDWrJEOrKKXe1Dqwhw3zTjisZWDnChvgm8zsQOfc22Z2ILA5zkZVREy1bYhxKmCeE4HLmlWiHrc0sFoH9sSJ8HLCh4TCBvgyYBow1/9zaWwtqoSIte2MTSlYeyneTy3G+nbmoT5XTvbc6EL1uEHhLclV6/q1GZx7bmUHHCuhlGmE9+MNWO5vZm8BV+IF9wNmdjawHvh2JRtZluC5ks3NcMwxkf833jMVEGI7QCH3oT573BBqX+4MBbckTRJWONZDD7uYUmahfKfAQzFNRY9R7t+Krq5I4R1bqaTA4hsoNbiBtXPKmhIICm5JjiSscGyEwM5V/ysxYxqQDHptFmycQrgdAaFocPc5o6Tn5vL3K8nQcWZSazFVLUMbPBjuuCNZA46VUN8BHvPvYZtS8Oql4KLUuHcOhP9zSdbyyLKDG8qey52hU3Gk2tJpmDq1tm2o9vzrpKjfAI8pvGPbpyTndPfgQ9sHwQlPlPI63aFq3KDFOFI9tZ4d0l9616WozwBPpyOH96YUrL0c70iLmPcpCT5ctMYNkWaWaPm7VFoFqpRla8T6dRzqI8Bj/BsU24ySAotvoNeCyr5f7L1wwQ0apJTKqPUMkeHDvfq5etjFJT/AY/rbFMvAZB8TtR19HBScq7urrEODc6nOLXFKwgyRSm/61KiSHeAxlEog4unuRRbfAHQD1/S1ravPgH3X3cLWPxTajapvKpdIHGq9aKa/DjWzDikAAAuDSURBVDhWQrIDfPbsUE/blIK1FwODAxfD7FNSoMdd0uk3OcbaTjY+/TW2ltmMDPW6JYwk9K4V2JWT7AB/882ybs/azjVMbxu8ZN62F9xwUd4ZJVDiwGSPbuyVuWx856mym9JEE11XdpX9POnfal3DVjmkepId4PvtV9LA5aYUrL2C6AtvCkwFzNySZ++pPl7T9czlDrOWQQOUUoqRI2Fr2F/rYqLArp1kB/gnn/T58DNLYNdo/4soPe4Co4+Z4N0+BK6/sLRyyWAzutdey65Nj4dqjurc0pckHAl2332aIZIUyQ7wAqeNRt6jBLx03jEA5l+af/FNGaGdMfYPd7Fx3T3Fb8xD+3NLrlqXQjLUw06uZAd4jtg2l+qjxl3u4CTAiKYmtq78a8IO7GsVpYAX2AsWQHd3bd6/qck7pV1hXT+SHeCjRsF778UT3EXq21DkvMk8Up/8Jx3PXRF6ZonKJf1bOg1nnAG7d9euDepd17dkB/hNN7Fq+FS696Fii2+g/OCeOXYsrz3bRsfvwy3EUY+7f6p1SURh3XgSHeCrj7iL7i2EP2uywHy/MNMBm4HFhx8Om9uZev+xZTbIox53/1LrOdhakt74Eh3gW7d2lBfeRYrYDug2WPaNcuZxez3unz/1Tab+PPy/Ri3EaWzBI1ebm72zRKpNPez+J9EBXrISetx9lMDzGt7czO2HHcapY8Ywcu5Itu4IV+ke0jyEbT/YFuq5klx9lUOqFd7aoU+aat2ASDI97nUHw+SVeZe8d+Pl+ldXlh7eM8eO5cMvf5lnOq/CrrLQ4T2zZabCu0G0tXkH32Y+alHLHjvWP6jJ/1B4S6J74E2/+yLdh/6ydxmlxFklJZ2AE2DAvYcf7gX3/eH+hRrGvSffq3JJnavlgGO9npAu1ZfoAO+efi3MuwhaXthzMc+RZRlh53EPAbYdcwwAR9xyBK+8G67WrUHK+pROw7RptalbZ4wdCxt0jKmUKdEBDpTUhQ47HRC8csmthx1Gek2asx45i53dO8tvIxqkrDe1PhZM9WuJQ/IDvA9Rgjs1YgTtkyaRXpNm72v+Gx/t/Kjs99eGU/WhVuWQYcO8lZWaxieVkugAH5EawdaO3gOIYWvcmdAGSK9JY1d9IVS7mmlm8cmL1eNOqFr3rkFT+qQ6IgW4mb0BfIg3pLjbOdfS9zPKM6l9EqtbV7MlJ8TLDW7YUyoBIk0L1CrK5KllYGtnPqmlOHrgxzrn3o3hdfI6dc4OXvlB+OdPHDKEl486CoC25W3c1hnud+lhA4ex4G8XqNedALVekq5ZIpIUiS6htK5ezSshNz8e0dTElq98pefrcdeNY+NH5e8XqB537dWyh63l6JJkUQPcAU+amQMWOOcWxtCmHh0hjhoJBnd6TZrZHbNZ//76sl9n+KDh3H7i7epxV1mt9w/RwKPUk6gB/j+ccxvMbDTwlJm96pxbFbzBzGYAMwDGjx8f8e0KyyzCOXXMGCDafG7NLqmeJAw4ag621KtIAe6c2+D/udnMfgp8EViVc89CYCFAS0tLmOMhiwrWuaMEt1ZRVt64cbAx7MkXMdDsEGkkoQPczIYBTc65D/3PjwN+FFvL8Kb99VVGuS/Q406vSTP14amh3megDWTn/w63gEcKS6fh/PNLOpe6YtS7lkYWpQc+BvipmWVe51+dcz+LpVW+9kmTaF29uleI587nPnvp2ezo2hHqPTRIGa9alkS0ulH6m9AB7pxbB3w+xrbklQnqfIb+81C2d4WbpTJ2+Fg2XKSuWRS1ns4H6mFL/1aX28m23tOKXWWhwrvZmpnZMlPhHUJra+23VE2lsrdUVXhLf5boeeC5otS5RwwewZbLtsTcosZW6x62Gdx7r6b0iRRSNwEediEOqM5dqlrOwU6loF3/iUTKkvgAb72nNfTp75pdUlitF8yABh1Fokp0DTxKeI8dPlbhHTBuXHb9utrhvdde3sZPOhJMJD6J7oGHCW/NLvEkYYWjFs2IVFaiA7wc/X0VZa0DWzVskepriADvb2dRptNw1lmws4YVIm2pKlJ7iQ7w1CGpPsso/Wl2yciREGJzxtiohy2SPIkexGw/vZ3UIale1yfuPxF3pWvY8B46NHvA0ay64T18eO8BR4W3SPIkugcONGxIB9W6d61yiEh9SnyAN6Jab6kKOstRpBEkuoTSCNJpGDw4uxxSi/CeOTO7JKLwFql/6oHHrNb7h4BWOIr0F+qBR9TWBs3Ntd2hL7d3rfAW6R/UAy9TOg1Tw22IGBvVr0UE1APvU1tb7+l81Q7vwYN7T+lTeIsIqAfeS61r2Kpfi0ip+nWA13r/ENAKRxEJr1+VUHJLItUO79zjwLTCUUSiaOgAT6dhwoTknOGosBaRODVUCaXWJRGdkC4i1VS3PfB0uvcMkWqGt1nv2SEKbxGpproJ8Nz6dbWn8w0blh3Y3d2azicitZX4EkqtyiKjRsFNNymkRSS5IvXAzex4M/uNmb1uZpfF1aiMaoZ3U1P2kvR331V4i0iyhe6Bm1kzcAvwVeAt4Fdmtsw5F9t555UMb82/FpF6F6UH/kXgdefcOufcTmAJMCWeZsUvdw62wltE6l2UGvg44A+Br98Cjsq9ycxmADMAxo8fH+HtyjNiBGzZUrW3ExGpuorPQnHOLXTOtTjnWg444ICynpvqfRxmQblT+hTeItLoogT4BuBTga8P8q/Fpr29cIjrhBkR6e+ilFB+Bfy5mR2CF9ynAN+NpVUBqlWLiOQXOsCdc7vN7DzgCaAZuNM5p41QRUSqJNJCHufcY8BjMbVFRETKUDdL6UVEJJsCXESkTinARUTqlDnnqvdmZu8A60M+fX/g3RibUw/0PfcP+p77hyjf88HOuV4Laaoa4FGYWadzrqXW7agmfc/9g77n/qES37NKKCIidUoBLiJSp+opwBfWugE1oO+5f9D33D/E/j3XTQ1cRESy1VMPXEREAhTgIiJ1KvEBXulzN5PGzD5lZivN7BUze9nMzq91m6rFzJrN7L/M7NFat6VazGyEmT1kZq+a2Voz+6tat6nSzOwf/b/bL5nZ/Wa2V63bFDczu9PMNpvZS4Fr+5nZU2b2W//PkVHfJ9EBHjh38+vAROA7Zjaxtq2quN3ARc65icCXgO/1g+8543xgba0bUWU3AT9zzn0G+DwN/v2b2ThgFtDinPss3k6mp9S2VRVxN3B8zrXLgA7n3J8DHf7XkSQ6wKmzczfj4Jx72zn3gv/5h3j/oMfVtlWVZ2YHAScAi2rdlmoxs32BrwB3ADjndjrntta2VVUxABhiZgOAocDGGrcnds65VcCfci5PARb7ny8GTor6PkkP8HznbjZ8mGWY2QTgC8BztW1JVdwIXAJ017ohVXQI8A5wl186WmRmw2rdqEpyzm0A5gNvAm8D7zvnnqxtq6pmjHPubf/zPwJjor5g0gO83zKz4cBPgAuccx/Uuj2VZGYnApudc8/Xui1VNgA4ErjNOfcF4GNi+LU6yfy67xS8/3mNBYaZ2dTatqr6nDd/O/Ic7qQHeMXP3UwiMxuIF95p59zDtW5PFRwNfMPM3sArk002s/tq26SqeAt4yzmX+Q3rIbxAb2StwO+dc+8453YBDwP/vcZtqpZNZnYggP/n5qgvmPQA7zl308wG4Q12LKtxmyrKzAyvJrrWOXd9rdtTDc65y51zBznnJuD9N17hnGv4Xplz7o/AH8zsL/xLKeCVGjapGt4EvmRmQ/2/6ykafOA2YBkwzf98GrA06gtGOlKt0vrpuZtHA6cBa8xstX/tCv/4Omk8/wCk/Q7KOuDMGrenopxzz5nZQ8ALeDOu/osGXFZvZvcDxwD7m9lbwJXAXOABMzsbb1vtb0d+Hy2lFxGpT0kvoYiISAEKcBGROqUAFxGpUwpwEZE6pQAXEalTCnARkTqlABcRqVP/HxVqHlq0JX3AAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0LyCIQe5Ivf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXTCZZF75Ivi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
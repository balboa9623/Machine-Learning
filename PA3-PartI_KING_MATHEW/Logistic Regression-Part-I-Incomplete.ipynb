{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First, you need to import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- [jdc](https://alexhagen.github.io/jdc/) : Jupyter magic that allows defining classes over multiple jupyter notebook cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jdc\n",
    "import matplotlib.pyplot as plt\n",
    "from plotutil import plotData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Problem Statement ##\n",
    "\n",
    "    - In Section 2.1, implement the helper function sigmoid \n",
    "    - In Section 2.2, implement the helper function normalize \n",
    "            (Attention: when you call it, DON'T use self.normalize becuase it is not a part of the LogisticRegression class)\n",
    "    - In Section 2.3, define the LogisticRegression class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Sigmoid Function ###\n",
    "\n",
    "Define a helper function 1: $sigmoid(Z) = \\frac{1}{1 + e^{-Z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of Z\n",
    "\n",
    "    Arguments:\n",
    "    Z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(Z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Feature Scaling ###\n",
    "Define helper function 2 -- features normalization:\n",
    "$ \\frac{x_{i} - mean}{\\sigma}$, where $\\sigma$ is the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(matrix):\n",
    "    '''\n",
    "    matrix: the matrix that needs to be normalized. Note that each column represents a training example. \n",
    "         The number of columns is the the number of training examples\n",
    "    '''\n",
    "    # Calculate mean for each feature\n",
    "    # Pay attention to the value of axis = ?\n",
    "    # set keepdims=True to avoid rank-1 array\n",
    "    ### START YOUR CODE HERE ### \n",
    "    # calculate mean (1 line of code)\n",
    "    mean = \n",
    "    # calculate standard deviation (1 line of code)\n",
    "    std = \n",
    "    # normalize the matrix based on mean and std\n",
    "    matrix = \n",
    "    ### YOUR CODE ENDS ###\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Logistic Regress Class ###\n",
    "You will create a neural network class - LogisticRegression:\n",
    "    - initialize parameters, such as weights, learning rate, etc.\n",
    "    - implement the gredient descent algorithm\n",
    "    - implement the predict function to make predictions for new data sets\n",
    "    - implement the normalization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, num_of_features=1, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        This function creates a vector of zeros of shape (num_of_features, 1) for W and initializes w_0 to 0.\n",
    "\n",
    "        Argument:\n",
    "        num_of_features -- size of the W vector, i.e., the number of features, excluding the bias\n",
    "\n",
    "        Returns:\n",
    "        W -- initialized vector of shape (num_of_features, 1)\n",
    "        w_0 -- initialized scalar (corresponds to the bias)\n",
    "        \"\"\"\n",
    "        # n is the number of features\n",
    "        self.n = num_of_features\n",
    "        # alpha is the learning rate\n",
    "        self.alpha = learning_rate\n",
    "        \n",
    "        ### START YOUR CODE HERE ### \n",
    "        #initialize self.W and self.w_0 to be 0's\n",
    "        self.W = \n",
    "        self.w_0 = \n",
    "        ### YOUR CODE ENDS ###\n",
    "        assert(self.W.shape == (self.n, 1))\n",
    "        assert(isinstance(self.w_0, float) or isinstance(self.w_0, int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Gradient Descent ##\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X with its shape as (n, m)\n",
    "- You compute  $$h_{W}(X) = a = \\sigma(w^T X + w_{0}) = \\frac{1}{1 + e^{-(w^T x + w_{0})}}\\tag{1}$$\n",
    "- You calculate the loss function:  $$L(W) = \\frac{1}{m} \\sum_{i=1}^{m}- y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{2}$$. \n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ dw_{j} =\\frac{\\partial L}{\\partial w_{j}} = \\frac{1}{m} \\sum_{i=1}^m (( h_{W}(x^{(i)}) -y^{(i)}) * x_{j}^{(i)})\\tag{3}$$\n",
    "$$ dw_{0} = \\frac{\\partial L}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^m (h_{W}(x^{(i)}) -y^{(i)})\\tag{4}$$\n",
    "\n",
    "The weights will be updated:\n",
    "$$ w_{j} = w_{j} - {\\alpha} * dw_{j}\\tag{5}$$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to LogisticRegression\n",
    "def fit(self, X, Y, epochs=1000, print_loss=True):\n",
    "    \"\"\"\n",
    "    This function implements the Gradient Descent Algorithm\n",
    "    Arguments:\n",
    "    X -- training data matrix: each column is a training example. \n",
    "            The number of columns is equal to the number of training examples\n",
    "    Y -- true \"label\" vector: shape (1, m)\n",
    "    epochs --\n",
    "\n",
    "    Return:\n",
    "    params -- dictionary containing weights\n",
    "    losses -- loss values of every 100 epochs\n",
    "    grads -- dictionary containing dw and dw_0\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        # Get the number of training examples\n",
    "        m = X.shape[1]\n",
    "\n",
    "        ### START YOUR CODE HERE ### \n",
    "        # Calculate the hypothesis outputs A (≈ 2 lines of code)\n",
    "        Z = \n",
    "        A = \n",
    "        # Calculate loss (≈ 1 line of code)\n",
    "        loss = \n",
    "        \n",
    "        # Calculate the gredients for W and w_0\n",
    "        dw = \n",
    "        dw_0 = \n",
    "\n",
    "        # Weight updates\n",
    "        self.W = \n",
    "        self.w_0 = \n",
    "        ### YOUR CODE ENDS ###\n",
    "\n",
    "        if((i % 100) == 0):\n",
    "            losses.append(loss)\n",
    "             # Print the cost every 100 training examples\n",
    "            if print_loss:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, loss))\n",
    "\n",
    "\n",
    "    params = {\n",
    "        \"W\": self.W,\n",
    "        \"w_0\": self.w_0\n",
    "    }\n",
    "\n",
    "    grads = {\n",
    "        \"dw\": dw,\n",
    "        \"dw_0\": dw_0\n",
    "    }\n",
    "\n",
    "    return params, grads, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions ###\n",
    "The predicted output is calculated as $h_{W}(X) = \\sigma(W^T * X + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to LogisticRegression\n",
    "def predict(self, X):\n",
    "    '''\n",
    "    Predict the actual values using learned parameters (self.W, self.w_0)\n",
    "\n",
    "    Arguments:\n",
    "    X -- data of size (n x m)\n",
    "\n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "\n",
    "    # Compute the actual values\n",
    "    ### START YOUR CODE HERE ### \n",
    "    Z_prediction = \n",
    "    A = \n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "\n",
    "        # Convert probabilities A[0,i] to actual predictions Y_prediction[0,i]\n",
    "        ### START CODE HERE ### (≈ 3 lines of code)\n",
    "        if A[0, i] <= :\n",
    "            Y_prediction[0, i] = \n",
    "        else:\n",
    "            Y_prediction[0, i] = \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    ### YOUR CODE ENDS ###\n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the Experiments ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 2000, learning_rate = 0.5, print_loss = False):\n",
    "    \"\"\"\n",
    "    Builds the multivariate linear regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array \n",
    "    Y_train -- training labels represented by a numpy array (vector) \n",
    "    X_test -- test set represented by a numpy array\n",
    "    Y_test -- test labels represented by a numpy array (vector)\n",
    "    epochs -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_loss -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    num_of_features = X_train.shape[0]\n",
    "    model = LogisticRegression(num_of_features, learning_rate)\n",
    "    \n",
    "    \n",
    "    ### START YOUR CODE HERE ###\n",
    "    # Obtain the parameters, gredients, and losses by calling a model's method (≈ 1 line of code)\n",
    "    parameters, grads, losses = \n",
    "    ### YOUR CODE ENDS ###\n",
    "    \n",
    "    ### START YOUR CODE HERE ###\n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = \n",
    "    Y_prediction_train = \n",
    "    ### YOUR CODE ENDS ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    W = parameters['W']\n",
    "    w_0 = parameters['w_0']\n",
    "    print(\"W is \" + str(W))\n",
    "    print(\"w_0 is \" + str(w_0))\n",
    "    \n",
    "    d = {\"losses\": losses,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"W\" : W, \n",
    "         \"w_0\" : w_0,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"epochs\": epochs}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Start the Learning Process ###\n",
    "You can change num_iterations and learning_rate to see the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from plotutil import plotData\n",
    "\n",
    "data = np.loadtxt('pa3-data1-train.csv', delimiter=',')\n",
    "X_train = data[:,:-1].T\n",
    "y_train = data[:,-1].T\n",
    "\n",
    "#plot data\n",
    "plotData(X_train, y_train, \"1st exam\", \"2nd exam\")\n",
    "\n",
    "data_test = np.loadtxt('pa3-data1-test.csv', delimiter=',')\n",
    "X_test = data_test[:, :-1].T\n",
    "y_test = data_test[:, -1].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = Run_Experiment(X_train, y_train, X_test, y_test, epochs = 2000, learning_rate = 0.02, print_loss = True)\n",
    "# Plot learning curve (with costs)\n",
    "losses = np.squeeze(d['losses'])\n",
    "plt.plot(losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the learning curve ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(normalize(X_train), y_train, xlabel=\"1st exam\", ylabel=\"2nd exam\", w = d[\"W\"], b = d[\"w_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

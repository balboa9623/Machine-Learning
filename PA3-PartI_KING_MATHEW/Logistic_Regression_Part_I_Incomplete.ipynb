{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Logistic Regression-Part-I-Incomplete.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBK3F0fYOAz3"
      },
      "source": [
        "## 1 - Packages ##\n",
        "\n",
        "First, you need to import all the packages that you will need during this assignment. \n",
        "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
        "- [pandas](pandas.pydata.org/) is an important package for Python data analysis.\n",
        "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
        "- [jdc](https://alexhagen.github.io/jdc/) : Jupyter magic that allows defining classes over multiple jupyter notebook cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv4S1lOoOAz4",
        "outputId": "7fc03941-856a-4a95-821b-d1021eb8f70a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install jdc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import jdc\n",
        "import matplotlib.pyplot as plt\n",
        "from plotutil import plotData"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jdc in /usr/local/lib/python3.6/dist-packages (0.0.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGmsfw77OAz8"
      },
      "source": [
        "## 2 - Problem Statement ##\n",
        "\n",
        "    - In Section 2.1, implement the helper function sigmoid \n",
        "    - In Section 2.2, implement the helper function normalize \n",
        "            (Attention: when you call it, DON'T use self.normalize becuase it is not a part of the LogisticRegression class)\n",
        "    - In Section 2.3, define the LogisticRegression class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUAEfNrkOAz8"
      },
      "source": [
        "### 2.1 - Sigmoid Function ###\n",
        "\n",
        "Define a helper function 1: $sigmoid(Z) = \\frac{1}{1 + e^{-Z}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgWD2k5OOAz9"
      },
      "source": [
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of Z\n",
        "\n",
        "    Arguments:\n",
        "    Z -- A scalar or numpy array of any size.\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(Z)\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    s = 1.0/(1+np.exp(Z))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return s"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5SzDmUTOA0A"
      },
      "source": [
        "### 2.2 - Feature Scaling ###\n",
        "Define helper function 2 -- features normalization:\n",
        "$ \\frac{x_{i} - mean}{\\sigma}$, where $\\sigma$ is the standard deviation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GocEM5_sOA0A"
      },
      "source": [
        "def normalize(matrix):\n",
        "    '''\n",
        "    matrix: the matrix that needs to be normalized. Note that each column represents a training example. \n",
        "         The number of columns is the the number of training examples\n",
        "    '''\n",
        "    # Calculate mean for each feature\n",
        "    # Pay attention to the value of axis = ?\n",
        "    # set keepdims=True to avoid rank-1 array\n",
        "    ### START YOUR CODE HERE ### \n",
        "    # calculate mean (1 line of code)\n",
        "    mean = np.mean(matrix, axis=1, keepdims=True)\n",
        "    # calculate standard deviation (1 line of code)\n",
        "    std = np.std(matrix, axis=1, keepdims=True)\n",
        "    # normalize the matrix based on mean and std\n",
        "    matrix = (matrix - mean) / std\n",
        "    ### YOUR CODE ENDS ###\n",
        "\n",
        "    return matrix"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMW3pzAtOA0D"
      },
      "source": [
        "### 2.3 - Logistic Regress Class ###\n",
        "You will create a neural network class - LogisticRegression:\n",
        "    - initialize parameters, such as weights, learning rate, etc.\n",
        "    - implement the gredient descent algorithm\n",
        "    - implement the predict function to make predictions for new data sets\n",
        "    - implement the normalization function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5HCIUQYOA0E"
      },
      "source": [
        "class LogisticRegression():\n",
        "    def __init__(self, num_of_features=1, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        This function creates a vector of zeros of shape (num_of_features, 1) for W and initializes w_0 to 0.\n",
        "\n",
        "        Argument:\n",
        "        num_of_features -- size of the W vector, i.e., the number of features, excluding the bias\n",
        "\n",
        "        Returns:\n",
        "        W -- initialized vector of shape (num_of_features, 1)\n",
        "        w_0 -- initialized scalar (corresponds to the bias)\n",
        "        \"\"\"\n",
        "        # n is the number of features\n",
        "        self.n = num_of_features\n",
        "        # alpha is the learning rate\n",
        "        self.alpha = learning_rate\n",
        "        \n",
        "        ### START YOUR CODE HERE ### \n",
        "        #initialize self.W and self.w_0 to be 0's\n",
        "        self.W = np.zeros(shape=(self.n, 1))\n",
        "        self.w_0 = 0\n",
        "        ### YOUR CODE ENDS ###\n",
        "        assert(self.W.shape == (self.n, 1))\n",
        "        assert(isinstance(self.w_0, float) or isinstance(self.w_0, int))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd-qq2yBOA0H"
      },
      "source": [
        "## 3 - Gradient Descent ##\n",
        "\n",
        "Forward Propagation:\n",
        "- You get X with its shape as (n, m)\n",
        "- You compute  $$h_{W}(X) = a = \\sigma(w^T X + w_{0}) = \\frac{1}{1 + e^{-(w^T x + w_{0})}}\\tag{1}$$\n",
        "- You calculate the loss function:  $$L(W) = \\frac{1}{m} \\sum_{i=1}^{m}- y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{2}$$. \n",
        "\n",
        "Here are the two formulas you will be using: \n",
        "\n",
        "$$ dw_{j} =\\frac{\\partial L}{\\partial w_{j}} = \\frac{1}{m} \\sum_{i=1}^m (( h_{W}(x^{(i)}) -y^{(i)}) * x_{j}^{(i)})\\tag{3}$$\n",
        "$$ dw_{0} = \\frac{\\partial L}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^m (h_{W}(x^{(i)}) -y^{(i)})\\tag{4}$$\n",
        "\n",
        "The weights will be updated:\n",
        "$$ w_{j} = w_{j} - {\\alpha} * dw_{j}\\tag{5}$$\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAHD2fyKOA0H"
      },
      "source": [
        "%%add_to LogisticRegression\n",
        "def fit(self, X, Y, epochs=1000, print_loss=True):\n",
        "    \"\"\"\n",
        "    This function implements the Gradient Descent Algorithm\n",
        "    Arguments:\n",
        "    X -- training data matrix: each column is a training example. \n",
        "            The number of columns is equal to the number of training examples\n",
        "    Y -- true \"label\" vector: shape (1, m)\n",
        "    epochs --\n",
        "\n",
        "    Return:\n",
        "    params -- dictionary containing weights\n",
        "    losses -- loss values of every 100 epochs\n",
        "    grads -- dictionary containing dw and dw_0\n",
        "    \"\"\"\n",
        "    losses = []\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    for i in range(epochs):\n",
        "        # Get the number of training examples\n",
        "        \n",
        "\n",
        "        ### START YOUR CODE HERE ### \n",
        "        # Calculate the hypothesis outputs A (≈ 2 lines of code)\n",
        "        Z = np.dot(self.W.T, X) + self.w_0\n",
        "        A = sigmoid(Z)\n",
        "        # Calculate loss (≈ 1 line of code)\n",
        "        loss = (1/m)*(-Y* np.log(A)) - (1-Y) * np.log(1-A)\n",
        "        \n",
        "        # Calculate the gredients for W and w_0\n",
        "        dw = (1/m)*np.sum((A - Y) * X)\n",
        "        dw_0 = (1/m)*np.sum(A - Y)\n",
        "\n",
        "        # Weight updates\n",
        "        self.W -= (self.alpha * dw)\n",
        "        self.w_0 -= (self.alpha * dw_0)\n",
        "        ### YOUR CODE ENDS ###\n",
        "\n",
        "        if((i % 100) == 0):\n",
        "            losses.append(loss)\n",
        "             # Print the cost every 100 training examples\n",
        "            if print_loss:\n",
        "                print (\"Cost after iteration %i: %f\" %(i, loss))\n",
        "\n",
        "\n",
        "    params = {\n",
        "        \"W\": self.W,\n",
        "        \"w_0\": self.w_0\n",
        "    }\n",
        "\n",
        "    grads = {\n",
        "        \"dw\": dw,\n",
        "        \"dw_0\": dw_0\n",
        "    }\n",
        "\n",
        "    return params, grads, losses"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yygZMLXQOA0J"
      },
      "source": [
        "### Make Predictions ###\n",
        "The predicted output is calculated as $h_{W}(X) = \\sigma(W^T * X + b)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AL8SVxDOA0K"
      },
      "source": [
        "%%add_to LogisticRegression\n",
        "def predict(self, X):\n",
        "    '''\n",
        "    Predict the actual values using learned parameters (self.W, self.w_0)\n",
        "\n",
        "    Arguments:\n",
        "    X -- data of size (n x m)\n",
        "\n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions for the examples in X\n",
        "    '''\n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "\n",
        "    # Compute the actual values\n",
        "    ### START YOUR CODE HERE ### \n",
        "    Z_prediction = np.dot(self.W.T, X) + self.w_0\n",
        "    A = sigmoid(Z_prediction)\n",
        "    \n",
        "    for i in range(A.shape[1]):\n",
        "\n",
        "        # Convert probabilities A[0,i] to actual predictions Y_prediction[0,i]\n",
        "        ### START CODE HERE ### (≈ 3 lines of code)\n",
        "\n",
        "        ## Not sure about this block. But I think the logic goes like: if the hypothesis is less than the average, that means it is going towards 0. The same for the else block, but going towards 1 ##\n",
        "        if A[0, i] <= 0.5:\n",
        "            Y_prediction[0, i] = 0\n",
        "        else:\n",
        "            Y_prediction[0, i] = 1\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    ### YOUR CODE ENDS ###\n",
        "    assert(Y_prediction.shape == (1, m))\n",
        "    return Y_prediction"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RWJcNMQOA0N"
      },
      "source": [
        "## 4. Run the Experiments ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqblmxRQOA0N"
      },
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def Run_Experiment(X_train, Y_train, X_test, Y_test, epochs = 2000, learning_rate = 0.5, print_loss = False):\n",
        "    \"\"\"\n",
        "    Builds the multivariate linear regression model by calling the function you've implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array \n",
        "    Y_train -- training labels represented by a numpy array (vector) \n",
        "    X_test -- test set represented by a numpy array\n",
        "    Y_test -- test labels represented by a numpy array (vector)\n",
        "    epochs -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_loss -- Set to true to print the cost every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    num_of_features = X_train.shape[0]\n",
        "    model = LogisticRegression(num_of_features, learning_rate)\n",
        "    \n",
        "    \n",
        "    ### START YOUR CODE HERE ###\n",
        "    # Obtain the parameters, gredients, and losses by calling a model's method (≈ 1 line of code)\n",
        "    parameters, grads, losses = model.fit(X_train, Y_train, epochs, print_loss)\n",
        "    ### YOUR CODE ENDS ###\n",
        "    \n",
        "    ### START YOUR CODE HERE ###\n",
        "    # Predict test/train set examples (≈ 2 lines of code)\n",
        "    Y_prediction_test = model.predict(X_test)\n",
        "    Y_prediction_train = model.predict(X_train)\n",
        "    ### YOUR CODE ENDS ###\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    W = parameters['W']\n",
        "    w_0 = parameters['w_0']\n",
        "    print(\"W is \" + str(W))\n",
        "    print(\"w_0 is \" + str(w_0))\n",
        "    \n",
        "    d = {\"losses\": losses,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"W\" : W, \n",
        "         \"w_0\" : w_0,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"epochs\": epochs}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-icgtMVEOA0P"
      },
      "source": [
        "### Load Data and Start the Learning Process ###\n",
        "You can change num_iterations and learning_rate to see the learning process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqAGsbunOA0Q",
        "outputId": "309550cb-40d4-406d-ff7b-7c7343807a4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "#from plotutil import plotData\n",
        "\n",
        "data = np.loadtxt('pa3-data1-train.csv', delimiter=',')\n",
        "X_train = data[:,:-1].T\n",
        "y_train = data[:,-1].T\n",
        "\n",
        "#plot data\n",
        "plotData(X_train, y_train, \"1st exam\", \"2nd exam\")\n",
        "\n",
        "data_test = np.loadtxt('pa3-data1-test.csv', delimiter=',')\n",
        "X_test = data_test[:, :-1].T\n",
        "y_test = data_test[:, -1].T"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "FSIYftg3OA0S",
        "outputId": "49626347-17b7-4be1-80ab-4518fa43a604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "source": [
        "d = Run_Experiment(X_train, y_train, X_test, y_test, epochs = 2000, learning_rate = 0.02, print_loss = True)\n",
        "# Plot learning curve (with costs)\n",
        "losses = np.squeeze(d['losses'])\n",
        "plt.plot(losses)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epochs (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ec210464fed3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRun_Experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Plot learning curve (with costs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'losses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-55b98c754702>\u001b[0m in \u001b[0;36mRun_Experiment\u001b[0;34m(X_train, Y_train, X_test, Y_test, epochs, learning_rate, print_loss)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m### START YOUR CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Obtain the parameters, gredients, and losses by calling a model's method (≈ 1 line of code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;31m### YOUR CODE ENDS ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, epochs, print_loss)\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpp-3BWCOA0U"
      },
      "source": [
        "### Plot the learning curve ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KDY1YElOA0U"
      },
      "source": [
        "plotData(normalize(X_train), y_train, xlabel=\"1st exam\", ylabel=\"2nd exam\", w = d[\"W\"], b = d[\"w_0\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfejL7l7OA0X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}